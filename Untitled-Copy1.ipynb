{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5bdaf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Train MNIST with PyTorch.'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "250667ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = \"assets/2\"\n",
    "log_path = os.path.join(directory, \"log\")\n",
    "checkpoint_path = os.path.join(directory, \"ckpt\")\n",
    "data_path = os.path.join(directory, \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c86eb5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.isdir(directory):\n",
    "    os.mkdir(directory)\n",
    "if not os.path.isdir(log_path):\n",
    "    os.mkdir(log_path)\n",
    "if not os.path.isdir(checkpoint_path):\n",
    "    os.mkdir(checkpoint_path)\n",
    "if not os.path.isdir(data_path):\n",
    "    os.mkdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "141a5f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_FC(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(simple_FC, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, n_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Linear(n_hidden, 10)\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7fb09452",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch, net):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        targets = torch.nn.functional.one_hot(targets, num_classes=10).float()\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.argmax(1)).sum().item()\n",
    "    return 100.*correct/total, train_loss/(batch_idx+1)\n",
    "\n",
    "\n",
    "def test(epoch, net, model_name, save_checkpoint=False):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            targets = torch.nn.functional.one_hot(targets, num_classes=10).float()\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.argmax(1)).sum().item()\n",
    "    acc = 100.*correct/total\n",
    "    if save_checkpoint:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        torch.save(state, os.path.join(checkpoint_path, '%s.pth'%model_name))\n",
    "        best_acc = acc\n",
    "    return 100.*correct/total, test_loss/(batch_idx+1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f68b74",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> Preparing data..\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to assets/2/data/MNIST/raw/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3171e63beb4425b89879f76de373ab6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9912422 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting assets/2/data/MNIST/raw/train-images-idx3-ubyte.gz to assets/2/data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to assets/2/data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee73cccdb5d044b682cf0c0e796858ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/28881 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting assets/2/data/MNIST/raw/train-labels-idx1-ubyte.gz to assets/2/data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to assets/2/data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1e32e3733ca44d9ac3170c48fc40c57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1648877 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting assets/2/data/MNIST/raw/t10k-images-idx3-ubyte.gz to assets/2/data/MNIST/raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to assets/2/data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c0375cb61ac42719854f5113594b847",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4542 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting assets/2/data/MNIST/raw/t10k-labels-idx1-ubyte.gz to assets/2/data/MNIST/raw\n",
      "\n",
      "Number of parameters: 805\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.090 | Acc: 11.100%\n",
      "Test Loss: 0.090 | Acc: 11.350%\n",
      "\n",
      "Number of parameters: 1600\n",
      "use previous checkpoints to initialize the weights\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.080 | Acc: 21.525%\n",
      "Test Loss: 0.083 | Acc: 20.460%\n",
      "\n",
      "Number of parameters: 2395\n",
      "use previous checkpoints to initialize the weights\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.071 | Acc: 30.975%\n",
      "Test Loss: 0.076 | Acc: 29.640%\n",
      "\n",
      "Number of parameters: 3190\n",
      "use previous checkpoints to initialize the weights\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.061 | Acc: 41.550%\n",
      "Test Loss: 0.070 | Acc: 38.410%\n",
      "\n",
      "Number of parameters: 3985\n",
      "use previous checkpoints to initialize the weights\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.052 | Acc: 51.100%\n",
      "Test Loss: 0.065 | Acc: 46.020%\n",
      "\n",
      "Number of parameters: 4780\n",
      "use previous checkpoints to initialize the weights\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.042 | Acc: 61.050%\n",
      "Test Loss: 0.061 | Acc: 53.860%\n",
      "\n",
      "Number of parameters: 5575\n",
      "use previous checkpoints to initialize the weights\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.033 | Acc: 70.650%\n",
      "Test Loss: 0.058 | Acc: 60.570%\n",
      "\n",
      "Number of parameters: 6370\n",
      "use previous checkpoints to initialize the weights\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.023 | Acc: 80.475%\n",
      "Test Loss: 0.052 | Acc: 68.960%\n",
      "\n",
      "Number of parameters: 7165\n",
      "use previous checkpoints to initialize the weights\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.016 | Acc: 89.000%\n",
      "Test Loss: 0.053 | Acc: 74.420%\n",
      "\n",
      "Number of parameters: 7960\n",
      "use previous checkpoints to initialize the weights\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.015 | Acc: 89.125%\n",
      "Test Loss: 0.057 | Acc: 73.530%\n",
      "\n",
      "Number of parameters: 8755\n",
      "use previous checkpoints to initialize the weights\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.010 | Acc: 97.475%\n",
      "Test Loss: 0.055 | Acc: 78.840%\n",
      "\n",
      "Number of parameters: 9550\n",
      "use previous checkpoints to initialize the weights\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.008 | Acc: 97.700%\n",
      "Test Loss: 0.060 | Acc: 77.860%\n",
      "\n",
      "Number of parameters: 10345\n",
      "use previous checkpoints to initialize the weights\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.007 | Acc: 97.850%\n",
      "Test Loss: 0.066 | Acc: 76.640%\n",
      "\n",
      "Number of parameters: 11140\n",
      "use previous checkpoints to initialize the weights\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.005 | Acc: 98.325%\n",
      "Test Loss: 0.061 | Acc: 77.880%\n",
      "\n",
      "Number of parameters: 11935\n",
      "use previous checkpoints to initialize the weights\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.005 | Acc: 98.475%\n",
      "Test Loss: 0.061 | Acc: 78.420%\n",
      "\n",
      "Number of parameters: 12730\n",
      "use previous checkpoints to initialize the weights\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.004 | Acc: 98.375%\n",
      "Test Loss: 0.063 | Acc: 78.800%\n",
      "\n",
      "Number of parameters: 13525\n",
      "use previous checkpoints to initialize the weights\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.004 | Acc: 98.375%\n",
      "Test Loss: 0.066 | Acc: 78.060%\n",
      "\n",
      "Number of parameters: 14320\n",
      "use previous checkpoints to initialize the weights\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.004 | Acc: 98.375%\n",
      "Test Loss: 0.069 | Acc: 77.420%\n",
      "\n",
      "Number of parameters: 15115\n",
      "use previous checkpoints to initialize the weights\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.004 | Acc: 98.425%\n",
      "Test Loss: 0.071 | Acc: 77.090%\n",
      "\n",
      "Number of parameters: 15910\n",
      "use previous checkpoints to initialize the weights\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.004 | Acc: 98.425%\n",
      "Test Loss: 0.074 | Acc: 77.030%\n",
      "\n",
      "Number of parameters: 16705\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.005 | Acc: 98.000%\n",
      "Test Loss: 0.028 | Acc: 87.530%\n",
      "\n",
      "Number of parameters: 17500\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.005 | Acc: 98.275%\n",
      "Test Loss: 0.027 | Acc: 87.420%\n",
      "\n",
      "Number of parameters: 18295\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.004 | Acc: 98.500%\n",
      "Test Loss: 0.028 | Acc: 88.050%\n",
      "\n",
      "Number of parameters: 19090\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.004 | Acc: 98.675%\n",
      "Test Loss: 0.028 | Acc: 87.330%\n",
      "\n",
      "Number of parameters: 19885\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.004 | Acc: 98.725%\n",
      "Test Loss: 0.027 | Acc: 88.110%\n",
      "\n",
      "Number of parameters: 20680\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.004 | Acc: 98.650%\n",
      "Test Loss: 0.027 | Acc: 87.290%\n",
      "\n",
      "Number of parameters: 21475\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.004 | Acc: 98.700%\n",
      "Test Loss: 0.027 | Acc: 87.490%\n",
      "\n",
      "Number of parameters: 22270\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.003 | Acc: 98.850%\n",
      "Test Loss: 0.027 | Acc: 88.110%\n",
      "\n",
      "Number of parameters: 23065\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.004 | Acc: 98.700%\n",
      "Test Loss: 0.026 | Acc: 87.850%\n",
      "\n",
      "Number of parameters: 23860\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.004 | Acc: 98.725%\n",
      "Test Loss: 0.026 | Acc: 88.060%\n",
      "\n",
      "Number of parameters: 24655\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.004 | Acc: 98.700%\n",
      "Test Loss: 0.027 | Acc: 87.980%\n",
      "\n",
      "Number of parameters: 25450\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.003 | Acc: 99.200%\n",
      "Test Loss: 0.025 | Acc: 88.920%\n",
      "\n",
      "Number of parameters: 26245\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.005 | Acc: 98.500%\n",
      "Test Loss: 0.026 | Acc: 87.820%\n",
      "\n",
      "Number of parameters: 27040\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.003 | Acc: 99.025%\n",
      "Test Loss: 0.026 | Acc: 88.350%\n",
      "\n",
      "Number of parameters: 27835\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.003 | Acc: 98.775%\n",
      "Test Loss: 0.027 | Acc: 87.350%\n",
      "\n",
      "Number of parameters: 28630\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.003 | Acc: 98.900%\n",
      "Test Loss: 0.026 | Acc: 88.450%\n",
      "\n",
      "Number of parameters: 29425\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.003 | Acc: 98.950%\n",
      "Test Loss: 0.025 | Acc: 88.670%\n",
      "\n",
      "Number of parameters: 30220\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.003 | Acc: 99.025%\n",
      "Test Loss: 0.025 | Acc: 88.290%\n",
      "\n",
      "Number of parameters: 31015\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.004 | Acc: 99.250%\n",
      "Test Loss: 0.026 | Acc: 88.270%\n",
      "\n",
      "Number of parameters: 31810\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.003 | Acc: 99.100%\n",
      "Test Loss: 0.026 | Acc: 88.500%\n",
      "\n",
      "Number of parameters: 32605\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.003 | Acc: 99.100%\n",
      "Test Loss: 0.026 | Acc: 87.900%\n",
      "\n",
      "Number of parameters: 33400\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.003 | Acc: 99.025%\n",
      "Test Loss: 0.024 | Acc: 89.340%\n",
      "\n",
      "Number of parameters: 34195\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.003 | Acc: 99.100%\n",
      "Test Loss: 0.025 | Acc: 88.480%\n",
      "\n",
      "Number of parameters: 34990\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.003 | Acc: 99.200%\n",
      "Test Loss: 0.025 | Acc: 88.520%\n",
      "\n",
      "Number of parameters: 35785\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.003 | Acc: 99.200%\n",
      "Test Loss: 0.027 | Acc: 87.860%\n",
      "\n",
      "Number of parameters: 36580\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.375%\n",
      "Test Loss: 0.025 | Acc: 88.800%\n",
      "\n",
      "Number of parameters: 37375\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.550%\n",
      "Test Loss: 0.025 | Acc: 89.170%\n",
      "\n",
      "Number of parameters: 38170\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.003 | Acc: 99.125%\n",
      "Test Loss: 0.025 | Acc: 88.520%\n",
      "\n",
      "Number of parameters: 38965\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.003 | Acc: 99.000%\n",
      "Test Loss: 0.025 | Acc: 88.530%\n",
      "\n",
      "Number of parameters: 39760\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.400%\n",
      "Test Loss: 0.024 | Acc: 89.280%\n",
      "\n",
      "Number of parameters: 40555\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.450%\n",
      "Test Loss: 0.025 | Acc: 88.980%\n",
      "\n",
      "Number of parameters: 41350\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.375%\n",
      "Test Loss: 0.026 | Acc: 88.680%\n",
      "\n",
      "Number of parameters: 42145\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.003 | Acc: 99.175%\n",
      "Test Loss: 0.024 | Acc: 89.330%\n",
      "\n",
      "Number of parameters: 42940\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.003 | Acc: 99.375%\n",
      "Test Loss: 0.023 | Acc: 89.480%\n",
      "\n",
      "Number of parameters: 43735\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.325%\n",
      "Test Loss: 0.024 | Acc: 89.410%\n",
      "\n",
      "Number of parameters: 44530\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.550%\n",
      "Test Loss: 0.024 | Acc: 89.280%\n",
      "\n",
      "Number of parameters: 45325\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.003 | Acc: 99.300%\n",
      "Test Loss: 0.024 | Acc: 89.420%\n",
      "\n",
      "Number of parameters: 46120\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.375%\n",
      "Test Loss: 0.024 | Acc: 89.590%\n",
      "\n",
      "Number of parameters: 46915\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.003 | Acc: 99.225%\n",
      "Test Loss: 0.024 | Acc: 89.170%\n",
      "\n",
      "Number of parameters: 47710\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.450%\n",
      "Test Loss: 0.024 | Acc: 88.970%\n",
      "\n",
      "Number of parameters: 48505\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.275%\n",
      "Test Loss: 0.025 | Acc: 88.350%\n",
      "\n",
      "Number of parameters: 49300\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.500%\n",
      "Test Loss: 0.023 | Acc: 90.380%\n",
      "\n",
      "Number of parameters: 50095\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.300%\n",
      "Test Loss: 0.024 | Acc: 89.550%\n",
      "\n",
      "Number of parameters: 50890\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.300%\n",
      "Test Loss: 0.025 | Acc: 88.910%\n",
      "\n",
      "Number of parameters: 51685\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.350%\n",
      "Test Loss: 0.023 | Acc: 89.660%\n",
      "\n",
      "Number of parameters: 52480\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.575%\n",
      "Test Loss: 0.024 | Acc: 89.400%\n",
      "\n",
      "Number of parameters: 53275\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.500%\n",
      "Test Loss: 0.024 | Acc: 89.420%\n",
      "\n",
      "Number of parameters: 54070\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.725%\n",
      "Test Loss: 0.023 | Acc: 89.610%\n",
      "\n",
      "Number of parameters: 54865\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.500%\n",
      "Test Loss: 0.024 | Acc: 89.110%\n",
      "\n",
      "Number of parameters: 55660\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.625%\n",
      "Test Loss: 0.022 | Acc: 90.250%\n",
      "\n",
      "Number of parameters: 56455\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.725%\n",
      "Test Loss: 0.024 | Acc: 89.380%\n",
      "\n",
      "Number of parameters: 57250\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.525%\n",
      "Test Loss: 0.023 | Acc: 89.430%\n",
      "\n",
      "Number of parameters: 58045\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.525%\n",
      "Test Loss: 0.024 | Acc: 89.430%\n",
      "\n",
      "Number of parameters: 58840\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.375%\n",
      "Test Loss: 0.023 | Acc: 89.890%\n",
      "\n",
      "Number of parameters: 59635\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.002 | Acc: 99.825%\n",
      "Test Loss: 0.024 | Acc: 89.720%\n",
      "\n",
      "Number of parameters: 60430\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Data: no data augmentation is used\n",
    "print('==> Preparing data..')\n",
    "my_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root=data_path, train=True, download=True, transform=my_transform)\n",
    "trainset = torch.utils.data.Subset(trainset, indices=np.arange(4000))\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=300, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(\n",
    "    root=data_path, train=False, download=True, transform=my_transform)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)\n",
    "\n",
    "n_hidden_units = [i for i in range(1,90)]\n",
    "n_epoch = 6000\n",
    "\n",
    "for n_hidden_unit in n_hidden_units:\n",
    "    # Model\n",
    "    net = simple_FC(n_hidden_unit)\n",
    "    print('Number of parameters: %d'%sum(p.numel() for p in net.parameters()))\n",
    "    net = net.cuda()\n",
    "    net = net.to(device)\n",
    "    if device == 'cuda':\n",
    "        net = net.cuda()\n",
    "        cudnn.benchmark = True\n",
    "    ### initialization\n",
    "    if n_hidden_unit == 1: # smallest network\n",
    "        torch.nn.init.xavier_uniform_(net.features[1].weight, gain=1.0)\n",
    "        torch.nn.init.xavier_uniform_(net.classifier.weight, gain=1.0)\n",
    "    elif n_hidden_unit > 20: # interpolation point: Number of data (4000) * number of class (10) = number of parameters (50*784 + 50 + 50*10 + 10)\n",
    "        torch.nn.init.normal_(net.features[1].weight, mean=0.0, std=0.1)\n",
    "        torch.nn.init.normal_(net.classifier.weight, mean=0.0, std=0.1)\n",
    "    else: \n",
    "        torch.nn.init.normal_(net.features[1].weight, mean=0.0, std=0.1)\n",
    "        torch.nn.init.normal_(net.classifier.weight, mean=0.0, std=0.1)\n",
    "        print('use previous checkpoints to initialize the weights')\n",
    "        i = 1 # load the closest previous model for weight reuse\n",
    "        while not os.path.exists(os.path.join(checkpoint_path, 'simple_FC_%d.pth'%(n_hidden_unit-i))):\n",
    "            print('loading from simple_FC_%d.pth'%(n_hidden_unit-i))\n",
    "            i += 1\n",
    "        checkpoint = torch.load(os.path.join(checkpoint_path, 'simple_FC_%d.pth'%(n_hidden_unit-i)))\n",
    "        with torch.no_grad():\n",
    "            net.features[1].weight[:n_hidden_unit-i, :].copy_(checkpoint['net']['features.1.weight'])\n",
    "            net.features[1].bias[:n_hidden_unit-i].copy_(checkpoint['net']['features.1.bias'])\n",
    "            net.classifier.weight[:, :n_hidden_unit-i].copy_(checkpoint['net']['classifier.weight'])\n",
    "            net.classifier.bias.copy_(checkpoint['net']['classifier.bias'])\n",
    "    ### training and testing\n",
    "    best_acc = 0\n",
    "    start_epoch = 0\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.95)\n",
    "    for epoch in range(start_epoch, start_epoch+n_epoch):\n",
    "        if (epoch+1) % 500 == 0:\n",
    "            if n_hidden_unit <= 20: # learning rate schedule\n",
    "                optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * 0.9\n",
    "        train_acc, train_loss = train(epoch, net)\n",
    "        if n_hidden_unit <= 20 and train_acc == 1 or epoch == start_epoch+n_epoch-1: # early stop before interpolation\n",
    "            test_acc, test_loss = test(epoch, net, 'simple_FC_%d'%(n_hidden_unit), save_checkpoint=True)\n",
    "            print('classification error reaches 0, stop training')\n",
    "            break\n",
    "    print('Training Loss: %.3f | Acc: %.3f%%' % (train_loss, train_acc))\n",
    "    print('Test Loss: %.3f | Acc: %.3f%%\\n' % (test_loss, test_acc))\n",
    "    with open(os.path.join(log_path, 'FC_%d.txt'%n_hidden_unit), 'w') as fw:\n",
    "        fw.write('Number of parameters: %d\\n'%sum(p.numel() for p in net.parameters()))\n",
    "        fw.write('Training Loss: %.3f | Acc: %.3f%%\\n' % (train_loss, train_acc))\n",
    "        fw.write('Test Loss: %.3f | Acc: %.3f%%\\n' % (test_loss, test_acc))\n",
    "\n",
    "\n",
    "\n",
    "model_names = sorted([int(fn.split('_')[1].split('.')[0]) for fn in os.listdir(log_path)])\n",
    "\n",
    "train_losses = {model_name:0. for model_name in model_names}\n",
    "test_losses = {model_name:0. for model_name in model_names}\n",
    "train_accs = {model_name:0. for model_name in model_names}\n",
    "test_accs = {model_name:0. for model_name in model_names}\n",
    "n_params = {model_name:0. for model_name in model_names}\n",
    "\n",
    "for model_name in model_names:\n",
    "    with open(os.path.join(log_path, 'FC_%d.txt'%(model_name))) as f:\n",
    "        for line in f:\n",
    "            if line.startswith('Number'):\n",
    "                n_params[model_name] = float(line.rstrip().split()[-1])\n",
    "            if line.startswith('Training'):\n",
    "                loss = re.search(r'Loss: (.*?) \\|', line).group(1)\n",
    "                train_losses[model_name] = float(loss)\n",
    "                acc = re.search(r'Acc: (.*?)\\%', line).group(1)\n",
    "                train_accs[model_name] = float(acc)\n",
    "            if line.startswith('Test'):\n",
    "                loss = re.search(r'Loss: (.*?) \\|', line).group(1)\n",
    "                test_losses[model_name] = float(loss)\n",
    "                acc = re.search(r'Acc: (.*?)\\%', line).group(1)\n",
    "                test_accs[model_name] = float(acc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93a65af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "plt.plot([n_params[model_name] for model_name in model_names], [train_losses[model_name] for model_name in model_names], marker='o', label='train', color='#e31a1c')\n",
    "plt.plot([n_params[model_name] for model_name in model_names], [test_losses[model_name] for model_name in model_names], marker='o', label='test', color='#1f78b4')\n",
    "plt.ylabel('loss')\n",
    "box = ax.get_position()\n",
    "plt.tight_layout()\n",
    "ax.set_position([box.x0, box.y0,\n",
    "             box.width, box.height * 0.9])\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), fancybox=True, ncol=4)\n",
    "plt.show()\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "plt.plot([n_params[model_name] for model_name in model_names], [train_accs[model_name] for model_name in model_names], marker='o', label='train', color='#e31a1c')\n",
    "plt.plot([n_params[model_name] for model_name in model_names], [test_accs[model_name] for model_name in model_names], marker='o', label='test', color='#1f78b4')\n",
    "plt.ylabel('accuracy')\n",
    "box = ax.get_position()\n",
    "plt.tight_layout()\n",
    "ax.set_position([box.x0, box.y0,\n",
    "             box.width, box.height * 0.9])\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), fancybox=True, ncol=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef2a22c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
