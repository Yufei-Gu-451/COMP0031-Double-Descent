{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93e6bf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "import re\n",
    "import argparse\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2432a106",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_FC(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(simple_FC, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, n_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Linear(n_hidden, 10)\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4c81962a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch, net):\n",
    "    net.train()\n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for batch_idx, (inputs, targets) in enumerate(trainloader):\n",
    "        targets = torch.nn.functional.one_hot(targets, num_classes=10).float()\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets.argmax(1)).sum().item()\n",
    "    return 100.*correct/total, train_loss/(batch_idx+1)\n",
    "\n",
    "\n",
    "def test(epoch, net, model_name, save_checkpoint=False):\n",
    "    global best_acc\n",
    "    net.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (inputs, targets) in enumerate(testloader):\n",
    "            targets = torch.nn.functional.one_hot(targets, num_classes=10).float()\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            total += targets.size(0)\n",
    "            correct += predicted.eq(targets.argmax(1)).sum().item()\n",
    "    acc = 100.*correct/total\n",
    "    if save_checkpoint:\n",
    "        print('Saving..')\n",
    "        state = {\n",
    "            'net': net.state_dict(),\n",
    "            'acc': acc,\n",
    "            'epoch': epoch,\n",
    "        }\n",
    "        torch.save(state, os.path.join('./ckpt', '%s.pth'%model_name))\n",
    "        best_acc = acc\n",
    "    return 100.*correct/total, test_loss/(batch_idx+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cb59f00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using: cuda\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"using:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d8d3ae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_units = [1, 3, 5, 7, 9, 10, 20, 30, 40, 45, 47, 49, 50, 51, 53, 55, 60, 70, 80, 90, 100, 110, 130]\n",
    "n_epoch = 6000\n",
    "n_samples = 4000\n",
    "epoch_step = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74314c3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "interpolation_H = int((n_samples  - 1)*10 / 795)\n",
    "interpolation_H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dfc1510d",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=True, download=True, transform=my_transform)\n",
    "trainset = torch.utils.data.Subset(trainset, indices=np.arange(n_samples))\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, batch_size=256, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(\n",
    "    root='./data', train=False, download=True, transform=my_transform)\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, batch_size=100, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b43e47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.079 | Acc: 21.800%\n",
      "Test Loss: 0.080 | Acc: 21.200%\n",
      "\n",
      "use previous checkpoints to initialize the weights\n",
      "loading from simple_FC_2.pth\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.060 | Acc: 41.525%\n",
      "Test Loss: 0.065 | Acc: 39.340%\n",
      "\n",
      "use previous checkpoints to initialize the weights\n",
      "loading from simple_FC_4.pth\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.042 | Acc: 61.150%\n",
      "Test Loss: 0.053 | Acc: 54.440%\n",
      "\n",
      "use previous checkpoints to initialize the weights\n",
      "loading from simple_FC_6.pth\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.025 | Acc: 80.400%\n",
      "Test Loss: 0.044 | Acc: 70.320%\n",
      "\n",
      "use previous checkpoints to initialize the weights\n",
      "loading from simple_FC_8.pth\n",
      "Saving..\n",
      "classification error reaches 0, stop training\n",
      "Training Loss: 0.009 | Acc: 97.600%\n",
      "Test Loss: 0.039 | Acc: 82.630%\n",
      "\n",
      "use previous checkpoints to initialize the weights\n"
     ]
    }
   ],
   "source": [
    "for n_hidden_unit in n_hidden_units:\n",
    "    # Model\n",
    "    net = simple_FC(n_hidden_unit)\n",
    "    net = net.cuda()\n",
    "    net = net.to(device)\n",
    "    if device == 'cuda':\n",
    "        net = net.cuda()\n",
    "        cudnn.benchmark = True\n",
    "    ### initialization\n",
    "    if n_hidden_unit == 1: # smallest network\n",
    "        torch.nn.init.xavier_uniform_(net.features[1].weight, gain=1.0)\n",
    "        torch.nn.init.xavier_uniform_(net.classifier.weight, gain=1.0)\n",
    "    elif n_hidden_unit > interpolation_H: # interpolation point: Number of data (4000) * number of class (10) = number of parameters (50*784 + 50 + 50*10 + 10)\n",
    "        torch.nn.init.normal_(net.features[1].weight, mean=0.0, std=0.1)\n",
    "        torch.nn.init.normal_(net.classifier.weight, mean=0.0, std=0.1)\n",
    "    else: \n",
    "        torch.nn.init.normal_(net.features[1].weight, mean=0.0, std=0.1)\n",
    "        torch.nn.init.normal_(net.classifier.weight, mean=0.0, std=0.1)\n",
    "        print('use previous checkpoints to initialize the weights')\n",
    "        i = 1 # load the closest previous model for weight reuse\n",
    "        while not os.path.exists(os.path.join('./ckpt', 'simple_FC_%d.pth'%(n_hidden_unit-i))):\n",
    "            print('loading from simple_FC_%d.pth'%(n_hidden_unit-i))\n",
    "            i += 1\n",
    "        checkpoint = torch.load(os.path.join('./ckpt', 'simple_FC_%d.pth'%(n_hidden_unit-i)))\n",
    "        with torch.no_grad():\n",
    "            net.features[1].weight[:n_hidden_unit-i, :].copy_(checkpoint['net']['features.1.weight'])\n",
    "            net.features[1].bias[:n_hidden_unit-i].copy_(checkpoint['net']['features.1.bias'])\n",
    "            net.classifier.weight[:, :n_hidden_unit-i].copy_(checkpoint['net']['classifier.weight'])\n",
    "            net.classifier.bias.copy_(checkpoint['net']['classifier.bias'])\n",
    "    ### training and testing\n",
    "    best_acc = 0\n",
    "    start_epoch = 0\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.SGD(net.parameters(), lr=0.01, momentum=0.95)\n",
    "    for epoch in range(start_epoch, start_epoch+n_epoch):\n",
    "        if (epoch+1) % 500 == 0:\n",
    "            if n_hidden_unit <= 50: # learning rate schedule\n",
    "                optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * 0.9\n",
    "        train_acc, train_loss = train(epoch, net)\n",
    "        if n_hidden_unit <= 50 and train_acc == 1 or epoch == start_epoch+n_epoch-1: # early stop before interpolation\n",
    "            test_acc, test_loss = test(epoch, net, 'simple_FC_%d'%(n_hidden_unit), save_checkpoint=True)\n",
    "            print('classification error reaches 0, stop training')\n",
    "            break\n",
    "    print('Training Loss: %.3f | Acc: %.3f%%' % (train_loss, train_acc))\n",
    "    print('Test Loss: %.3f | Acc: %.3f%%\\n' % (test_loss, test_acc))\n",
    "    with open(os.path.join('./logs', 'FC_%d.txt'%n_hidden_unit), 'w') as fw:\n",
    "        fw.write('Number of parameters: %d\\n'%sum(p.numel() for p in net.parameters()))\n",
    "        fw.write('Training Loss: %.3f | Acc: %.3f%%\\n' % (train_loss, train_acc))\n",
    "        fw.write('Test Loss: %.3f | Acc: %.3f%%\\n' % (test_loss, test_acc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad154fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_names = sorted([int(fn.split('_')[1].split('.')[0]) for fn in os.listdir('./logs')])\n",
    "\n",
    "train_losses = {model_name:0. for model_name in model_names}\n",
    "test_losses = {model_name:0. for model_name in model_names}\n",
    "train_accs = {model_name:0. for model_name in model_names}\n",
    "test_accs = {model_name:0. for model_name in model_names}\n",
    "n_params = {model_name:0. for model_name in model_names}\n",
    "\n",
    "for model_name in model_names:\n",
    "    with open(os.path.join('log', 'FC_%d.txt'%(model_name))) as f:\n",
    "        for line in f:\n",
    "            if line.startswith('Number'):\n",
    "                n_params[model_name] = float(line.rstrip().split()[-1])\n",
    "            if line.startswith('Training'):\n",
    "                loss = re.search(r'Loss: (.*?) \\|', line).group(1)\n",
    "                train_losses[model_name] = float(loss)\n",
    "                acc = re.search(r'Acc: (.*?)\\%', line).group(1)\n",
    "                train_accs[model_name] = float(acc)\n",
    "            if line.startswith('Test'):\n",
    "                loss = re.search(r'Loss: (.*?) \\|', line).group(1)\n",
    "                test_losses[model_name] = float(loss)\n",
    "                acc = re.search(r'Acc: (.*?)\\%', line).group(1)\n",
    "                test_accs[model_name] = float(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84723ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot \n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "plt.plot([n_params[model_name] for model_name in model_names], [train_losses[model_name] for model_name in model_names], marker='o', label='train', color='#e31a1c')\n",
    "plt.plot([n_params[model_name] for model_name in model_names], [test_losses[model_name] for model_name in model_names], marker='o', label='test', color='#1f78b4')\n",
    "plt.ylabel('loss')\n",
    "box = ax.get_position()\n",
    "plt.tight_layout()\n",
    "ax.set_position([box.x0, box.y0,\n",
    "             box.width, box.height * 0.9])\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), fancybox=True, ncol=4)\n",
    "plt.savefig('MNIST_double_descent_loss_w_weight_reuse.png')\n",
    "\n",
    "\n",
    "plt.clf()\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "plt.plot([n_params[model_name] for model_name in model_names], [train_accs[model_name] for model_name in model_names], marker='o', label='train', color='#e31a1c')\n",
    "plt.plot([n_params[model_name] for model_name in model_names], [test_accs[model_name] for model_name in model_names], marker='o', label='test', color='#1f78b4')\n",
    "plt.ylabel('accuracy')\n",
    "box = ax.get_position()\n",
    "plt.tight_layout()\n",
    "ax.set_position([box.x0, box.y0,\n",
    "             box.width, box.height * 0.9])\n",
    "plt.legend(loc='upper center', bbox_to_anchor=(0.5, 1.25), fancybox=True, ncol=4)\n",
    "plt.savefig('MNIST_double_descent_accuracy_w_weight_reuse.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970cee3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
