{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03167983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "240b8948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device : cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device :', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0fd4f97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load MINST success\n"
     ]
    }
   ],
   "source": [
    "my_transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "\n",
    "trainset = datasets.MNIST(root='./data', train=True,\n",
    "                                        download=True, transform = my_transform)\n",
    "\n",
    "subset = list(range(0, 4000, 1))\n",
    "\n",
    "trainset_1 = torch.utils.data.Subset(trainset, subset)\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(trainset_1, batch_size=128,\n",
    "                                          shuffle=True, num_workers=4)\n",
    "\n",
    "testset = datasets.MNIST(root='./data', train=False,\n",
    "                                       download=True, transform = my_transform)\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=128,\n",
    "                                         shuffle=False, num_workers=4)\n",
    "                          \n",
    "print('Load MINST success')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f942de6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self, hidden_units):\n",
    "        super(Net, self).__init__()\n",
    "        self.layer1 = torch.nn.Linear(28*28, hidden_units)\n",
    "        #self.layer2 = torch.nn.Linear(hidden_units, hidden_units)\n",
    "        self.layer3 = torch.nn.Linear(hidden_units, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.layer1(x))\n",
    "        #x = torch.relu(self.layer2(x))\n",
    "        x = torch.sigmoid(self.layer3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a4e42b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_FC(nn.Module):\n",
    "    def __init__(self, n_hidden):\n",
    "        super(simple_FC, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(784, n_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.classifier = nn.Linear(n_hidden, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b76c6fd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, hidden_unit, optimizer, criterion, n_epochs):\n",
    "    train_losses, test_losses = [], []\n",
    "    train_acc, test_acc, epoch = 0, 0, 0\n",
    "    \n",
    "    while epoch <= n_epochs and train_acc < 0.99:\n",
    "        if epoch >= 1:\n",
    "            print(\"Epoch : %d ; Train Loss : %f ; Train Acc : %.3f ; Test Loss : %f ; Test Acc : %.3f ; LR : %.3f\" \n",
    "                  % (epoch, train_losses[-1], train_acc, test_losses[-1], test_acc, optimizer.param_groups[0]['lr']))\n",
    "        epoch += 1\n",
    "        \n",
    "        if hidden_unit <= 50 and epoch % 50 == 1:\n",
    "            optimizer.param_groups[0]['lr'] = optimizer.param_groups[0]['lr'] * 0.9\n",
    "    \n",
    "        model.train()\n",
    "        \n",
    "        train_loss, correct, total = 0.0, 0, 0\n",
    "        for inputs, labels in trainloader:\n",
    "            labels = torch.nn.functional.one_hot(labels, num_classes=10).float()\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels.argmax(1)).sum().item()\n",
    "\n",
    "        train_losses.append(train_loss/len(trainloader))\n",
    "        train_acc = correct/total\n",
    "        \n",
    "        \n",
    "        model.eval()\n",
    "        \n",
    "        test_loss, correct, total = 0.0, 0, 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in testloader:\n",
    "                labels = torch.nn.functional.one_hot(labels, num_classes=10).float()\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                test_loss += loss.item()\n",
    "                \n",
    "                _, predicted = outputs.max(1)\n",
    "            total += labels.size(0)\n",
    "            correct += predicted.eq(labels.argmax(1)).sum().item()\n",
    "            \n",
    "        test_losses.append(test_loss/len(testloader))\n",
    "        test_acc = correct/total\n",
    "\n",
    "    return train_losses[-1], train_acc, test_losses[-1], test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7c5a6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Settings\n",
    "\n",
    "hidden_units = [1, 5, 10, 20, 30, 40, 45, 47, 49, 50, 51, 53, 55, 60, 80, 100]\n",
    "n_epochs = 600\n",
    "momentum = 0.95\n",
    "learning_rate = 0.05\n",
    "\n",
    "parameters = []\n",
    "for hidden_unit in hidden_units:\n",
    "    parameters.append(784 * hidden_unit + hidden_unit * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35931d74",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 1 ; Train Loss : 0.221349 ; Train Acc : 0.109 ; Test Loss : 0.158171 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 2 ; Train Loss : 0.115999 ; Train Acc : 0.090 ; Test Loss : 0.101261 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 3 ; Train Loss : 0.095103 ; Train Acc : 0.100 ; Test Loss : 0.091455 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 4 ; Train Loss : 0.090915 ; Train Acc : 0.114 ; Test Loss : 0.090256 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 5 ; Train Loss : 0.090185 ; Train Acc : 0.108 ; Test Loss : 0.090000 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 6 ; Train Loss : 0.090060 ; Train Acc : 0.108 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 7 ; Train Loss : 0.090045 ; Train Acc : 0.094 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 8 ; Train Loss : 0.090019 ; Train Acc : 0.105 ; Test Loss : 0.090008 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 9 ; Train Loss : 0.090021 ; Train Acc : 0.111 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 10 ; Train Loss : 0.089988 ; Train Acc : 0.106 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 11 ; Train Loss : 0.090033 ; Train Acc : 0.101 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 12 ; Train Loss : 0.090008 ; Train Acc : 0.105 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 13 ; Train Loss : 0.090040 ; Train Acc : 0.111 ; Test Loss : 0.089979 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 14 ; Train Loss : 0.090058 ; Train Acc : 0.108 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 15 ; Train Loss : 0.090029 ; Train Acc : 0.111 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 16 ; Train Loss : 0.090025 ; Train Acc : 0.103 ; Test Loss : 0.089980 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 17 ; Train Loss : 0.090035 ; Train Acc : 0.110 ; Test Loss : 0.090026 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 18 ; Train Loss : 0.090039 ; Train Acc : 0.101 ; Test Loss : 0.089974 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 19 ; Train Loss : 0.090020 ; Train Acc : 0.100 ; Test Loss : 0.090007 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 20 ; Train Loss : 0.090027 ; Train Acc : 0.106 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 21 ; Train Loss : 0.090020 ; Train Acc : 0.105 ; Test Loss : 0.090026 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 22 ; Train Loss : 0.090012 ; Train Acc : 0.111 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 23 ; Train Loss : 0.090019 ; Train Acc : 0.102 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 24 ; Train Loss : 0.090007 ; Train Acc : 0.103 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 25 ; Train Loss : 0.090002 ; Train Acc : 0.111 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 26 ; Train Loss : 0.089997 ; Train Acc : 0.100 ; Test Loss : 0.090009 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 27 ; Train Loss : 0.090007 ; Train Acc : 0.111 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 28 ; Train Loss : 0.090047 ; Train Acc : 0.106 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 29 ; Train Loss : 0.090000 ; Train Acc : 0.099 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 30 ; Train Loss : 0.090007 ; Train Acc : 0.111 ; Test Loss : 0.090015 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 31 ; Train Loss : 0.090038 ; Train Acc : 0.111 ; Test Loss : 0.089982 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 32 ; Train Loss : 0.090027 ; Train Acc : 0.104 ; Test Loss : 0.090002 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 33 ; Train Loss : 0.089991 ; Train Acc : 0.103 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 34 ; Train Loss : 0.090020 ; Train Acc : 0.105 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 35 ; Train Loss : 0.089997 ; Train Acc : 0.109 ; Test Loss : 0.090000 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 36 ; Train Loss : 0.090000 ; Train Acc : 0.108 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 37 ; Train Loss : 0.090000 ; Train Acc : 0.106 ; Test Loss : 0.089991 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 38 ; Train Loss : 0.090006 ; Train Acc : 0.108 ; Test Loss : 0.089993 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 39 ; Train Loss : 0.090027 ; Train Acc : 0.110 ; Test Loss : 0.090011 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 40 ; Train Loss : 0.090000 ; Train Acc : 0.104 ; Test Loss : 0.089989 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 41 ; Train Loss : 0.090018 ; Train Acc : 0.098 ; Test Loss : 0.090004 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 42 ; Train Loss : 0.090054 ; Train Acc : 0.108 ; Test Loss : 0.090017 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 43 ; Train Loss : 0.090025 ; Train Acc : 0.098 ; Test Loss : 0.089980 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 44 ; Train Loss : 0.090018 ; Train Acc : 0.106 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 45 ; Train Loss : 0.090019 ; Train Acc : 0.103 ; Test Loss : 0.089972 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 46 ; Train Loss : 0.090005 ; Train Acc : 0.103 ; Test Loss : 0.090012 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 47 ; Train Loss : 0.089986 ; Train Acc : 0.107 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 48 ; Train Loss : 0.089997 ; Train Acc : 0.111 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 49 ; Train Loss : 0.090007 ; Train Acc : 0.107 ; Test Loss : 0.090002 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 50 ; Train Loss : 0.090009 ; Train Acc : 0.103 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 51 ; Train Loss : 0.090024 ; Train Acc : 0.102 ; Test Loss : 0.090000 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 52 ; Train Loss : 0.090053 ; Train Acc : 0.104 ; Test Loss : 0.090001 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 53 ; Train Loss : 0.090035 ; Train Acc : 0.105 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 54 ; Train Loss : 0.090023 ; Train Acc : 0.092 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 55 ; Train Loss : 0.089985 ; Train Acc : 0.108 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 56 ; Train Loss : 0.090031 ; Train Acc : 0.108 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 57 ; Train Loss : 0.090020 ; Train Acc : 0.111 ; Test Loss : 0.090011 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 58 ; Train Loss : 0.090053 ; Train Acc : 0.109 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 59 ; Train Loss : 0.089997 ; Train Acc : 0.105 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 60 ; Train Loss : 0.090009 ; Train Acc : 0.107 ; Test Loss : 0.090015 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 61 ; Train Loss : 0.090038 ; Train Acc : 0.109 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 62 ; Train Loss : 0.089995 ; Train Acc : 0.101 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 63 ; Train Loss : 0.090027 ; Train Acc : 0.102 ; Test Loss : 0.090016 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 64 ; Train Loss : 0.090029 ; Train Acc : 0.103 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 65 ; Train Loss : 0.089994 ; Train Acc : 0.111 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 66 ; Train Loss : 0.089986 ; Train Acc : 0.113 ; Test Loss : 0.090008 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 67 ; Train Loss : 0.089996 ; Train Acc : 0.113 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 68 ; Train Loss : 0.090034 ; Train Acc : 0.104 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 69 ; Train Loss : 0.089998 ; Train Acc : 0.106 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 70 ; Train Loss : 0.089999 ; Train Acc : 0.102 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 71 ; Train Loss : 0.090003 ; Train Acc : 0.102 ; Test Loss : 0.089990 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 72 ; Train Loss : 0.090012 ; Train Acc : 0.103 ; Test Loss : 0.089996 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 73 ; Train Loss : 0.090021 ; Train Acc : 0.088 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 74 ; Train Loss : 0.090022 ; Train Acc : 0.105 ; Test Loss : 0.090006 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 75 ; Train Loss : 0.090027 ; Train Acc : 0.101 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 76 ; Train Loss : 0.090044 ; Train Acc : 0.111 ; Test Loss : 0.090005 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 77 ; Train Loss : 0.090003 ; Train Acc : 0.102 ; Test Loss : 0.090002 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 78 ; Train Loss : 0.090050 ; Train Acc : 0.107 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 79 ; Train Loss : 0.090011 ; Train Acc : 0.111 ; Test Loss : 0.090003 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 80 ; Train Loss : 0.089987 ; Train Acc : 0.105 ; Test Loss : 0.089987 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 81 ; Train Loss : 0.090009 ; Train Acc : 0.106 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 82 ; Train Loss : 0.090055 ; Train Acc : 0.111 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 83 ; Train Loss : 0.090008 ; Train Acc : 0.109 ; Test Loss : 0.089997 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 84 ; Train Loss : 0.089999 ; Train Acc : 0.111 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 85 ; Train Loss : 0.090012 ; Train Acc : 0.108 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 86 ; Train Loss : 0.090011 ; Train Acc : 0.094 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 87 ; Train Loss : 0.090007 ; Train Acc : 0.113 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 88 ; Train Loss : 0.090013 ; Train Acc : 0.102 ; Test Loss : 0.090002 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 89 ; Train Loss : 0.090013 ; Train Acc : 0.108 ; Test Loss : 0.089978 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 90 ; Train Loss : 0.090011 ; Train Acc : 0.105 ; Test Loss : 0.090005 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 91 ; Train Loss : 0.090004 ; Train Acc : 0.101 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 92 ; Train Loss : 0.089986 ; Train Acc : 0.098 ; Test Loss : 0.089997 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 93 ; Train Loss : 0.090013 ; Train Acc : 0.107 ; Test Loss : 0.090003 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 94 ; Train Loss : 0.089984 ; Train Acc : 0.104 ; Test Loss : 0.089974 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 95 ; Train Loss : 0.089997 ; Train Acc : 0.111 ; Test Loss : 0.090002 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 96 ; Train Loss : 0.089993 ; Train Acc : 0.110 ; Test Loss : 0.090001 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 97 ; Train Loss : 0.090017 ; Train Acc : 0.102 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 98 ; Train Loss : 0.090009 ; Train Acc : 0.100 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 99 ; Train Loss : 0.089985 ; Train Acc : 0.105 ; Test Loss : 0.090019 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 100 ; Train Loss : 0.090008 ; Train Acc : 0.103 ; Test Loss : 0.089978 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 101 ; Train Loss : 0.089981 ; Train Acc : 0.111 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 102 ; Train Loss : 0.089988 ; Train Acc : 0.103 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 103 ; Train Loss : 0.089996 ; Train Acc : 0.111 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 104 ; Train Loss : 0.089973 ; Train Acc : 0.108 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 105 ; Train Loss : 0.089998 ; Train Acc : 0.107 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 106 ; Train Loss : 0.090012 ; Train Acc : 0.111 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 107 ; Train Loss : 0.089998 ; Train Acc : 0.102 ; Test Loss : 0.089987 ; Test Acc : 0.062 ; LR : 0.036\n",
      "Epoch : 108 ; Train Loss : 0.089986 ; Train Acc : 0.103 ; Test Loss : 0.089994 ; Test Acc : 0.062 ; LR : 0.036\n",
      "Epoch : 109 ; Train Loss : 0.090010 ; Train Acc : 0.108 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 110 ; Train Loss : 0.090009 ; Train Acc : 0.103 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 111 ; Train Loss : 0.089991 ; Train Acc : 0.108 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 112 ; Train Loss : 0.090024 ; Train Acc : 0.111 ; Test Loss : 0.089970 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 113 ; Train Loss : 0.090025 ; Train Acc : 0.100 ; Test Loss : 0.089997 ; Test Acc : 0.062 ; LR : 0.036\n",
      "Epoch : 114 ; Train Loss : 0.090005 ; Train Acc : 0.108 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 115 ; Train Loss : 0.090000 ; Train Acc : 0.102 ; Test Loss : 0.090005 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 116 ; Train Loss : 0.089966 ; Train Acc : 0.110 ; Test Loss : 0.089980 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 117 ; Train Loss : 0.089992 ; Train Acc : 0.111 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 118 ; Train Loss : 0.090012 ; Train Acc : 0.106 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 119 ; Train Loss : 0.090006 ; Train Acc : 0.106 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 120 ; Train Loss : 0.089978 ; Train Acc : 0.104 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 121 ; Train Loss : 0.089979 ; Train Acc : 0.100 ; Test Loss : 0.090002 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 122 ; Train Loss : 0.089996 ; Train Acc : 0.111 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 123 ; Train Loss : 0.090044 ; Train Acc : 0.095 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 124 ; Train Loss : 0.090047 ; Train Acc : 0.107 ; Test Loss : 0.090008 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 125 ; Train Loss : 0.090048 ; Train Acc : 0.099 ; Test Loss : 0.089997 ; Test Acc : 0.062 ; LR : 0.036\n",
      "Epoch : 126 ; Train Loss : 0.090025 ; Train Acc : 0.112 ; Test Loss : 0.090007 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 127 ; Train Loss : 0.090016 ; Train Acc : 0.111 ; Test Loss : 0.089987 ; Test Acc : 0.062 ; LR : 0.036\n",
      "Epoch : 128 ; Train Loss : 0.090015 ; Train Acc : 0.099 ; Test Loss : 0.090015 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 129 ; Train Loss : 0.090022 ; Train Acc : 0.102 ; Test Loss : 0.089978 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 130 ; Train Loss : 0.090025 ; Train Acc : 0.107 ; Test Loss : 0.090012 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 131 ; Train Loss : 0.089997 ; Train Acc : 0.111 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 132 ; Train Loss : 0.089984 ; Train Acc : 0.107 ; Test Loss : 0.090006 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 133 ; Train Loss : 0.090001 ; Train Acc : 0.107 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 134 ; Train Loss : 0.089979 ; Train Acc : 0.106 ; Test Loss : 0.090008 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 135 ; Train Loss : 0.089984 ; Train Acc : 0.102 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 136 ; Train Loss : 0.089975 ; Train Acc : 0.105 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 137 ; Train Loss : 0.089993 ; Train Acc : 0.104 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 138 ; Train Loss : 0.090005 ; Train Acc : 0.106 ; Test Loss : 0.089996 ; Test Acc : 0.062 ; LR : 0.036\n",
      "Epoch : 139 ; Train Loss : 0.090004 ; Train Acc : 0.109 ; Test Loss : 0.090000 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 140 ; Train Loss : 0.090012 ; Train Acc : 0.102 ; Test Loss : 0.089996 ; Test Acc : 0.062 ; LR : 0.036\n",
      "Epoch : 141 ; Train Loss : 0.090021 ; Train Acc : 0.110 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 142 ; Train Loss : 0.090045 ; Train Acc : 0.103 ; Test Loss : 0.090027 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 143 ; Train Loss : 0.090036 ; Train Acc : 0.110 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 144 ; Train Loss : 0.090007 ; Train Acc : 0.106 ; Test Loss : 0.090014 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 145 ; Train Loss : 0.090010 ; Train Acc : 0.110 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 146 ; Train Loss : 0.090004 ; Train Acc : 0.100 ; Test Loss : 0.089981 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 147 ; Train Loss : 0.089994 ; Train Acc : 0.095 ; Test Loss : 0.089978 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 148 ; Train Loss : 0.090020 ; Train Acc : 0.105 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 149 ; Train Loss : 0.090009 ; Train Acc : 0.107 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 150 ; Train Loss : 0.089990 ; Train Acc : 0.104 ; Test Loss : 0.090012 ; Test Acc : 0.125 ; LR : 0.036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 151 ; Train Loss : 0.089998 ; Train Acc : 0.108 ; Test Loss : 0.089984 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 152 ; Train Loss : 0.089996 ; Train Acc : 0.111 ; Test Loss : 0.090001 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 153 ; Train Loss : 0.090017 ; Train Acc : 0.095 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 154 ; Train Loss : 0.089985 ; Train Acc : 0.103 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 155 ; Train Loss : 0.090027 ; Train Acc : 0.106 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 156 ; Train Loss : 0.090004 ; Train Acc : 0.111 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 157 ; Train Loss : 0.089984 ; Train Acc : 0.103 ; Test Loss : 0.089999 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 158 ; Train Loss : 0.089998 ; Train Acc : 0.107 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 159 ; Train Loss : 0.089996 ; Train Acc : 0.107 ; Test Loss : 0.090000 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 160 ; Train Loss : 0.090001 ; Train Acc : 0.109 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 161 ; Train Loss : 0.089991 ; Train Acc : 0.105 ; Test Loss : 0.090010 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 162 ; Train Loss : 0.089998 ; Train Acc : 0.096 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 163 ; Train Loss : 0.089994 ; Train Acc : 0.106 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 164 ; Train Loss : 0.090023 ; Train Acc : 0.094 ; Test Loss : 0.090009 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 165 ; Train Loss : 0.090016 ; Train Acc : 0.111 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 166 ; Train Loss : 0.090023 ; Train Acc : 0.105 ; Test Loss : 0.089985 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 167 ; Train Loss : 0.090015 ; Train Acc : 0.102 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 168 ; Train Loss : 0.090042 ; Train Acc : 0.106 ; Test Loss : 0.090006 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 169 ; Train Loss : 0.090013 ; Train Acc : 0.106 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 170 ; Train Loss : 0.090027 ; Train Acc : 0.104 ; Test Loss : 0.090013 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 171 ; Train Loss : 0.089995 ; Train Acc : 0.107 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 172 ; Train Loss : 0.089984 ; Train Acc : 0.103 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 173 ; Train Loss : 0.090035 ; Train Acc : 0.094 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 174 ; Train Loss : 0.089995 ; Train Acc : 0.100 ; Test Loss : 0.090015 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 175 ; Train Loss : 0.090039 ; Train Acc : 0.110 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 176 ; Train Loss : 0.089998 ; Train Acc : 0.107 ; Test Loss : 0.090015 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 177 ; Train Loss : 0.089985 ; Train Acc : 0.106 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 178 ; Train Loss : 0.089987 ; Train Acc : 0.111 ; Test Loss : 0.090015 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 179 ; Train Loss : 0.089998 ; Train Acc : 0.103 ; Test Loss : 0.089993 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 180 ; Train Loss : 0.089989 ; Train Acc : 0.109 ; Test Loss : 0.090003 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 181 ; Train Loss : 0.090003 ; Train Acc : 0.097 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 182 ; Train Loss : 0.090015 ; Train Acc : 0.111 ; Test Loss : 0.090009 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 183 ; Train Loss : 0.089977 ; Train Acc : 0.106 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 184 ; Train Loss : 0.089996 ; Train Acc : 0.101 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 185 ; Train Loss : 0.089993 ; Train Acc : 0.111 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 186 ; Train Loss : 0.090012 ; Train Acc : 0.103 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 187 ; Train Loss : 0.090011 ; Train Acc : 0.106 ; Test Loss : 0.089995 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 188 ; Train Loss : 0.089992 ; Train Acc : 0.099 ; Test Loss : 0.090003 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 189 ; Train Loss : 0.089984 ; Train Acc : 0.108 ; Test Loss : 0.090004 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 190 ; Train Loss : 0.089987 ; Train Acc : 0.103 ; Test Loss : 0.090006 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 191 ; Train Loss : 0.089982 ; Train Acc : 0.099 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 192 ; Train Loss : 0.089986 ; Train Acc : 0.108 ; Test Loss : 0.090003 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 193 ; Train Loss : 0.090018 ; Train Acc : 0.108 ; Test Loss : 0.090002 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 194 ; Train Loss : 0.089995 ; Train Acc : 0.101 ; Test Loss : 0.089979 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 195 ; Train Loss : 0.089987 ; Train Acc : 0.108 ; Test Loss : 0.090017 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 196 ; Train Loss : 0.089987 ; Train Acc : 0.111 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 197 ; Train Loss : 0.090009 ; Train Acc : 0.101 ; Test Loss : 0.090010 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 198 ; Train Loss : 0.089986 ; Train Acc : 0.106 ; Test Loss : 0.089985 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 199 ; Train Loss : 0.089992 ; Train Acc : 0.105 ; Test Loss : 0.090006 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 200 ; Train Loss : 0.089984 ; Train Acc : 0.107 ; Test Loss : 0.090000 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 201 ; Train Loss : 0.090005 ; Train Acc : 0.102 ; Test Loss : 0.089981 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 202 ; Train Loss : 0.089992 ; Train Acc : 0.106 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 203 ; Train Loss : 0.089973 ; Train Acc : 0.103 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 204 ; Train Loss : 0.090002 ; Train Acc : 0.107 ; Test Loss : 0.089996 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 205 ; Train Loss : 0.089998 ; Train Acc : 0.103 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 206 ; Train Loss : 0.089981 ; Train Acc : 0.106 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 207 ; Train Loss : 0.090010 ; Train Acc : 0.107 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 208 ; Train Loss : 0.089976 ; Train Acc : 0.107 ; Test Loss : 0.089995 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 209 ; Train Loss : 0.089984 ; Train Acc : 0.102 ; Test Loss : 0.089981 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 210 ; Train Loss : 0.089980 ; Train Acc : 0.111 ; Test Loss : 0.090000 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 211 ; Train Loss : 0.090012 ; Train Acc : 0.116 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 212 ; Train Loss : 0.090018 ; Train Acc : 0.098 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 213 ; Train Loss : 0.090006 ; Train Acc : 0.102 ; Test Loss : 0.090000 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 214 ; Train Loss : 0.089966 ; Train Acc : 0.108 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 215 ; Train Loss : 0.089971 ; Train Acc : 0.111 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 216 ; Train Loss : 0.089986 ; Train Acc : 0.108 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 217 ; Train Loss : 0.089977 ; Train Acc : 0.111 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 218 ; Train Loss : 0.089980 ; Train Acc : 0.103 ; Test Loss : 0.090003 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 219 ; Train Loss : 0.090007 ; Train Acc : 0.108 ; Test Loss : 0.089985 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 220 ; Train Loss : 0.090006 ; Train Acc : 0.108 ; Test Loss : 0.090006 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 221 ; Train Loss : 0.090035 ; Train Acc : 0.104 ; Test Loss : 0.089997 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 222 ; Train Loss : 0.089975 ; Train Acc : 0.100 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 223 ; Train Loss : 0.090005 ; Train Acc : 0.107 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 224 ; Train Loss : 0.089975 ; Train Acc : 0.102 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 225 ; Train Loss : 0.089983 ; Train Acc : 0.112 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 226 ; Train Loss : 0.090015 ; Train Acc : 0.102 ; Test Loss : 0.090002 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 227 ; Train Loss : 0.089993 ; Train Acc : 0.107 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 228 ; Train Loss : 0.089990 ; Train Acc : 0.111 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 229 ; Train Loss : 0.089987 ; Train Acc : 0.107 ; Test Loss : 0.090002 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 230 ; Train Loss : 0.090001 ; Train Acc : 0.109 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 231 ; Train Loss : 0.090011 ; Train Acc : 0.096 ; Test Loss : 0.089988 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 232 ; Train Loss : 0.089997 ; Train Acc : 0.108 ; Test Loss : 0.090008 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 233 ; Train Loss : 0.089997 ; Train Acc : 0.109 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 234 ; Train Loss : 0.089984 ; Train Acc : 0.105 ; Test Loss : 0.089998 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 235 ; Train Loss : 0.089988 ; Train Acc : 0.106 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 236 ; Train Loss : 0.089995 ; Train Acc : 0.112 ; Test Loss : 0.089999 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 237 ; Train Loss : 0.090000 ; Train Acc : 0.107 ; Test Loss : 0.090010 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 238 ; Train Loss : 0.089984 ; Train Acc : 0.099 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 239 ; Train Loss : 0.089993 ; Train Acc : 0.107 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 240 ; Train Loss : 0.090002 ; Train Acc : 0.101 ; Test Loss : 0.089999 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 241 ; Train Loss : 0.090011 ; Train Acc : 0.108 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 242 ; Train Loss : 0.090013 ; Train Acc : 0.095 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 243 ; Train Loss : 0.090000 ; Train Acc : 0.103 ; Test Loss : 0.090019 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 244 ; Train Loss : 0.090019 ; Train Acc : 0.111 ; Test Loss : 0.090006 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 245 ; Train Loss : 0.090003 ; Train Acc : 0.102 ; Test Loss : 0.089998 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 246 ; Train Loss : 0.089968 ; Train Acc : 0.105 ; Test Loss : 0.089981 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 247 ; Train Loss : 0.089975 ; Train Acc : 0.111 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 248 ; Train Loss : 0.089975 ; Train Acc : 0.111 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 249 ; Train Loss : 0.089991 ; Train Acc : 0.110 ; Test Loss : 0.090000 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 250 ; Train Loss : 0.089982 ; Train Acc : 0.102 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 251 ; Train Loss : 0.089980 ; Train Acc : 0.111 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 252 ; Train Loss : 0.089999 ; Train Acc : 0.106 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 253 ; Train Loss : 0.089988 ; Train Acc : 0.109 ; Test Loss : 0.090012 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 254 ; Train Loss : 0.089982 ; Train Acc : 0.104 ; Test Loss : 0.089981 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 255 ; Train Loss : 0.089986 ; Train Acc : 0.111 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 256 ; Train Loss : 0.089975 ; Train Acc : 0.105 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 257 ; Train Loss : 0.089975 ; Train Acc : 0.107 ; Test Loss : 0.089985 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 258 ; Train Loss : 0.089985 ; Train Acc : 0.105 ; Test Loss : 0.089992 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 259 ; Train Loss : 0.089984 ; Train Acc : 0.111 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 260 ; Train Loss : 0.090012 ; Train Acc : 0.100 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 261 ; Train Loss : 0.090000 ; Train Acc : 0.102 ; Test Loss : 0.090001 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 262 ; Train Loss : 0.090027 ; Train Acc : 0.102 ; Test Loss : 0.090013 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 263 ; Train Loss : 0.090003 ; Train Acc : 0.108 ; Test Loss : 0.089992 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 264 ; Train Loss : 0.089993 ; Train Acc : 0.110 ; Test Loss : 0.090001 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 265 ; Train Loss : 0.089995 ; Train Acc : 0.110 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 266 ; Train Loss : 0.089983 ; Train Acc : 0.111 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 267 ; Train Loss : 0.089978 ; Train Acc : 0.110 ; Test Loss : 0.089993 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 268 ; Train Loss : 0.089996 ; Train Acc : 0.107 ; Test Loss : 0.090013 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 269 ; Train Loss : 0.089998 ; Train Acc : 0.110 ; Test Loss : 0.089974 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 270 ; Train Loss : 0.089990 ; Train Acc : 0.104 ; Test Loss : 0.090016 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 271 ; Train Loss : 0.089990 ; Train Acc : 0.112 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 272 ; Train Loss : 0.089978 ; Train Acc : 0.111 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 273 ; Train Loss : 0.089983 ; Train Acc : 0.110 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 274 ; Train Loss : 0.089981 ; Train Acc : 0.106 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 275 ; Train Loss : 0.089993 ; Train Acc : 0.103 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 276 ; Train Loss : 0.090001 ; Train Acc : 0.105 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 277 ; Train Loss : 0.089974 ; Train Acc : 0.106 ; Test Loss : 0.090001 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 278 ; Train Loss : 0.090005 ; Train Acc : 0.112 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 279 ; Train Loss : 0.089979 ; Train Acc : 0.107 ; Test Loss : 0.090003 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 280 ; Train Loss : 0.089999 ; Train Acc : 0.098 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 281 ; Train Loss : 0.089991 ; Train Acc : 0.111 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 282 ; Train Loss : 0.089975 ; Train Acc : 0.107 ; Test Loss : 0.090011 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 283 ; Train Loss : 0.089980 ; Train Acc : 0.105 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 284 ; Train Loss : 0.090008 ; Train Acc : 0.111 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 285 ; Train Loss : 0.089999 ; Train Acc : 0.111 ; Test Loss : 0.090006 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 286 ; Train Loss : 0.089989 ; Train Acc : 0.108 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 287 ; Train Loss : 0.089993 ; Train Acc : 0.105 ; Test Loss : 0.089996 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 288 ; Train Loss : 0.089998 ; Train Acc : 0.099 ; Test Loss : 0.090002 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 289 ; Train Loss : 0.089980 ; Train Acc : 0.111 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 290 ; Train Loss : 0.089988 ; Train Acc : 0.107 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 291 ; Train Loss : 0.089980 ; Train Acc : 0.111 ; Test Loss : 0.089994 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 292 ; Train Loss : 0.089988 ; Train Acc : 0.107 ; Test Loss : 0.089981 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 293 ; Train Loss : 0.089991 ; Train Acc : 0.103 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 294 ; Train Loss : 0.089984 ; Train Acc : 0.111 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 295 ; Train Loss : 0.090006 ; Train Acc : 0.104 ; Test Loss : 0.090005 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 296 ; Train Loss : 0.089977 ; Train Acc : 0.111 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 297 ; Train Loss : 0.089985 ; Train Acc : 0.111 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 298 ; Train Loss : 0.090001 ; Train Acc : 0.102 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 299 ; Train Loss : 0.089977 ; Train Acc : 0.111 ; Test Loss : 0.089980 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 300 ; Train Loss : 0.089979 ; Train Acc : 0.108 ; Test Loss : 0.090006 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 301 ; Train Loss : 0.089974 ; Train Acc : 0.104 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 302 ; Train Loss : 0.089970 ; Train Acc : 0.108 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 303 ; Train Loss : 0.089989 ; Train Acc : 0.104 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 304 ; Train Loss : 0.089989 ; Train Acc : 0.104 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 305 ; Train Loss : 0.089973 ; Train Acc : 0.111 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 306 ; Train Loss : 0.089987 ; Train Acc : 0.098 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 307 ; Train Loss : 0.089996 ; Train Acc : 0.105 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 308 ; Train Loss : 0.089987 ; Train Acc : 0.106 ; Test Loss : 0.089988 ; Test Acc : 0.062 ; LR : 0.024\n",
      "Epoch : 309 ; Train Loss : 0.089993 ; Train Acc : 0.103 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 310 ; Train Loss : 0.089996 ; Train Acc : 0.105 ; Test Loss : 0.090001 ; Test Acc : 0.062 ; LR : 0.024\n",
      "Epoch : 311 ; Train Loss : 0.089990 ; Train Acc : 0.108 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 312 ; Train Loss : 0.090002 ; Train Acc : 0.105 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 313 ; Train Loss : 0.089987 ; Train Acc : 0.104 ; Test Loss : 0.090019 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 314 ; Train Loss : 0.089983 ; Train Acc : 0.111 ; Test Loss : 0.089974 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 315 ; Train Loss : 0.089973 ; Train Acc : 0.107 ; Test Loss : 0.090010 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 316 ; Train Loss : 0.089982 ; Train Acc : 0.106 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 317 ; Train Loss : 0.089997 ; Train Acc : 0.108 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 318 ; Train Loss : 0.089976 ; Train Acc : 0.104 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 319 ; Train Loss : 0.089994 ; Train Acc : 0.111 ; Test Loss : 0.089978 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 320 ; Train Loss : 0.089980 ; Train Acc : 0.100 ; Test Loss : 0.090000 ; Test Acc : 0.062 ; LR : 0.024\n",
      "Epoch : 321 ; Train Loss : 0.089970 ; Train Acc : 0.111 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 322 ; Train Loss : 0.089994 ; Train Acc : 0.112 ; Test Loss : 0.090004 ; Test Acc : 0.062 ; LR : 0.024\n",
      "Epoch : 323 ; Train Loss : 0.090010 ; Train Acc : 0.110 ; Test Loss : 0.090006 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 324 ; Train Loss : 0.089976 ; Train Acc : 0.105 ; Test Loss : 0.089991 ; Test Acc : 0.062 ; LR : 0.024\n",
      "Epoch : 325 ; Train Loss : 0.090022 ; Train Acc : 0.106 ; Test Loss : 0.090000 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 326 ; Train Loss : 0.089971 ; Train Acc : 0.107 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 327 ; Train Loss : 0.089997 ; Train Acc : 0.098 ; Test Loss : 0.090013 ; Test Acc : 0.062 ; LR : 0.024\n",
      "Epoch : 328 ; Train Loss : 0.089973 ; Train Acc : 0.109 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 329 ; Train Loss : 0.089979 ; Train Acc : 0.102 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 330 ; Train Loss : 0.089967 ; Train Acc : 0.111 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 331 ; Train Loss : 0.089984 ; Train Acc : 0.108 ; Test Loss : 0.090000 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 332 ; Train Loss : 0.089981 ; Train Acc : 0.111 ; Test Loss : 0.090010 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 333 ; Train Loss : 0.089976 ; Train Acc : 0.109 ; Test Loss : 0.089998 ; Test Acc : 0.062 ; LR : 0.024\n",
      "Epoch : 334 ; Train Loss : 0.089987 ; Train Acc : 0.104 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 335 ; Train Loss : 0.089996 ; Train Acc : 0.111 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 336 ; Train Loss : 0.089987 ; Train Acc : 0.098 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 337 ; Train Loss : 0.089961 ; Train Acc : 0.108 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 338 ; Train Loss : 0.089988 ; Train Acc : 0.111 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 339 ; Train Loss : 0.089990 ; Train Acc : 0.103 ; Test Loss : 0.090015 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 340 ; Train Loss : 0.089963 ; Train Acc : 0.110 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 341 ; Train Loss : 0.089988 ; Train Acc : 0.111 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 342 ; Train Loss : 0.089964 ; Train Acc : 0.102 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 343 ; Train Loss : 0.089982 ; Train Acc : 0.105 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 344 ; Train Loss : 0.090006 ; Train Acc : 0.105 ; Test Loss : 0.090013 ; Test Acc : 0.062 ; LR : 0.024\n",
      "Epoch : 345 ; Train Loss : 0.089997 ; Train Acc : 0.102 ; Test Loss : 0.089973 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 346 ; Train Loss : 0.090001 ; Train Acc : 0.102 ; Test Loss : 0.090011 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 347 ; Train Loss : 0.090023 ; Train Acc : 0.105 ; Test Loss : 0.090004 ; Test Acc : 0.062 ; LR : 0.024\n",
      "Epoch : 348 ; Train Loss : 0.090012 ; Train Acc : 0.111 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 349 ; Train Loss : 0.090005 ; Train Acc : 0.111 ; Test Loss : 0.090017 ; Test Acc : 0.062 ; LR : 0.024\n",
      "Epoch : 350 ; Train Loss : 0.089995 ; Train Acc : 0.108 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 351 ; Train Loss : 0.089987 ; Train Acc : 0.099 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 352 ; Train Loss : 0.089987 ; Train Acc : 0.111 ; Test Loss : 0.090003 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 353 ; Train Loss : 0.090001 ; Train Acc : 0.106 ; Test Loss : 0.090010 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 354 ; Train Loss : 0.089996 ; Train Acc : 0.102 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 355 ; Train Loss : 0.089972 ; Train Acc : 0.111 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 356 ; Train Loss : 0.089973 ; Train Acc : 0.111 ; Test Loss : 0.090011 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 357 ; Train Loss : 0.089977 ; Train Acc : 0.107 ; Test Loss : 0.089992 ; Test Acc : 0.062 ; LR : 0.022\n",
      "Epoch : 358 ; Train Loss : 0.089989 ; Train Acc : 0.108 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 359 ; Train Loss : 0.089988 ; Train Acc : 0.111 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 360 ; Train Loss : 0.089979 ; Train Acc : 0.110 ; Test Loss : 0.090023 ; Test Acc : 0.062 ; LR : 0.022\n",
      "Epoch : 361 ; Train Loss : 0.090002 ; Train Acc : 0.108 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 362 ; Train Loss : 0.089984 ; Train Acc : 0.102 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 363 ; Train Loss : 0.089970 ; Train Acc : 0.111 ; Test Loss : 0.090010 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 364 ; Train Loss : 0.089988 ; Train Acc : 0.102 ; Test Loss : 0.089994 ; Test Acc : 0.062 ; LR : 0.022\n",
      "Epoch : 365 ; Train Loss : 0.089983 ; Train Acc : 0.103 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 366 ; Train Loss : 0.089977 ; Train Acc : 0.107 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 367 ; Train Loss : 0.089977 ; Train Acc : 0.108 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 368 ; Train Loss : 0.089984 ; Train Acc : 0.111 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 369 ; Train Loss : 0.089978 ; Train Acc : 0.106 ; Test Loss : 0.090004 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 370 ; Train Loss : 0.089986 ; Train Acc : 0.112 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 371 ; Train Loss : 0.089983 ; Train Acc : 0.111 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 372 ; Train Loss : 0.089970 ; Train Acc : 0.111 ; Test Loss : 0.089991 ; Test Acc : 0.062 ; LR : 0.022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 373 ; Train Loss : 0.089972 ; Train Acc : 0.108 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 374 ; Train Loss : 0.089973 ; Train Acc : 0.109 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 375 ; Train Loss : 0.089984 ; Train Acc : 0.111 ; Test Loss : 0.089981 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 376 ; Train Loss : 0.089967 ; Train Acc : 0.110 ; Test Loss : 0.090017 ; Test Acc : 0.062 ; LR : 0.022\n",
      "Epoch : 377 ; Train Loss : 0.089990 ; Train Acc : 0.105 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 378 ; Train Loss : 0.089983 ; Train Acc : 0.108 ; Test Loss : 0.090002 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 379 ; Train Loss : 0.089984 ; Train Acc : 0.110 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 380 ; Train Loss : 0.089978 ; Train Acc : 0.101 ; Test Loss : 0.089997 ; Test Acc : 0.062 ; LR : 0.022\n",
      "Epoch : 381 ; Train Loss : 0.089986 ; Train Acc : 0.112 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 382 ; Train Loss : 0.089987 ; Train Acc : 0.111 ; Test Loss : 0.089987 ; Test Acc : 0.062 ; LR : 0.022\n",
      "Epoch : 383 ; Train Loss : 0.090004 ; Train Acc : 0.100 ; Test Loss : 0.089993 ; Test Acc : 0.062 ; LR : 0.022\n",
      "Epoch : 384 ; Train Loss : 0.089989 ; Train Acc : 0.110 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 385 ; Train Loss : 0.089989 ; Train Acc : 0.111 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 386 ; Train Loss : 0.089983 ; Train Acc : 0.105 ; Test Loss : 0.090006 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 387 ; Train Loss : 0.089978 ; Train Acc : 0.112 ; Test Loss : 0.089978 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 388 ; Train Loss : 0.090002 ; Train Acc : 0.111 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 389 ; Train Loss : 0.089971 ; Train Acc : 0.112 ; Test Loss : 0.090010 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 390 ; Train Loss : 0.089979 ; Train Acc : 0.098 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 391 ; Train Loss : 0.089972 ; Train Acc : 0.112 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 392 ; Train Loss : 0.089975 ; Train Acc : 0.108 ; Test Loss : 0.089993 ; Test Acc : 0.062 ; LR : 0.022\n",
      "Epoch : 393 ; Train Loss : 0.089993 ; Train Acc : 0.106 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 394 ; Train Loss : 0.089975 ; Train Acc : 0.106 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 395 ; Train Loss : 0.089987 ; Train Acc : 0.105 ; Test Loss : 0.089996 ; Test Acc : 0.062 ; LR : 0.022\n",
      "Epoch : 396 ; Train Loss : 0.089983 ; Train Acc : 0.106 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 397 ; Train Loss : 0.089968 ; Train Acc : 0.111 ; Test Loss : 0.089973 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 398 ; Train Loss : 0.089977 ; Train Acc : 0.102 ; Test Loss : 0.089997 ; Test Acc : 0.062 ; LR : 0.022\n",
      "Epoch : 399 ; Train Loss : 0.089987 ; Train Acc : 0.105 ; Test Loss : 0.090008 ; Test Acc : 0.062 ; LR : 0.022\n",
      "Epoch : 400 ; Train Loss : 0.089970 ; Train Acc : 0.111 ; Test Loss : 0.089973 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 401 ; Train Loss : 0.089997 ; Train Acc : 0.104 ; Test Loss : 0.090010 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 402 ; Train Loss : 0.089976 ; Train Acc : 0.108 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 403 ; Train Loss : 0.089997 ; Train Acc : 0.111 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 404 ; Train Loss : 0.089989 ; Train Acc : 0.100 ; Test Loss : 0.089993 ; Test Acc : 0.062 ; LR : 0.019\n",
      "Epoch : 405 ; Train Loss : 0.089976 ; Train Acc : 0.101 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 406 ; Train Loss : 0.089977 ; Train Acc : 0.111 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 407 ; Train Loss : 0.089974 ; Train Acc : 0.102 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 408 ; Train Loss : 0.089973 ; Train Acc : 0.111 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 409 ; Train Loss : 0.089970 ; Train Acc : 0.110 ; Test Loss : 0.090008 ; Test Acc : 0.062 ; LR : 0.019\n",
      "Epoch : 410 ; Train Loss : 0.089986 ; Train Acc : 0.101 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 411 ; Train Loss : 0.089976 ; Train Acc : 0.099 ; Test Loss : 0.089989 ; Test Acc : 0.062 ; LR : 0.019\n",
      "Epoch : 412 ; Train Loss : 0.089975 ; Train Acc : 0.106 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 413 ; Train Loss : 0.089965 ; Train Acc : 0.107 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 414 ; Train Loss : 0.089965 ; Train Acc : 0.106 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 415 ; Train Loss : 0.089972 ; Train Acc : 0.102 ; Test Loss : 0.089988 ; Test Acc : 0.062 ; LR : 0.019\n",
      "Epoch : 416 ; Train Loss : 0.089977 ; Train Acc : 0.106 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 417 ; Train Loss : 0.089966 ; Train Acc : 0.111 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 418 ; Train Loss : 0.089973 ; Train Acc : 0.109 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 419 ; Train Loss : 0.089975 ; Train Acc : 0.110 ; Test Loss : 0.089986 ; Test Acc : 0.062 ; LR : 0.019\n",
      "Epoch : 420 ; Train Loss : 0.089979 ; Train Acc : 0.107 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 421 ; Train Loss : 0.089974 ; Train Acc : 0.108 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 422 ; Train Loss : 0.089983 ; Train Acc : 0.106 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 423 ; Train Loss : 0.089973 ; Train Acc : 0.111 ; Test Loss : 0.089980 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 424 ; Train Loss : 0.089990 ; Train Acc : 0.111 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 425 ; Train Loss : 0.089979 ; Train Acc : 0.105 ; Test Loss : 0.089999 ; Test Acc : 0.062 ; LR : 0.019\n",
      "Epoch : 426 ; Train Loss : 0.089976 ; Train Acc : 0.105 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 427 ; Train Loss : 0.089971 ; Train Acc : 0.106 ; Test Loss : 0.089978 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 428 ; Train Loss : 0.089989 ; Train Acc : 0.104 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 429 ; Train Loss : 0.089981 ; Train Acc : 0.105 ; Test Loss : 0.090009 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 430 ; Train Loss : 0.089988 ; Train Acc : 0.108 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 431 ; Train Loss : 0.089973 ; Train Acc : 0.111 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 432 ; Train Loss : 0.089981 ; Train Acc : 0.111 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 433 ; Train Loss : 0.089981 ; Train Acc : 0.111 ; Test Loss : 0.090003 ; Test Acc : 0.062 ; LR : 0.019\n",
      "Epoch : 434 ; Train Loss : 0.089968 ; Train Acc : 0.106 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 435 ; Train Loss : 0.089968 ; Train Acc : 0.111 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 436 ; Train Loss : 0.089974 ; Train Acc : 0.111 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 437 ; Train Loss : 0.089970 ; Train Acc : 0.099 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 438 ; Train Loss : 0.089968 ; Train Acc : 0.107 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 439 ; Train Loss : 0.089982 ; Train Acc : 0.105 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 440 ; Train Loss : 0.089983 ; Train Acc : 0.107 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 441 ; Train Loss : 0.089992 ; Train Acc : 0.102 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 442 ; Train Loss : 0.089982 ; Train Acc : 0.111 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 443 ; Train Loss : 0.089986 ; Train Acc : 0.111 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 444 ; Train Loss : 0.089959 ; Train Acc : 0.105 ; Test Loss : 0.089989 ; Test Acc : 0.062 ; LR : 0.019\n",
      "Epoch : 445 ; Train Loss : 0.089975 ; Train Acc : 0.109 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 446 ; Train Loss : 0.089979 ; Train Acc : 0.106 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 447 ; Train Loss : 0.089981 ; Train Acc : 0.104 ; Test Loss : 0.090004 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 448 ; Train Loss : 0.089981 ; Train Acc : 0.107 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 449 ; Train Loss : 0.089975 ; Train Acc : 0.108 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 450 ; Train Loss : 0.089979 ; Train Acc : 0.111 ; Test Loss : 0.090009 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 451 ; Train Loss : 0.089971 ; Train Acc : 0.106 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 452 ; Train Loss : 0.089983 ; Train Acc : 0.104 ; Test Loss : 0.089986 ; Test Acc : 0.062 ; LR : 0.017\n",
      "Epoch : 453 ; Train Loss : 0.089986 ; Train Acc : 0.107 ; Test Loss : 0.090002 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 454 ; Train Loss : 0.089959 ; Train Acc : 0.107 ; Test Loss : 0.089989 ; Test Acc : 0.062 ; LR : 0.017\n",
      "Epoch : 455 ; Train Loss : 0.089968 ; Train Acc : 0.106 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 456 ; Train Loss : 0.089968 ; Train Acc : 0.111 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 457 ; Train Loss : 0.089972 ; Train Acc : 0.108 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 458 ; Train Loss : 0.089982 ; Train Acc : 0.105 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 459 ; Train Loss : 0.089963 ; Train Acc : 0.106 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 460 ; Train Loss : 0.089990 ; Train Acc : 0.107 ; Test Loss : 0.090014 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 461 ; Train Loss : 0.089967 ; Train Acc : 0.112 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 462 ; Train Loss : 0.089971 ; Train Acc : 0.111 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 463 ; Train Loss : 0.089974 ; Train Acc : 0.111 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 464 ; Train Loss : 0.089969 ; Train Acc : 0.101 ; Test Loss : 0.089987 ; Test Acc : 0.062 ; LR : 0.017\n",
      "Epoch : 465 ; Train Loss : 0.089967 ; Train Acc : 0.100 ; Test Loss : 0.090001 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 466 ; Train Loss : 0.089968 ; Train Acc : 0.111 ; Test Loss : 0.089980 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 467 ; Train Loss : 0.089984 ; Train Acc : 0.106 ; Test Loss : 0.089999 ; Test Acc : 0.062 ; LR : 0.017\n",
      "Epoch : 468 ; Train Loss : 0.089966 ; Train Acc : 0.107 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 469 ; Train Loss : 0.089969 ; Train Acc : 0.111 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 470 ; Train Loss : 0.089972 ; Train Acc : 0.111 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 471 ; Train Loss : 0.089983 ; Train Acc : 0.106 ; Test Loss : 0.090020 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 472 ; Train Loss : 0.089991 ; Train Acc : 0.099 ; Test Loss : 0.089990 ; Test Acc : 0.062 ; LR : 0.017\n",
      "Epoch : 473 ; Train Loss : 0.089969 ; Train Acc : 0.107 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 474 ; Train Loss : 0.089978 ; Train Acc : 0.106 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 475 ; Train Loss : 0.089977 ; Train Acc : 0.111 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 476 ; Train Loss : 0.089969 ; Train Acc : 0.111 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 477 ; Train Loss : 0.089992 ; Train Acc : 0.105 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 478 ; Train Loss : 0.089984 ; Train Acc : 0.113 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 479 ; Train Loss : 0.089968 ; Train Acc : 0.111 ; Test Loss : 0.090000 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 480 ; Train Loss : 0.089966 ; Train Acc : 0.104 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 481 ; Train Loss : 0.089988 ; Train Acc : 0.111 ; Test Loss : 0.090000 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 482 ; Train Loss : 0.089971 ; Train Acc : 0.111 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 483 ; Train Loss : 0.089987 ; Train Acc : 0.107 ; Test Loss : 0.090001 ; Test Acc : 0.062 ; LR : 0.017\n",
      "Epoch : 484 ; Train Loss : 0.089955 ; Train Acc : 0.112 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 485 ; Train Loss : 0.089961 ; Train Acc : 0.111 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 486 ; Train Loss : 0.089966 ; Train Acc : 0.111 ; Test Loss : 0.090003 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 487 ; Train Loss : 0.089974 ; Train Acc : 0.111 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 488 ; Train Loss : 0.089977 ; Train Acc : 0.111 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 489 ; Train Loss : 0.089970 ; Train Acc : 0.110 ; Test Loss : 0.089993 ; Test Acc : 0.062 ; LR : 0.017\n",
      "Epoch : 490 ; Train Loss : 0.089975 ; Train Acc : 0.109 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 491 ; Train Loss : 0.089966 ; Train Acc : 0.103 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 492 ; Train Loss : 0.089982 ; Train Acc : 0.111 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 493 ; Train Loss : 0.089970 ; Train Acc : 0.103 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 494 ; Train Loss : 0.089982 ; Train Acc : 0.107 ; Test Loss : 0.089994 ; Test Acc : 0.062 ; LR : 0.017\n",
      "Epoch : 495 ; Train Loss : 0.089980 ; Train Acc : 0.108 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 496 ; Train Loss : 0.089975 ; Train Acc : 0.111 ; Test Loss : 0.089981 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 497 ; Train Loss : 0.089979 ; Train Acc : 0.105 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 498 ; Train Loss : 0.089977 ; Train Acc : 0.111 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 499 ; Train Loss : 0.089973 ; Train Acc : 0.106 ; Test Loss : 0.089991 ; Test Acc : 0.062 ; LR : 0.017\n",
      "Epoch : 500 ; Train Loss : 0.089969 ; Train Acc : 0.112 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 501 ; Train Loss : 0.089966 ; Train Acc : 0.111 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 502 ; Train Loss : 0.089978 ; Train Acc : 0.111 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 503 ; Train Loss : 0.089970 ; Train Acc : 0.108 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 504 ; Train Loss : 0.089969 ; Train Acc : 0.111 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 505 ; Train Loss : 0.089972 ; Train Acc : 0.111 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 506 ; Train Loss : 0.089964 ; Train Acc : 0.108 ; Test Loss : 0.089994 ; Test Acc : 0.062 ; LR : 0.016\n",
      "Epoch : 507 ; Train Loss : 0.089976 ; Train Acc : 0.110 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 508 ; Train Loss : 0.089964 ; Train Acc : 0.111 ; Test Loss : 0.089980 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 509 ; Train Loss : 0.089964 ; Train Acc : 0.111 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 510 ; Train Loss : 0.089967 ; Train Acc : 0.111 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 511 ; Train Loss : 0.089955 ; Train Acc : 0.111 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 512 ; Train Loss : 0.089959 ; Train Acc : 0.111 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 513 ; Train Loss : 0.089968 ; Train Acc : 0.111 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 514 ; Train Loss : 0.089974 ; Train Acc : 0.102 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 515 ; Train Loss : 0.089987 ; Train Acc : 0.107 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 516 ; Train Loss : 0.089969 ; Train Acc : 0.111 ; Test Loss : 0.089979 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 517 ; Train Loss : 0.089964 ; Train Acc : 0.111 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 518 ; Train Loss : 0.089983 ; Train Acc : 0.111 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 519 ; Train Loss : 0.089968 ; Train Acc : 0.111 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 520 ; Train Loss : 0.089974 ; Train Acc : 0.110 ; Test Loss : 0.090010 ; Test Acc : 0.125 ; LR : 0.016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 521 ; Train Loss : 0.089959 ; Train Acc : 0.108 ; Test Loss : 0.089985 ; Test Acc : 0.062 ; LR : 0.016\n",
      "Epoch : 522 ; Train Loss : 0.089982 ; Train Acc : 0.108 ; Test Loss : 0.090010 ; Test Acc : 0.062 ; LR : 0.016\n",
      "Epoch : 523 ; Train Loss : 0.089966 ; Train Acc : 0.110 ; Test Loss : 0.089976 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 524 ; Train Loss : 0.089964 ; Train Acc : 0.111 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 525 ; Train Loss : 0.089979 ; Train Acc : 0.111 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 526 ; Train Loss : 0.089958 ; Train Acc : 0.111 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 527 ; Train Loss : 0.089979 ; Train Acc : 0.102 ; Test Loss : 0.090005 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 528 ; Train Loss : 0.089977 ; Train Acc : 0.112 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 529 ; Train Loss : 0.089970 ; Train Acc : 0.111 ; Test Loss : 0.089981 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 530 ; Train Loss : 0.089970 ; Train Acc : 0.109 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 531 ; Train Loss : 0.089972 ; Train Acc : 0.102 ; Test Loss : 0.090003 ; Test Acc : 0.062 ; LR : 0.016\n",
      "Epoch : 532 ; Train Loss : 0.089964 ; Train Acc : 0.111 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 533 ; Train Loss : 0.089984 ; Train Acc : 0.101 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 534 ; Train Loss : 0.089967 ; Train Acc : 0.111 ; Test Loss : 0.089981 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 535 ; Train Loss : 0.089965 ; Train Acc : 0.108 ; Test Loss : 0.089981 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 536 ; Train Loss : 0.089981 ; Train Acc : 0.111 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 537 ; Train Loss : 0.089969 ; Train Acc : 0.111 ; Test Loss : 0.089979 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 538 ; Train Loss : 0.089977 ; Train Acc : 0.111 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 539 ; Train Loss : 0.089959 ; Train Acc : 0.111 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 540 ; Train Loss : 0.089978 ; Train Acc : 0.111 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 541 ; Train Loss : 0.089990 ; Train Acc : 0.103 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 542 ; Train Loss : 0.089966 ; Train Acc : 0.107 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 543 ; Train Loss : 0.089977 ; Train Acc : 0.111 ; Test Loss : 0.089981 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 544 ; Train Loss : 0.089971 ; Train Acc : 0.107 ; Test Loss : 0.090003 ; Test Acc : 0.062 ; LR : 0.016\n",
      "Epoch : 545 ; Train Loss : 0.089962 ; Train Acc : 0.106 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 546 ; Train Loss : 0.089979 ; Train Acc : 0.111 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 547 ; Train Loss : 0.089964 ; Train Acc : 0.105 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 548 ; Train Loss : 0.089961 ; Train Acc : 0.111 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 549 ; Train Loss : 0.089971 ; Train Acc : 0.108 ; Test Loss : 0.090003 ; Test Acc : 0.062 ; LR : 0.016\n",
      "Epoch : 550 ; Train Loss : 0.089976 ; Train Acc : 0.108 ; Test Loss : 0.089991 ; Test Acc : 0.062 ; LR : 0.016\n",
      "Epoch : 551 ; Train Loss : 0.089979 ; Train Acc : 0.107 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 552 ; Train Loss : 0.089953 ; Train Acc : 0.107 ; Test Loss : 0.089986 ; Test Acc : 0.062 ; LR : 0.014\n",
      "Epoch : 553 ; Train Loss : 0.089961 ; Train Acc : 0.107 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 554 ; Train Loss : 0.089966 ; Train Acc : 0.111 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 555 ; Train Loss : 0.089966 ; Train Acc : 0.111 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 556 ; Train Loss : 0.089971 ; Train Acc : 0.110 ; Test Loss : 0.090006 ; Test Acc : 0.062 ; LR : 0.014\n",
      "Epoch : 557 ; Train Loss : 0.089984 ; Train Acc : 0.104 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 558 ; Train Loss : 0.089983 ; Train Acc : 0.112 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 559 ; Train Loss : 0.089961 ; Train Acc : 0.105 ; Test Loss : 0.089984 ; Test Acc : 0.062 ; LR : 0.014\n",
      "Epoch : 560 ; Train Loss : 0.089959 ; Train Acc : 0.107 ; Test Loss : 0.090000 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 561 ; Train Loss : 0.089970 ; Train Acc : 0.108 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 562 ; Train Loss : 0.089964 ; Train Acc : 0.111 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 563 ; Train Loss : 0.089977 ; Train Acc : 0.110 ; Test Loss : 0.090005 ; Test Acc : 0.062 ; LR : 0.014\n",
      "Epoch : 564 ; Train Loss : 0.089958 ; Train Acc : 0.109 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 565 ; Train Loss : 0.089973 ; Train Acc : 0.111 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 566 ; Train Loss : 0.089974 ; Train Acc : 0.110 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 567 ; Train Loss : 0.089969 ; Train Acc : 0.107 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 568 ; Train Loss : 0.089975 ; Train Acc : 0.104 ; Test Loss : 0.089998 ; Test Acc : 0.062 ; LR : 0.014\n",
      "Epoch : 569 ; Train Loss : 0.089962 ; Train Acc : 0.104 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 570 ; Train Loss : 0.089971 ; Train Acc : 0.103 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 571 ; Train Loss : 0.089969 ; Train Acc : 0.111 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 572 ; Train Loss : 0.089973 ; Train Acc : 0.111 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 573 ; Train Loss : 0.089973 ; Train Acc : 0.111 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 574 ; Train Loss : 0.089971 ; Train Acc : 0.106 ; Test Loss : 0.089994 ; Test Acc : 0.062 ; LR : 0.014\n",
      "Epoch : 575 ; Train Loss : 0.089965 ; Train Acc : 0.105 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 576 ; Train Loss : 0.089972 ; Train Acc : 0.111 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 577 ; Train Loss : 0.089979 ; Train Acc : 0.111 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 578 ; Train Loss : 0.089969 ; Train Acc : 0.104 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 579 ; Train Loss : 0.089968 ; Train Acc : 0.111 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 580 ; Train Loss : 0.089979 ; Train Acc : 0.111 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 581 ; Train Loss : 0.089978 ; Train Acc : 0.111 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 582 ; Train Loss : 0.089958 ; Train Acc : 0.111 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 583 ; Train Loss : 0.089961 ; Train Acc : 0.111 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 584 ; Train Loss : 0.089965 ; Train Acc : 0.107 ; Test Loss : 0.090008 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 585 ; Train Loss : 0.089971 ; Train Acc : 0.111 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 586 ; Train Loss : 0.089965 ; Train Acc : 0.103 ; Test Loss : 0.089979 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 587 ; Train Loss : 0.089956 ; Train Acc : 0.111 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 588 ; Train Loss : 0.089979 ; Train Acc : 0.111 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 589 ; Train Loss : 0.089970 ; Train Acc : 0.101 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 590 ; Train Loss : 0.089983 ; Train Acc : 0.111 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 591 ; Train Loss : 0.089965 ; Train Acc : 0.101 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 592 ; Train Loss : 0.089966 ; Train Acc : 0.112 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 593 ; Train Loss : 0.089957 ; Train Acc : 0.111 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 594 ; Train Loss : 0.089963 ; Train Acc : 0.103 ; Test Loss : 0.089999 ; Test Acc : 0.062 ; LR : 0.014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 595 ; Train Loss : 0.089970 ; Train Acc : 0.111 ; Test Loss : 0.089978 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 596 ; Train Loss : 0.089966 ; Train Acc : 0.111 ; Test Loss : 0.090002 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 597 ; Train Loss : 0.089966 ; Train Acc : 0.111 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 598 ; Train Loss : 0.089964 ; Train Acc : 0.111 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 599 ; Train Loss : 0.089975 ; Train Acc : 0.110 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 600 ; Train Loss : 0.089980 ; Train Acc : 0.109 ; Test Loss : 0.089988 ; Test Acc : 0.062 ; LR : 0.014\n",
      "\n",
      "Hidden Neurons : 1 ; Train Loss : 0.089953 ; Train Acc : 0.108 ; Test Loss : 0.089989 ; Test Acc : 0.062\n",
      "\n",
      "\n",
      "Epoch : 1 ; Train Loss : 0.125712 ; Train Acc : 0.111 ; Test Loss : 0.103978 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 2 ; Train Loss : 0.094750 ; Train Acc : 0.095 ; Test Loss : 0.092434 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 3 ; Train Loss : 0.090967 ; Train Acc : 0.104 ; Test Loss : 0.090433 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 4 ; Train Loss : 0.090300 ; Train Acc : 0.105 ; Test Loss : 0.090069 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 5 ; Train Loss : 0.090035 ; Train Acc : 0.106 ; Test Loss : 0.090002 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 6 ; Train Loss : 0.090002 ; Train Acc : 0.106 ; Test Loss : 0.089989 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 7 ; Train Loss : 0.090036 ; Train Acc : 0.101 ; Test Loss : 0.090010 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 8 ; Train Loss : 0.090029 ; Train Acc : 0.101 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 9 ; Train Loss : 0.090019 ; Train Acc : 0.111 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 10 ; Train Loss : 0.090032 ; Train Acc : 0.106 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 11 ; Train Loss : 0.090018 ; Train Acc : 0.111 ; Test Loss : 0.089990 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 12 ; Train Loss : 0.089995 ; Train Acc : 0.107 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 13 ; Train Loss : 0.090024 ; Train Acc : 0.104 ; Test Loss : 0.089998 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 14 ; Train Loss : 0.089996 ; Train Acc : 0.111 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 15 ; Train Loss : 0.090032 ; Train Acc : 0.102 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 16 ; Train Loss : 0.090008 ; Train Acc : 0.098 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 17 ; Train Loss : 0.089995 ; Train Acc : 0.111 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 18 ; Train Loss : 0.090019 ; Train Acc : 0.105 ; Test Loss : 0.090004 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 19 ; Train Loss : 0.090005 ; Train Acc : 0.110 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 20 ; Train Loss : 0.090023 ; Train Acc : 0.107 ; Test Loss : 0.089999 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 21 ; Train Loss : 0.090046 ; Train Acc : 0.110 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 22 ; Train Loss : 0.090032 ; Train Acc : 0.108 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 23 ; Train Loss : 0.090011 ; Train Acc : 0.108 ; Test Loss : 0.090002 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 24 ; Train Loss : 0.090010 ; Train Acc : 0.105 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 25 ; Train Loss : 0.089993 ; Train Acc : 0.100 ; Test Loss : 0.090005 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 26 ; Train Loss : 0.090015 ; Train Acc : 0.105 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 27 ; Train Loss : 0.089989 ; Train Acc : 0.102 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 28 ; Train Loss : 0.090005 ; Train Acc : 0.105 ; Test Loss : 0.090004 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 29 ; Train Loss : 0.089996 ; Train Acc : 0.108 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 30 ; Train Loss : 0.090000 ; Train Acc : 0.101 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 31 ; Train Loss : 0.089999 ; Train Acc : 0.108 ; Test Loss : 0.089997 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 32 ; Train Loss : 0.090015 ; Train Acc : 0.108 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 33 ; Train Loss : 0.090015 ; Train Acc : 0.109 ; Test Loss : 0.089998 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 34 ; Train Loss : 0.090016 ; Train Acc : 0.097 ; Test Loss : 0.089996 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 35 ; Train Loss : 0.090013 ; Train Acc : 0.110 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 36 ; Train Loss : 0.090030 ; Train Acc : 0.101 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 37 ; Train Loss : 0.090039 ; Train Acc : 0.107 ; Test Loss : 0.089975 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 38 ; Train Loss : 0.090049 ; Train Acc : 0.102 ; Test Loss : 0.090001 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 39 ; Train Loss : 0.090011 ; Train Acc : 0.106 ; Test Loss : 0.090004 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 40 ; Train Loss : 0.089999 ; Train Acc : 0.102 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 41 ; Train Loss : 0.090001 ; Train Acc : 0.104 ; Test Loss : 0.090002 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 42 ; Train Loss : 0.090025 ; Train Acc : 0.111 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 43 ; Train Loss : 0.090007 ; Train Acc : 0.108 ; Test Loss : 0.090005 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 44 ; Train Loss : 0.090018 ; Train Acc : 0.103 ; Test Loss : 0.089986 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 45 ; Train Loss : 0.090024 ; Train Acc : 0.106 ; Test Loss : 0.089999 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 46 ; Train Loss : 0.090014 ; Train Acc : 0.108 ; Test Loss : 0.090003 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 47 ; Train Loss : 0.090010 ; Train Acc : 0.106 ; Test Loss : 0.089979 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 48 ; Train Loss : 0.089998 ; Train Acc : 0.111 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 49 ; Train Loss : 0.090000 ; Train Acc : 0.102 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 50 ; Train Loss : 0.090015 ; Train Acc : 0.111 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 51 ; Train Loss : 0.090023 ; Train Acc : 0.105 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 52 ; Train Loss : 0.090012 ; Train Acc : 0.101 ; Test Loss : 0.089987 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 53 ; Train Loss : 0.090012 ; Train Acc : 0.111 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 54 ; Train Loss : 0.090004 ; Train Acc : 0.105 ; Test Loss : 0.090004 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 55 ; Train Loss : 0.089973 ; Train Acc : 0.109 ; Test Loss : 0.089975 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 56 ; Train Loss : 0.090019 ; Train Acc : 0.111 ; Test Loss : 0.090005 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 57 ; Train Loss : 0.090037 ; Train Acc : 0.104 ; Test Loss : 0.090005 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 58 ; Train Loss : 0.090007 ; Train Acc : 0.109 ; Test Loss : 0.090003 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 59 ; Train Loss : 0.090006 ; Train Acc : 0.109 ; Test Loss : 0.089988 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 60 ; Train Loss : 0.090023 ; Train Acc : 0.103 ; Test Loss : 0.090006 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 61 ; Train Loss : 0.090017 ; Train Acc : 0.104 ; Test Loss : 0.089979 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 62 ; Train Loss : 0.090007 ; Train Acc : 0.111 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 63 ; Train Loss : 0.089993 ; Train Acc : 0.106 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 64 ; Train Loss : 0.089994 ; Train Acc : 0.105 ; Test Loss : 0.090023 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 65 ; Train Loss : 0.089991 ; Train Acc : 0.107 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 66 ; Train Loss : 0.090020 ; Train Acc : 0.111 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 67 ; Train Loss : 0.090017 ; Train Acc : 0.113 ; Test Loss : 0.089992 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 68 ; Train Loss : 0.089993 ; Train Acc : 0.106 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 69 ; Train Loss : 0.089990 ; Train Acc : 0.109 ; Test Loss : 0.090000 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 70 ; Train Loss : 0.090001 ; Train Acc : 0.107 ; Test Loss : 0.090015 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 71 ; Train Loss : 0.090016 ; Train Acc : 0.105 ; Test Loss : 0.090004 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 72 ; Train Loss : 0.090005 ; Train Acc : 0.106 ; Test Loss : 0.090000 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 73 ; Train Loss : 0.089997 ; Train Acc : 0.103 ; Test Loss : 0.089976 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 74 ; Train Loss : 0.090009 ; Train Acc : 0.104 ; Test Loss : 0.090001 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 75 ; Train Loss : 0.090018 ; Train Acc : 0.109 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 76 ; Train Loss : 0.090007 ; Train Acc : 0.102 ; Test Loss : 0.089999 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 77 ; Train Loss : 0.090013 ; Train Acc : 0.102 ; Test Loss : 0.090008 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 78 ; Train Loss : 0.090015 ; Train Acc : 0.104 ; Test Loss : 0.089993 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 79 ; Train Loss : 0.089989 ; Train Acc : 0.102 ; Test Loss : 0.090003 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 80 ; Train Loss : 0.089986 ; Train Acc : 0.103 ; Test Loss : 0.089992 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 81 ; Train Loss : 0.090027 ; Train Acc : 0.105 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 82 ; Train Loss : 0.090013 ; Train Acc : 0.101 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 83 ; Train Loss : 0.089985 ; Train Acc : 0.107 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 84 ; Train Loss : 0.090016 ; Train Acc : 0.105 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 85 ; Train Loss : 0.089987 ; Train Acc : 0.111 ; Test Loss : 0.089994 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 86 ; Train Loss : 0.090014 ; Train Acc : 0.100 ; Test Loss : 0.089980 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 87 ; Train Loss : 0.090003 ; Train Acc : 0.098 ; Test Loss : 0.090002 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 88 ; Train Loss : 0.089984 ; Train Acc : 0.108 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 89 ; Train Loss : 0.090001 ; Train Acc : 0.106 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 90 ; Train Loss : 0.090029 ; Train Acc : 0.101 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 91 ; Train Loss : 0.090055 ; Train Acc : 0.101 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 92 ; Train Loss : 0.090009 ; Train Acc : 0.111 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 93 ; Train Loss : 0.089997 ; Train Acc : 0.108 ; Test Loss : 0.090008 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 94 ; Train Loss : 0.090030 ; Train Acc : 0.101 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 95 ; Train Loss : 0.090077 ; Train Acc : 0.109 ; Test Loss : 0.090008 ; Test Acc : 0.062 ; LR : 0.041\n",
      "Epoch : 96 ; Train Loss : 0.090024 ; Train Acc : 0.100 ; Test Loss : 0.090013 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 97 ; Train Loss : 0.090006 ; Train Acc : 0.103 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 98 ; Train Loss : 0.090039 ; Train Acc : 0.103 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 99 ; Train Loss : 0.089985 ; Train Acc : 0.102 ; Test Loss : 0.090003 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 100 ; Train Loss : 0.089998 ; Train Acc : 0.111 ; Test Loss : 0.090001 ; Test Acc : 0.125 ; LR : 0.041\n",
      "Epoch : 101 ; Train Loss : 0.089991 ; Train Acc : 0.105 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 102 ; Train Loss : 0.090018 ; Train Acc : 0.098 ; Test Loss : 0.090010 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 103 ; Train Loss : 0.090023 ; Train Acc : 0.102 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 104 ; Train Loss : 0.090002 ; Train Acc : 0.098 ; Test Loss : 0.090005 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 105 ; Train Loss : 0.089997 ; Train Acc : 0.108 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 106 ; Train Loss : 0.090022 ; Train Acc : 0.111 ; Test Loss : 0.090004 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 107 ; Train Loss : 0.090003 ; Train Acc : 0.101 ; Test Loss : 0.090000 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 108 ; Train Loss : 0.090027 ; Train Acc : 0.110 ; Test Loss : 0.090010 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 109 ; Train Loss : 0.090022 ; Train Acc : 0.106 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 110 ; Train Loss : 0.090020 ; Train Acc : 0.095 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 111 ; Train Loss : 0.090026 ; Train Acc : 0.108 ; Test Loss : 0.090009 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 112 ; Train Loss : 0.090015 ; Train Acc : 0.097 ; Test Loss : 0.089976 ; Test Acc : 0.062 ; LR : 0.036\n",
      "Epoch : 113 ; Train Loss : 0.090015 ; Train Acc : 0.102 ; Test Loss : 0.090015 ; Test Acc : 0.062 ; LR : 0.036\n",
      "Epoch : 114 ; Train Loss : 0.090024 ; Train Acc : 0.107 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 115 ; Train Loss : 0.090021 ; Train Acc : 0.111 ; Test Loss : 0.090002 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 116 ; Train Loss : 0.090021 ; Train Acc : 0.103 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 117 ; Train Loss : 0.090000 ; Train Acc : 0.111 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 118 ; Train Loss : 0.090003 ; Train Acc : 0.111 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 119 ; Train Loss : 0.090020 ; Train Acc : 0.111 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 120 ; Train Loss : 0.090028 ; Train Acc : 0.100 ; Test Loss : 0.090006 ; Test Acc : 0.062 ; LR : 0.036\n",
      "Epoch : 121 ; Train Loss : 0.090031 ; Train Acc : 0.112 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 122 ; Train Loss : 0.089993 ; Train Acc : 0.098 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 123 ; Train Loss : 0.089993 ; Train Acc : 0.105 ; Test Loss : 0.090026 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 124 ; Train Loss : 0.090006 ; Train Acc : 0.108 ; Test Loss : 0.089977 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 125 ; Train Loss : 0.089974 ; Train Acc : 0.104 ; Test Loss : 0.089993 ; Test Acc : 0.062 ; LR : 0.036\n",
      "Epoch : 126 ; Train Loss : 0.090002 ; Train Acc : 0.110 ; Test Loss : 0.090007 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 127 ; Train Loss : 0.089990 ; Train Acc : 0.099 ; Test Loss : 0.090000 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 128 ; Train Loss : 0.090019 ; Train Acc : 0.111 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 129 ; Train Loss : 0.089994 ; Train Acc : 0.098 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 130 ; Train Loss : 0.089996 ; Train Acc : 0.109 ; Test Loss : 0.090003 ; Test Acc : 0.062 ; LR : 0.036\n",
      "Epoch : 131 ; Train Loss : 0.090038 ; Train Acc : 0.101 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 132 ; Train Loss : 0.090030 ; Train Acc : 0.110 ; Test Loss : 0.090015 ; Test Acc : 0.062 ; LR : 0.036\n",
      "Epoch : 133 ; Train Loss : 0.090020 ; Train Acc : 0.106 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 134 ; Train Loss : 0.089996 ; Train Acc : 0.106 ; Test Loss : 0.090008 ; Test Acc : 0.062 ; LR : 0.036\n",
      "Epoch : 135 ; Train Loss : 0.089997 ; Train Acc : 0.110 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 136 ; Train Loss : 0.090021 ; Train Acc : 0.099 ; Test Loss : 0.089996 ; Test Acc : 0.062 ; LR : 0.036\n",
      "Epoch : 137 ; Train Loss : 0.090035 ; Train Acc : 0.105 ; Test Loss : 0.090001 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 138 ; Train Loss : 0.090011 ; Train Acc : 0.106 ; Test Loss : 0.089993 ; Test Acc : 0.062 ; LR : 0.036\n",
      "Epoch : 139 ; Train Loss : 0.089998 ; Train Acc : 0.104 ; Test Loss : 0.090009 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 140 ; Train Loss : 0.090007 ; Train Acc : 0.109 ; Test Loss : 0.090003 ; Test Acc : 0.062 ; LR : 0.036\n",
      "Epoch : 141 ; Train Loss : 0.089992 ; Train Acc : 0.108 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 142 ; Train Loss : 0.089988 ; Train Acc : 0.104 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 143 ; Train Loss : 0.090007 ; Train Acc : 0.108 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 144 ; Train Loss : 0.090008 ; Train Acc : 0.111 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 145 ; Train Loss : 0.090027 ; Train Acc : 0.101 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 146 ; Train Loss : 0.090015 ; Train Acc : 0.111 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 147 ; Train Loss : 0.090011 ; Train Acc : 0.105 ; Test Loss : 0.089997 ; Test Acc : 0.062 ; LR : 0.036\n",
      "Epoch : 148 ; Train Loss : 0.090009 ; Train Acc : 0.109 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 149 ; Train Loss : 0.090000 ; Train Acc : 0.104 ; Test Loss : 0.089993 ; Test Acc : 0.062 ; LR : 0.036\n",
      "Epoch : 150 ; Train Loss : 0.089981 ; Train Acc : 0.112 ; Test Loss : 0.089980 ; Test Acc : 0.125 ; LR : 0.036\n",
      "Epoch : 151 ; Train Loss : 0.089987 ; Train Acc : 0.108 ; Test Loss : 0.090005 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 152 ; Train Loss : 0.090005 ; Train Acc : 0.108 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 153 ; Train Loss : 0.090016 ; Train Acc : 0.105 ; Test Loss : 0.090004 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 154 ; Train Loss : 0.089988 ; Train Acc : 0.112 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 155 ; Train Loss : 0.089976 ; Train Acc : 0.111 ; Test Loss : 0.090001 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 156 ; Train Loss : 0.089992 ; Train Acc : 0.108 ; Test Loss : 0.089989 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 157 ; Train Loss : 0.089991 ; Train Acc : 0.105 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 158 ; Train Loss : 0.090009 ; Train Acc : 0.106 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 159 ; Train Loss : 0.090033 ; Train Acc : 0.112 ; Test Loss : 0.090004 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 160 ; Train Loss : 0.090012 ; Train Acc : 0.110 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 161 ; Train Loss : 0.090044 ; Train Acc : 0.107 ; Test Loss : 0.090002 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 162 ; Train Loss : 0.089989 ; Train Acc : 0.112 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 163 ; Train Loss : 0.090004 ; Train Acc : 0.110 ; Test Loss : 0.090006 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 164 ; Train Loss : 0.089974 ; Train Acc : 0.114 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 165 ; Train Loss : 0.089979 ; Train Acc : 0.111 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 166 ; Train Loss : 0.089990 ; Train Acc : 0.101 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 167 ; Train Loss : 0.089987 ; Train Acc : 0.111 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 168 ; Train Loss : 0.089986 ; Train Acc : 0.107 ; Test Loss : 0.090008 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 169 ; Train Loss : 0.090002 ; Train Acc : 0.105 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 170 ; Train Loss : 0.089990 ; Train Acc : 0.111 ; Test Loss : 0.090000 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 171 ; Train Loss : 0.090012 ; Train Acc : 0.102 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 172 ; Train Loss : 0.090001 ; Train Acc : 0.105 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 173 ; Train Loss : 0.090065 ; Train Acc : 0.105 ; Test Loss : 0.090015 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 174 ; Train Loss : 0.090022 ; Train Acc : 0.102 ; Test Loss : 0.089973 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 175 ; Train Loss : 0.089993 ; Train Acc : 0.109 ; Test Loss : 0.090005 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 176 ; Train Loss : 0.090009 ; Train Acc : 0.110 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 177 ; Train Loss : 0.089999 ; Train Acc : 0.096 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 178 ; Train Loss : 0.089989 ; Train Acc : 0.105 ; Test Loss : 0.090008 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 179 ; Train Loss : 0.089991 ; Train Acc : 0.107 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 180 ; Train Loss : 0.090017 ; Train Acc : 0.108 ; Test Loss : 0.090003 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 181 ; Train Loss : 0.089982 ; Train Acc : 0.101 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 182 ; Train Loss : 0.089981 ; Train Acc : 0.111 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 183 ; Train Loss : 0.089982 ; Train Acc : 0.101 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 184 ; Train Loss : 0.089994 ; Train Acc : 0.108 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 185 ; Train Loss : 0.090017 ; Train Acc : 0.102 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 186 ; Train Loss : 0.090001 ; Train Acc : 0.111 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 187 ; Train Loss : 0.090004 ; Train Acc : 0.103 ; Test Loss : 0.090016 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 188 ; Train Loss : 0.089990 ; Train Acc : 0.111 ; Test Loss : 0.089976 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 189 ; Train Loss : 0.090001 ; Train Acc : 0.106 ; Test Loss : 0.090011 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 190 ; Train Loss : 0.090006 ; Train Acc : 0.111 ; Test Loss : 0.089981 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 191 ; Train Loss : 0.090016 ; Train Acc : 0.097 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 192 ; Train Loss : 0.089988 ; Train Acc : 0.102 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 193 ; Train Loss : 0.090011 ; Train Acc : 0.111 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 194 ; Train Loss : 0.090024 ; Train Acc : 0.094 ; Test Loss : 0.089997 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 195 ; Train Loss : 0.089991 ; Train Acc : 0.104 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 196 ; Train Loss : 0.089980 ; Train Acc : 0.110 ; Test Loss : 0.090007 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 197 ; Train Loss : 0.090002 ; Train Acc : 0.106 ; Test Loss : 0.089979 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 198 ; Train Loss : 0.090048 ; Train Acc : 0.100 ; Test Loss : 0.090005 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 199 ; Train Loss : 0.090037 ; Train Acc : 0.107 ; Test Loss : 0.089981 ; Test Acc : 0.062 ; LR : 0.033\n",
      "Epoch : 200 ; Train Loss : 0.089992 ; Train Acc : 0.107 ; Test Loss : 0.090026 ; Test Acc : 0.125 ; LR : 0.033\n",
      "Epoch : 201 ; Train Loss : 0.089996 ; Train Acc : 0.111 ; Test Loss : 0.089977 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 202 ; Train Loss : 0.090002 ; Train Acc : 0.109 ; Test Loss : 0.090011 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 203 ; Train Loss : 0.089983 ; Train Acc : 0.108 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 204 ; Train Loss : 0.089993 ; Train Acc : 0.111 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 205 ; Train Loss : 0.089976 ; Train Acc : 0.105 ; Test Loss : 0.089990 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 206 ; Train Loss : 0.089994 ; Train Acc : 0.099 ; Test Loss : 0.089987 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 207 ; Train Loss : 0.090010 ; Train Acc : 0.104 ; Test Loss : 0.090007 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 208 ; Train Loss : 0.089979 ; Train Acc : 0.107 ; Test Loss : 0.089993 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 209 ; Train Loss : 0.089979 ; Train Acc : 0.109 ; Test Loss : 0.090000 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 210 ; Train Loss : 0.089996 ; Train Acc : 0.104 ; Test Loss : 0.090014 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 211 ; Train Loss : 0.089995 ; Train Acc : 0.111 ; Test Loss : 0.089974 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 212 ; Train Loss : 0.089981 ; Train Acc : 0.111 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 213 ; Train Loss : 0.089971 ; Train Acc : 0.108 ; Test Loss : 0.090000 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 214 ; Train Loss : 0.089991 ; Train Acc : 0.106 ; Test Loss : 0.090000 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 215 ; Train Loss : 0.090001 ; Train Acc : 0.110 ; Test Loss : 0.089979 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 216 ; Train Loss : 0.089995 ; Train Acc : 0.111 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 217 ; Train Loss : 0.089986 ; Train Acc : 0.099 ; Test Loss : 0.090019 ; Test Acc : 0.125 ; LR : 0.030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 218 ; Train Loss : 0.089981 ; Train Acc : 0.107 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 219 ; Train Loss : 0.090003 ; Train Acc : 0.102 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 220 ; Train Loss : 0.089970 ; Train Acc : 0.104 ; Test Loss : 0.089998 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 221 ; Train Loss : 0.089980 ; Train Acc : 0.105 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 222 ; Train Loss : 0.090001 ; Train Acc : 0.105 ; Test Loss : 0.090003 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 223 ; Train Loss : 0.089994 ; Train Acc : 0.108 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 224 ; Train Loss : 0.089991 ; Train Acc : 0.109 ; Test Loss : 0.090009 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 225 ; Train Loss : 0.089996 ; Train Acc : 0.100 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 226 ; Train Loss : 0.089990 ; Train Acc : 0.103 ; Test Loss : 0.090006 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 227 ; Train Loss : 0.090000 ; Train Acc : 0.103 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 228 ; Train Loss : 0.090023 ; Train Acc : 0.111 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 229 ; Train Loss : 0.089989 ; Train Acc : 0.108 ; Test Loss : 0.090010 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 230 ; Train Loss : 0.089971 ; Train Acc : 0.108 ; Test Loss : 0.089984 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 231 ; Train Loss : 0.089994 ; Train Acc : 0.105 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 232 ; Train Loss : 0.089995 ; Train Acc : 0.111 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 233 ; Train Loss : 0.090004 ; Train Acc : 0.105 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 234 ; Train Loss : 0.090010 ; Train Acc : 0.111 ; Test Loss : 0.090011 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 235 ; Train Loss : 0.089982 ; Train Acc : 0.104 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 236 ; Train Loss : 0.090008 ; Train Acc : 0.111 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 237 ; Train Loss : 0.090004 ; Train Acc : 0.105 ; Test Loss : 0.090003 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 238 ; Train Loss : 0.089996 ; Train Acc : 0.110 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 239 ; Train Loss : 0.089990 ; Train Acc : 0.112 ; Test Loss : 0.090002 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 240 ; Train Loss : 0.089985 ; Train Acc : 0.104 ; Test Loss : 0.089985 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 241 ; Train Loss : 0.090012 ; Train Acc : 0.111 ; Test Loss : 0.090004 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 242 ; Train Loss : 0.089984 ; Train Acc : 0.102 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 243 ; Train Loss : 0.089989 ; Train Acc : 0.104 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 244 ; Train Loss : 0.089970 ; Train Acc : 0.111 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 245 ; Train Loss : 0.089983 ; Train Acc : 0.108 ; Test Loss : 0.090008 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 246 ; Train Loss : 0.090000 ; Train Acc : 0.107 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 247 ; Train Loss : 0.089975 ; Train Acc : 0.101 ; Test Loss : 0.089986 ; Test Acc : 0.062 ; LR : 0.030\n",
      "Epoch : 248 ; Train Loss : 0.089990 ; Train Acc : 0.108 ; Test Loss : 0.089979 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 249 ; Train Loss : 0.089989 ; Train Acc : 0.111 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 250 ; Train Loss : 0.090004 ; Train Acc : 0.106 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.030\n",
      "Epoch : 251 ; Train Loss : 0.089987 ; Train Acc : 0.108 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 252 ; Train Loss : 0.089982 ; Train Acc : 0.111 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 253 ; Train Loss : 0.089994 ; Train Acc : 0.111 ; Test Loss : 0.090003 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 254 ; Train Loss : 0.089991 ; Train Acc : 0.100 ; Test Loss : 0.089986 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 255 ; Train Loss : 0.089997 ; Train Acc : 0.108 ; Test Loss : 0.089997 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 256 ; Train Loss : 0.089982 ; Train Acc : 0.103 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 257 ; Train Loss : 0.089996 ; Train Acc : 0.111 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 258 ; Train Loss : 0.089986 ; Train Acc : 0.104 ; Test Loss : 0.090002 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 259 ; Train Loss : 0.089972 ; Train Acc : 0.108 ; Test Loss : 0.089979 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 260 ; Train Loss : 0.089986 ; Train Acc : 0.104 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 261 ; Train Loss : 0.090003 ; Train Acc : 0.103 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 262 ; Train Loss : 0.089993 ; Train Acc : 0.101 ; Test Loss : 0.090002 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 263 ; Train Loss : 0.090000 ; Train Acc : 0.113 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 264 ; Train Loss : 0.089990 ; Train Acc : 0.108 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 265 ; Train Loss : 0.089984 ; Train Acc : 0.106 ; Test Loss : 0.089989 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 266 ; Train Loss : 0.090003 ; Train Acc : 0.106 ; Test Loss : 0.090005 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 267 ; Train Loss : 0.089974 ; Train Acc : 0.111 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 268 ; Train Loss : 0.089997 ; Train Acc : 0.098 ; Test Loss : 0.089980 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 269 ; Train Loss : 0.089969 ; Train Acc : 0.105 ; Test Loss : 0.090006 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 270 ; Train Loss : 0.089978 ; Train Acc : 0.105 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 271 ; Train Loss : 0.089969 ; Train Acc : 0.111 ; Test Loss : 0.089979 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 272 ; Train Loss : 0.089980 ; Train Acc : 0.099 ; Test Loss : 0.089990 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 273 ; Train Loss : 0.089984 ; Train Acc : 0.111 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 274 ; Train Loss : 0.090019 ; Train Acc : 0.111 ; Test Loss : 0.090003 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 275 ; Train Loss : 0.089989 ; Train Acc : 0.100 ; Test Loss : 0.090022 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 276 ; Train Loss : 0.089988 ; Train Acc : 0.102 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 277 ; Train Loss : 0.089992 ; Train Acc : 0.109 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 278 ; Train Loss : 0.089965 ; Train Acc : 0.111 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 279 ; Train Loss : 0.089994 ; Train Acc : 0.103 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 280 ; Train Loss : 0.089981 ; Train Acc : 0.110 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 281 ; Train Loss : 0.089970 ; Train Acc : 0.109 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 282 ; Train Loss : 0.089993 ; Train Acc : 0.111 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 283 ; Train Loss : 0.089981 ; Train Acc : 0.105 ; Test Loss : 0.090008 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 284 ; Train Loss : 0.089992 ; Train Acc : 0.108 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 285 ; Train Loss : 0.089995 ; Train Acc : 0.107 ; Test Loss : 0.090020 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 286 ; Train Loss : 0.090009 ; Train Acc : 0.110 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 287 ; Train Loss : 0.090006 ; Train Acc : 0.104 ; Test Loss : 0.090005 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 288 ; Train Loss : 0.090002 ; Train Acc : 0.103 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 289 ; Train Loss : 0.089997 ; Train Acc : 0.111 ; Test Loss : 0.090001 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 290 ; Train Loss : 0.089988 ; Train Acc : 0.107 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 291 ; Train Loss : 0.089991 ; Train Acc : 0.107 ; Test Loss : 0.090006 ; Test Acc : 0.125 ; LR : 0.027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 292 ; Train Loss : 0.089988 ; Train Acc : 0.114 ; Test Loss : 0.089977 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 293 ; Train Loss : 0.089989 ; Train Acc : 0.107 ; Test Loss : 0.090020 ; Test Acc : 0.062 ; LR : 0.027\n",
      "Epoch : 294 ; Train Loss : 0.089993 ; Train Acc : 0.112 ; Test Loss : 0.089979 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 295 ; Train Loss : 0.090016 ; Train Acc : 0.101 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 296 ; Train Loss : 0.089983 ; Train Acc : 0.111 ; Test Loss : 0.090013 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 297 ; Train Loss : 0.090001 ; Train Acc : 0.104 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 298 ; Train Loss : 0.090009 ; Train Acc : 0.109 ; Test Loss : 0.090030 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 299 ; Train Loss : 0.090000 ; Train Acc : 0.103 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 300 ; Train Loss : 0.090007 ; Train Acc : 0.103 ; Test Loss : 0.090007 ; Test Acc : 0.125 ; LR : 0.027\n",
      "Epoch : 301 ; Train Loss : 0.089979 ; Train Acc : 0.103 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 302 ; Train Loss : 0.089966 ; Train Acc : 0.103 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 303 ; Train Loss : 0.089981 ; Train Acc : 0.111 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 304 ; Train Loss : 0.089974 ; Train Acc : 0.107 ; Test Loss : 0.090007 ; Test Acc : 0.062 ; LR : 0.024\n",
      "Epoch : 305 ; Train Loss : 0.089990 ; Train Acc : 0.111 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 306 ; Train Loss : 0.089972 ; Train Acc : 0.111 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 307 ; Train Loss : 0.089984 ; Train Acc : 0.103 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 308 ; Train Loss : 0.089975 ; Train Acc : 0.111 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 309 ; Train Loss : 0.089997 ; Train Acc : 0.106 ; Test Loss : 0.089995 ; Test Acc : 0.062 ; LR : 0.024\n",
      "Epoch : 310 ; Train Loss : 0.089962 ; Train Acc : 0.106 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 311 ; Train Loss : 0.089972 ; Train Acc : 0.111 ; Test Loss : 0.089978 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 312 ; Train Loss : 0.089977 ; Train Acc : 0.103 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 313 ; Train Loss : 0.089993 ; Train Acc : 0.107 ; Test Loss : 0.090000 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 314 ; Train Loss : 0.089983 ; Train Acc : 0.106 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 315 ; Train Loss : 0.089988 ; Train Acc : 0.106 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 316 ; Train Loss : 0.090013 ; Train Acc : 0.103 ; Test Loss : 0.090010 ; Test Acc : 0.062 ; LR : 0.024\n",
      "Epoch : 317 ; Train Loss : 0.089987 ; Train Acc : 0.109 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 318 ; Train Loss : 0.089994 ; Train Acc : 0.108 ; Test Loss : 0.089995 ; Test Acc : 0.062 ; LR : 0.024\n",
      "Epoch : 319 ; Train Loss : 0.089987 ; Train Acc : 0.107 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 320 ; Train Loss : 0.089994 ; Train Acc : 0.106 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 321 ; Train Loss : 0.089972 ; Train Acc : 0.111 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 322 ; Train Loss : 0.089986 ; Train Acc : 0.111 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 323 ; Train Loss : 0.089971 ; Train Acc : 0.103 ; Test Loss : 0.089980 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 324 ; Train Loss : 0.089999 ; Train Acc : 0.105 ; Test Loss : 0.089999 ; Test Acc : 0.062 ; LR : 0.024\n",
      "Epoch : 325 ; Train Loss : 0.089984 ; Train Acc : 0.103 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 326 ; Train Loss : 0.090010 ; Train Acc : 0.111 ; Test Loss : 0.090002 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 327 ; Train Loss : 0.089976 ; Train Acc : 0.103 ; Test Loss : 0.090009 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 328 ; Train Loss : 0.089973 ; Train Acc : 0.108 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 329 ; Train Loss : 0.089983 ; Train Acc : 0.105 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 330 ; Train Loss : 0.090002 ; Train Acc : 0.099 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 331 ; Train Loss : 0.089999 ; Train Acc : 0.111 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 332 ; Train Loss : 0.089996 ; Train Acc : 0.110 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 333 ; Train Loss : 0.089974 ; Train Acc : 0.109 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 334 ; Train Loss : 0.089989 ; Train Acc : 0.107 ; Test Loss : 0.089999 ; Test Acc : 0.062 ; LR : 0.024\n",
      "Epoch : 335 ; Train Loss : 0.089987 ; Train Acc : 0.101 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 336 ; Train Loss : 0.089988 ; Train Acc : 0.106 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 337 ; Train Loss : 0.089976 ; Train Acc : 0.105 ; Test Loss : 0.090000 ; Test Acc : 0.062 ; LR : 0.024\n",
      "Epoch : 338 ; Train Loss : 0.089979 ; Train Acc : 0.104 ; Test Loss : 0.089976 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 339 ; Train Loss : 0.089984 ; Train Acc : 0.111 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 340 ; Train Loss : 0.089977 ; Train Acc : 0.111 ; Test Loss : 0.090006 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 341 ; Train Loss : 0.089978 ; Train Acc : 0.106 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 342 ; Train Loss : 0.089985 ; Train Acc : 0.104 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 343 ; Train Loss : 0.089986 ; Train Acc : 0.111 ; Test Loss : 0.089973 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 344 ; Train Loss : 0.090001 ; Train Acc : 0.105 ; Test Loss : 0.090011 ; Test Acc : 0.062 ; LR : 0.024\n",
      "Epoch : 345 ; Train Loss : 0.089991 ; Train Acc : 0.107 ; Test Loss : 0.089979 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 346 ; Train Loss : 0.089999 ; Train Acc : 0.108 ; Test Loss : 0.090011 ; Test Acc : 0.062 ; LR : 0.024\n",
      "Epoch : 347 ; Train Loss : 0.089998 ; Train Acc : 0.102 ; Test Loss : 0.089975 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 348 ; Train Loss : 0.089993 ; Train Acc : 0.111 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 349 ; Train Loss : 0.089983 ; Train Acc : 0.110 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 350 ; Train Loss : 0.089979 ; Train Acc : 0.110 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.024\n",
      "Epoch : 351 ; Train Loss : 0.089983 ; Train Acc : 0.108 ; Test Loss : 0.089991 ; Test Acc : 0.062 ; LR : 0.022\n",
      "Epoch : 352 ; Train Loss : 0.089964 ; Train Acc : 0.114 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 353 ; Train Loss : 0.089965 ; Train Acc : 0.111 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 354 ; Train Loss : 0.089987 ; Train Acc : 0.107 ; Test Loss : 0.090009 ; Test Acc : 0.062 ; LR : 0.022\n",
      "Epoch : 355 ; Train Loss : 0.089973 ; Train Acc : 0.105 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 356 ; Train Loss : 0.089979 ; Train Acc : 0.108 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 357 ; Train Loss : 0.089989 ; Train Acc : 0.111 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 358 ; Train Loss : 0.089969 ; Train Acc : 0.108 ; Test Loss : 0.090002 ; Test Acc : 0.062 ; LR : 0.022\n",
      "Epoch : 359 ; Train Loss : 0.089997 ; Train Acc : 0.109 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 360 ; Train Loss : 0.090025 ; Train Acc : 0.104 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 361 ; Train Loss : 0.089968 ; Train Acc : 0.111 ; Test Loss : 0.089992 ; Test Acc : 0.062 ; LR : 0.022\n",
      "Epoch : 362 ; Train Loss : 0.089979 ; Train Acc : 0.108 ; Test Loss : 0.090003 ; Test Acc : 0.062 ; LR : 0.022\n",
      "Epoch : 363 ; Train Loss : 0.089972 ; Train Acc : 0.109 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 364 ; Train Loss : 0.089974 ; Train Acc : 0.111 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 365 ; Train Loss : 0.089970 ; Train Acc : 0.111 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 366 ; Train Loss : 0.089978 ; Train Acc : 0.102 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 367 ; Train Loss : 0.089956 ; Train Acc : 0.107 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 368 ; Train Loss : 0.089964 ; Train Acc : 0.108 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 369 ; Train Loss : 0.089989 ; Train Acc : 0.108 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 370 ; Train Loss : 0.089981 ; Train Acc : 0.111 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 371 ; Train Loss : 0.089972 ; Train Acc : 0.105 ; Test Loss : 0.090014 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 372 ; Train Loss : 0.089973 ; Train Acc : 0.116 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 373 ; Train Loss : 0.090014 ; Train Acc : 0.111 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 374 ; Train Loss : 0.089980 ; Train Acc : 0.115 ; Test Loss : 0.090016 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 375 ; Train Loss : 0.089990 ; Train Acc : 0.104 ; Test Loss : 0.089997 ; Test Acc : 0.062 ; LR : 0.022\n",
      "Epoch : 376 ; Train Loss : 0.089976 ; Train Acc : 0.108 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 377 ; Train Loss : 0.090009 ; Train Acc : 0.111 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 378 ; Train Loss : 0.089968 ; Train Acc : 0.106 ; Test Loss : 0.089983 ; Test Acc : 0.062 ; LR : 0.022\n",
      "Epoch : 379 ; Train Loss : 0.089973 ; Train Acc : 0.108 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 380 ; Train Loss : 0.090000 ; Train Acc : 0.108 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 381 ; Train Loss : 0.090010 ; Train Acc : 0.098 ; Test Loss : 0.089992 ; Test Acc : 0.062 ; LR : 0.022\n",
      "Epoch : 382 ; Train Loss : 0.090010 ; Train Acc : 0.102 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 383 ; Train Loss : 0.089962 ; Train Acc : 0.111 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 384 ; Train Loss : 0.089978 ; Train Acc : 0.111 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 385 ; Train Loss : 0.090002 ; Train Acc : 0.103 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 386 ; Train Loss : 0.089972 ; Train Acc : 0.111 ; Test Loss : 0.089979 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 387 ; Train Loss : 0.089971 ; Train Acc : 0.110 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 388 ; Train Loss : 0.089989 ; Train Acc : 0.100 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 389 ; Train Loss : 0.089988 ; Train Acc : 0.110 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 390 ; Train Loss : 0.089997 ; Train Acc : 0.106 ; Test Loss : 0.090021 ; Test Acc : 0.062 ; LR : 0.022\n",
      "Epoch : 391 ; Train Loss : 0.089991 ; Train Acc : 0.109 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 392 ; Train Loss : 0.089971 ; Train Acc : 0.111 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 393 ; Train Loss : 0.089990 ; Train Acc : 0.100 ; Test Loss : 0.090008 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 394 ; Train Loss : 0.089979 ; Train Acc : 0.111 ; Test Loss : 0.089975 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 395 ; Train Loss : 0.089977 ; Train Acc : 0.108 ; Test Loss : 0.090001 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 396 ; Train Loss : 0.089995 ; Train Acc : 0.105 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 397 ; Train Loss : 0.089987 ; Train Acc : 0.105 ; Test Loss : 0.090016 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 398 ; Train Loss : 0.089986 ; Train Acc : 0.105 ; Test Loss : 0.089978 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 399 ; Train Loss : 0.089990 ; Train Acc : 0.111 ; Test Loss : 0.090011 ; Test Acc : 0.125 ; LR : 0.022\n",
      "Epoch : 400 ; Train Loss : 0.089960 ; Train Acc : 0.112 ; Test Loss : 0.089988 ; Test Acc : 0.062 ; LR : 0.022\n",
      "Epoch : 401 ; Train Loss : 0.089986 ; Train Acc : 0.099 ; Test Loss : 0.090010 ; Test Acc : 0.062 ; LR : 0.019\n",
      "Epoch : 402 ; Train Loss : 0.089979 ; Train Acc : 0.103 ; Test Loss : 0.089976 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 403 ; Train Loss : 0.089972 ; Train Acc : 0.103 ; Test Loss : 0.090009 ; Test Acc : 0.062 ; LR : 0.019\n",
      "Epoch : 404 ; Train Loss : 0.089976 ; Train Acc : 0.101 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 405 ; Train Loss : 0.089989 ; Train Acc : 0.111 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 406 ; Train Loss : 0.089968 ; Train Acc : 0.108 ; Test Loss : 0.089985 ; Test Acc : 0.062 ; LR : 0.019\n",
      "Epoch : 407 ; Train Loss : 0.089971 ; Train Acc : 0.108 ; Test Loss : 0.089980 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 408 ; Train Loss : 0.089988 ; Train Acc : 0.103 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 409 ; Train Loss : 0.089966 ; Train Acc : 0.111 ; Test Loss : 0.090002 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 410 ; Train Loss : 0.089979 ; Train Acc : 0.107 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 411 ; Train Loss : 0.089962 ; Train Acc : 0.107 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 412 ; Train Loss : 0.089965 ; Train Acc : 0.104 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 413 ; Train Loss : 0.089966 ; Train Acc : 0.111 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 414 ; Train Loss : 0.089986 ; Train Acc : 0.099 ; Test Loss : 0.089994 ; Test Acc : 0.062 ; LR : 0.019\n",
      "Epoch : 415 ; Train Loss : 0.089966 ; Train Acc : 0.110 ; Test Loss : 0.089981 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 416 ; Train Loss : 0.089985 ; Train Acc : 0.111 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 417 ; Train Loss : 0.089988 ; Train Acc : 0.111 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 418 ; Train Loss : 0.089966 ; Train Acc : 0.111 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 419 ; Train Loss : 0.089980 ; Train Acc : 0.107 ; Test Loss : 0.090007 ; Test Acc : 0.062 ; LR : 0.019\n",
      "Epoch : 420 ; Train Loss : 0.089971 ; Train Acc : 0.110 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 421 ; Train Loss : 0.089970 ; Train Acc : 0.111 ; Test Loss : 0.089981 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 422 ; Train Loss : 0.089978 ; Train Acc : 0.105 ; Test Loss : 0.090001 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 423 ; Train Loss : 0.089966 ; Train Acc : 0.104 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 424 ; Train Loss : 0.089972 ; Train Acc : 0.107 ; Test Loss : 0.089997 ; Test Acc : 0.062 ; LR : 0.019\n",
      "Epoch : 425 ; Train Loss : 0.089986 ; Train Acc : 0.108 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 426 ; Train Loss : 0.089995 ; Train Acc : 0.112 ; Test Loss : 0.089995 ; Test Acc : 0.062 ; LR : 0.019\n",
      "Epoch : 427 ; Train Loss : 0.090006 ; Train Acc : 0.096 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 428 ; Train Loss : 0.089973 ; Train Acc : 0.111 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 429 ; Train Loss : 0.089982 ; Train Acc : 0.105 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 430 ; Train Loss : 0.089977 ; Train Acc : 0.107 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 431 ; Train Loss : 0.089979 ; Train Acc : 0.111 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 432 ; Train Loss : 0.089984 ; Train Acc : 0.111 ; Test Loss : 0.090007 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 433 ; Train Loss : 0.089975 ; Train Acc : 0.105 ; Test Loss : 0.089987 ; Test Acc : 0.062 ; LR : 0.019\n",
      "Epoch : 434 ; Train Loss : 0.089984 ; Train Acc : 0.111 ; Test Loss : 0.089979 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 435 ; Train Loss : 0.089979 ; Train Acc : 0.111 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 436 ; Train Loss : 0.089984 ; Train Acc : 0.109 ; Test Loss : 0.090007 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 437 ; Train Loss : 0.089976 ; Train Acc : 0.105 ; Test Loss : 0.089979 ; Test Acc : 0.062 ; LR : 0.019\n",
      "Epoch : 438 ; Train Loss : 0.089991 ; Train Acc : 0.110 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 439 ; Train Loss : 0.089981 ; Train Acc : 0.105 ; Test Loss : 0.090004 ; Test Acc : 0.125 ; LR : 0.019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 440 ; Train Loss : 0.089982 ; Train Acc : 0.111 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 441 ; Train Loss : 0.089982 ; Train Acc : 0.098 ; Test Loss : 0.090001 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 442 ; Train Loss : 0.089987 ; Train Acc : 0.103 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 443 ; Train Loss : 0.089988 ; Train Acc : 0.105 ; Test Loss : 0.089985 ; Test Acc : 0.062 ; LR : 0.019\n",
      "Epoch : 444 ; Train Loss : 0.089975 ; Train Acc : 0.110 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 445 ; Train Loss : 0.089965 ; Train Acc : 0.111 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 446 ; Train Loss : 0.089974 ; Train Acc : 0.102 ; Test Loss : 0.090017 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 447 ; Train Loss : 0.090002 ; Train Acc : 0.105 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 448 ; Train Loss : 0.089974 ; Train Acc : 0.110 ; Test Loss : 0.090012 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 449 ; Train Loss : 0.089993 ; Train Acc : 0.102 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 450 ; Train Loss : 0.089988 ; Train Acc : 0.107 ; Test Loss : 0.089980 ; Test Acc : 0.125 ; LR : 0.019\n",
      "Epoch : 451 ; Train Loss : 0.089977 ; Train Acc : 0.111 ; Test Loss : 0.090001 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 452 ; Train Loss : 0.089978 ; Train Acc : 0.111 ; Test Loss : 0.089990 ; Test Acc : 0.062 ; LR : 0.017\n",
      "Epoch : 453 ; Train Loss : 0.089991 ; Train Acc : 0.103 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 454 ; Train Loss : 0.089972 ; Train Acc : 0.109 ; Test Loss : 0.090010 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 455 ; Train Loss : 0.089975 ; Train Acc : 0.105 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 456 ; Train Loss : 0.089976 ; Train Acc : 0.111 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 457 ; Train Loss : 0.089978 ; Train Acc : 0.105 ; Test Loss : 0.089996 ; Test Acc : 0.062 ; LR : 0.017\n",
      "Epoch : 458 ; Train Loss : 0.089969 ; Train Acc : 0.105 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 459 ; Train Loss : 0.089981 ; Train Acc : 0.108 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 460 ; Train Loss : 0.089959 ; Train Acc : 0.113 ; Test Loss : 0.089985 ; Test Acc : 0.062 ; LR : 0.017\n",
      "Epoch : 461 ; Train Loss : 0.089977 ; Train Acc : 0.108 ; Test Loss : 0.089995 ; Test Acc : 0.062 ; LR : 0.017\n",
      "Epoch : 462 ; Train Loss : 0.089978 ; Train Acc : 0.108 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 463 ; Train Loss : 0.089973 ; Train Acc : 0.111 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 464 ; Train Loss : 0.089973 ; Train Acc : 0.111 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 465 ; Train Loss : 0.089976 ; Train Acc : 0.110 ; Test Loss : 0.089993 ; Test Acc : 0.062 ; LR : 0.017\n",
      "Epoch : 466 ; Train Loss : 0.089997 ; Train Acc : 0.108 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 467 ; Train Loss : 0.089954 ; Train Acc : 0.112 ; Test Loss : 0.089985 ; Test Acc : 0.062 ; LR : 0.017\n",
      "Epoch : 468 ; Train Loss : 0.089969 ; Train Acc : 0.108 ; Test Loss : 0.089989 ; Test Acc : 0.062 ; LR : 0.017\n",
      "Epoch : 469 ; Train Loss : 0.089967 ; Train Acc : 0.110 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 470 ; Train Loss : 0.089972 ; Train Acc : 0.105 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 471 ; Train Loss : 0.089967 ; Train Acc : 0.106 ; Test Loss : 0.089988 ; Test Acc : 0.062 ; LR : 0.017\n",
      "Epoch : 472 ; Train Loss : 0.089953 ; Train Acc : 0.108 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 473 ; Train Loss : 0.089972 ; Train Acc : 0.111 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 474 ; Train Loss : 0.089963 ; Train Acc : 0.111 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 475 ; Train Loss : 0.089969 ; Train Acc : 0.110 ; Test Loss : 0.089988 ; Test Acc : 0.062 ; LR : 0.017\n",
      "Epoch : 476 ; Train Loss : 0.089964 ; Train Acc : 0.108 ; Test Loss : 0.089996 ; Test Acc : 0.062 ; LR : 0.017\n",
      "Epoch : 477 ; Train Loss : 0.089979 ; Train Acc : 0.105 ; Test Loss : 0.090004 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 478 ; Train Loss : 0.089979 ; Train Acc : 0.106 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 479 ; Train Loss : 0.089973 ; Train Acc : 0.111 ; Test Loss : 0.089971 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 480 ; Train Loss : 0.089969 ; Train Acc : 0.111 ; Test Loss : 0.090004 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 481 ; Train Loss : 0.089975 ; Train Acc : 0.106 ; Test Loss : 0.090000 ; Test Acc : 0.062 ; LR : 0.017\n",
      "Epoch : 482 ; Train Loss : 0.089971 ; Train Acc : 0.106 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 483 ; Train Loss : 0.089969 ; Train Acc : 0.111 ; Test Loss : 0.090004 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 484 ; Train Loss : 0.089989 ; Train Acc : 0.110 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 485 ; Train Loss : 0.089990 ; Train Acc : 0.111 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 486 ; Train Loss : 0.089983 ; Train Acc : 0.111 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 487 ; Train Loss : 0.089971 ; Train Acc : 0.111 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 488 ; Train Loss : 0.089971 ; Train Acc : 0.103 ; Test Loss : 0.089998 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 489 ; Train Loss : 0.089962 ; Train Acc : 0.107 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 490 ; Train Loss : 0.089969 ; Train Acc : 0.105 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 491 ; Train Loss : 0.089963 ; Train Acc : 0.111 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 492 ; Train Loss : 0.089977 ; Train Acc : 0.111 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 493 ; Train Loss : 0.089975 ; Train Acc : 0.107 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 494 ; Train Loss : 0.089973 ; Train Acc : 0.108 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 495 ; Train Loss : 0.089992 ; Train Acc : 0.104 ; Test Loss : 0.089995 ; Test Acc : 0.062 ; LR : 0.017\n",
      "Epoch : 496 ; Train Loss : 0.089967 ; Train Acc : 0.111 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 497 ; Train Loss : 0.089999 ; Train Acc : 0.111 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 498 ; Train Loss : 0.089966 ; Train Acc : 0.110 ; Test Loss : 0.089985 ; Test Acc : 0.062 ; LR : 0.017\n",
      "Epoch : 499 ; Train Loss : 0.089971 ; Train Acc : 0.108 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 500 ; Train Loss : 0.089981 ; Train Acc : 0.106 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.017\n",
      "Epoch : 501 ; Train Loss : 0.089984 ; Train Acc : 0.111 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 502 ; Train Loss : 0.089959 ; Train Acc : 0.108 ; Test Loss : 0.089989 ; Test Acc : 0.062 ; LR : 0.016\n",
      "Epoch : 503 ; Train Loss : 0.089977 ; Train Acc : 0.107 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 504 ; Train Loss : 0.089960 ; Train Acc : 0.111 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 505 ; Train Loss : 0.089981 ; Train Acc : 0.105 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 506 ; Train Loss : 0.089970 ; Train Acc : 0.108 ; Test Loss : 0.089975 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 507 ; Train Loss : 0.089971 ; Train Acc : 0.111 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 508 ; Train Loss : 0.089981 ; Train Acc : 0.111 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 509 ; Train Loss : 0.089971 ; Train Acc : 0.103 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 510 ; Train Loss : 0.089971 ; Train Acc : 0.111 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 511 ; Train Loss : 0.089967 ; Train Acc : 0.111 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 512 ; Train Loss : 0.089969 ; Train Acc : 0.111 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 513 ; Train Loss : 0.089971 ; Train Acc : 0.111 ; Test Loss : 0.089981 ; Test Acc : 0.125 ; LR : 0.016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 514 ; Train Loss : 0.089968 ; Train Acc : 0.111 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 515 ; Train Loss : 0.089978 ; Train Acc : 0.105 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 516 ; Train Loss : 0.089977 ; Train Acc : 0.105 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 517 ; Train Loss : 0.089965 ; Train Acc : 0.101 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 518 ; Train Loss : 0.089973 ; Train Acc : 0.111 ; Test Loss : 0.089985 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 519 ; Train Loss : 0.089965 ; Train Acc : 0.111 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 520 ; Train Loss : 0.089971 ; Train Acc : 0.102 ; Test Loss : 0.089997 ; Test Acc : 0.062 ; LR : 0.016\n",
      "Epoch : 521 ; Train Loss : 0.089966 ; Train Acc : 0.111 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 522 ; Train Loss : 0.089971 ; Train Acc : 0.111 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 523 ; Train Loss : 0.089986 ; Train Acc : 0.111 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 524 ; Train Loss : 0.089972 ; Train Acc : 0.107 ; Test Loss : 0.089987 ; Test Acc : 0.062 ; LR : 0.016\n",
      "Epoch : 525 ; Train Loss : 0.089970 ; Train Acc : 0.104 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 526 ; Train Loss : 0.089987 ; Train Acc : 0.104 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 527 ; Train Loss : 0.089994 ; Train Acc : 0.111 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 528 ; Train Loss : 0.089974 ; Train Acc : 0.105 ; Test Loss : 0.089985 ; Test Acc : 0.062 ; LR : 0.016\n",
      "Epoch : 529 ; Train Loss : 0.089966 ; Train Acc : 0.111 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 530 ; Train Loss : 0.089969 ; Train Acc : 0.111 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 531 ; Train Loss : 0.089963 ; Train Acc : 0.106 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 532 ; Train Loss : 0.089971 ; Train Acc : 0.107 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 533 ; Train Loss : 0.089970 ; Train Acc : 0.108 ; Test Loss : 0.089994 ; Test Acc : 0.062 ; LR : 0.016\n",
      "Epoch : 534 ; Train Loss : 0.089974 ; Train Acc : 0.108 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 535 ; Train Loss : 0.089963 ; Train Acc : 0.105 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 536 ; Train Loss : 0.089987 ; Train Acc : 0.111 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 537 ; Train Loss : 0.089966 ; Train Acc : 0.106 ; Test Loss : 0.089996 ; Test Acc : 0.062 ; LR : 0.016\n",
      "Epoch : 538 ; Train Loss : 0.089975 ; Train Acc : 0.108 ; Test Loss : 0.090004 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 539 ; Train Loss : 0.089986 ; Train Acc : 0.111 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 540 ; Train Loss : 0.089981 ; Train Acc : 0.104 ; Test Loss : 0.089983 ; Test Acc : 0.062 ; LR : 0.016\n",
      "Epoch : 541 ; Train Loss : 0.089975 ; Train Acc : 0.107 ; Test Loss : 0.090000 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 542 ; Train Loss : 0.089980 ; Train Acc : 0.110 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 543 ; Train Loss : 0.089969 ; Train Acc : 0.105 ; Test Loss : 0.089997 ; Test Acc : 0.062 ; LR : 0.016\n",
      "Epoch : 544 ; Train Loss : 0.089970 ; Train Acc : 0.108 ; Test Loss : 0.090002 ; Test Acc : 0.062 ; LR : 0.016\n",
      "Epoch : 545 ; Train Loss : 0.089983 ; Train Acc : 0.110 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 546 ; Train Loss : 0.089979 ; Train Acc : 0.104 ; Test Loss : 0.089981 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 547 ; Train Loss : 0.089988 ; Train Acc : 0.105 ; Test Loss : 0.089988 ; Test Acc : 0.062 ; LR : 0.016\n",
      "Epoch : 548 ; Train Loss : 0.089972 ; Train Acc : 0.107 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 549 ; Train Loss : 0.089976 ; Train Acc : 0.106 ; Test Loss : 0.090002 ; Test Acc : 0.062 ; LR : 0.016\n",
      "Epoch : 550 ; Train Loss : 0.089967 ; Train Acc : 0.110 ; Test Loss : 0.089980 ; Test Acc : 0.125 ; LR : 0.016\n",
      "Epoch : 551 ; Train Loss : 0.089962 ; Train Acc : 0.105 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 552 ; Train Loss : 0.089972 ; Train Acc : 0.111 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 553 ; Train Loss : 0.089969 ; Train Acc : 0.108 ; Test Loss : 0.089994 ; Test Acc : 0.062 ; LR : 0.014\n",
      "Epoch : 554 ; Train Loss : 0.089977 ; Train Acc : 0.108 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 555 ; Train Loss : 0.089962 ; Train Acc : 0.111 ; Test Loss : 0.089987 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 556 ; Train Loss : 0.089967 ; Train Acc : 0.111 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 557 ; Train Loss : 0.089981 ; Train Acc : 0.111 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 558 ; Train Loss : 0.089977 ; Train Acc : 0.110 ; Test Loss : 0.089996 ; Test Acc : 0.062 ; LR : 0.014\n",
      "Epoch : 559 ; Train Loss : 0.089968 ; Train Acc : 0.107 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 560 ; Train Loss : 0.089965 ; Train Acc : 0.111 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 561 ; Train Loss : 0.089967 ; Train Acc : 0.111 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 562 ; Train Loss : 0.089979 ; Train Acc : 0.111 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 563 ; Train Loss : 0.089968 ; Train Acc : 0.104 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 564 ; Train Loss : 0.089968 ; Train Acc : 0.104 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 565 ; Train Loss : 0.089973 ; Train Acc : 0.102 ; Test Loss : 0.089981 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 566 ; Train Loss : 0.089989 ; Train Acc : 0.111 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 567 ; Train Loss : 0.089965 ; Train Acc : 0.108 ; Test Loss : 0.090002 ; Test Acc : 0.062 ; LR : 0.014\n",
      "Epoch : 568 ; Train Loss : 0.089973 ; Train Acc : 0.102 ; Test Loss : 0.089992 ; Test Acc : 0.062 ; LR : 0.014\n",
      "Epoch : 569 ; Train Loss : 0.089984 ; Train Acc : 0.103 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 570 ; Train Loss : 0.089973 ; Train Acc : 0.111 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 571 ; Train Loss : 0.089964 ; Train Acc : 0.111 ; Test Loss : 0.089997 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 572 ; Train Loss : 0.089976 ; Train Acc : 0.111 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 573 ; Train Loss : 0.089968 ; Train Acc : 0.111 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 574 ; Train Loss : 0.089978 ; Train Acc : 0.111 ; Test Loss : 0.089978 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 575 ; Train Loss : 0.089961 ; Train Acc : 0.105 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 576 ; Train Loss : 0.089963 ; Train Acc : 0.105 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 577 ; Train Loss : 0.089969 ; Train Acc : 0.111 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 578 ; Train Loss : 0.089971 ; Train Acc : 0.111 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 579 ; Train Loss : 0.089964 ; Train Acc : 0.111 ; Test Loss : 0.089986 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 580 ; Train Loss : 0.089978 ; Train Acc : 0.111 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 581 ; Train Loss : 0.089964 ; Train Acc : 0.111 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 582 ; Train Loss : 0.089977 ; Train Acc : 0.102 ; Test Loss : 0.090008 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 583 ; Train Loss : 0.089978 ; Train Acc : 0.104 ; Test Loss : 0.089982 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 584 ; Train Loss : 0.089950 ; Train Acc : 0.111 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 585 ; Train Loss : 0.089983 ; Train Acc : 0.108 ; Test Loss : 0.090002 ; Test Acc : 0.062 ; LR : 0.014\n",
      "Epoch : 586 ; Train Loss : 0.089966 ; Train Acc : 0.102 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 587 ; Train Loss : 0.089963 ; Train Acc : 0.108 ; Test Loss : 0.089983 ; Test Acc : 0.125 ; LR : 0.014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 588 ; Train Loss : 0.089961 ; Train Acc : 0.111 ; Test Loss : 0.089989 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 589 ; Train Loss : 0.089970 ; Train Acc : 0.108 ; Test Loss : 0.089998 ; Test Acc : 0.062 ; LR : 0.014\n",
      "Epoch : 590 ; Train Loss : 0.089974 ; Train Acc : 0.102 ; Test Loss : 0.089985 ; Test Acc : 0.062 ; LR : 0.014\n",
      "Epoch : 591 ; Train Loss : 0.089977 ; Train Acc : 0.112 ; Test Loss : 0.089988 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 592 ; Train Loss : 0.089971 ; Train Acc : 0.111 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 593 ; Train Loss : 0.089958 ; Train Acc : 0.111 ; Test Loss : 0.089980 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 594 ; Train Loss : 0.089961 ; Train Acc : 0.102 ; Test Loss : 0.089995 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 595 ; Train Loss : 0.089958 ; Train Acc : 0.108 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 596 ; Train Loss : 0.089971 ; Train Acc : 0.111 ; Test Loss : 0.089984 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 597 ; Train Loss : 0.089958 ; Train Acc : 0.111 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 598 ; Train Loss : 0.089968 ; Train Acc : 0.111 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.014\n",
      "Epoch : 599 ; Train Loss : 0.089968 ; Train Acc : 0.110 ; Test Loss : 0.089988 ; Test Acc : 0.062 ; LR : 0.014\n",
      "Epoch : 600 ; Train Loss : 0.089964 ; Train Acc : 0.106 ; Test Loss : 0.089994 ; Test Acc : 0.125 ; LR : 0.014\n",
      "\n",
      "Hidden Neurons : 5 ; Train Loss : 0.089982 ; Train Acc : 0.111 ; Test Loss : 0.089983 ; Test Acc : 0.125\n",
      "\n",
      "\n",
      "Epoch : 1 ; Train Loss : 0.126481 ; Train Acc : 0.096 ; Test Loss : 0.093946 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 2 ; Train Loss : 0.091781 ; Train Acc : 0.104 ; Test Loss : 0.090727 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 3 ; Train Loss : 0.090439 ; Train Acc : 0.100 ; Test Loss : 0.090034 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 4 ; Train Loss : 0.090054 ; Train Acc : 0.106 ; Test Loss : 0.090051 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 5 ; Train Loss : 0.090026 ; Train Acc : 0.107 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 6 ; Train Loss : 0.090026 ; Train Acc : 0.103 ; Test Loss : 0.090000 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 7 ; Train Loss : 0.090046 ; Train Acc : 0.099 ; Test Loss : 0.090008 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 8 ; Train Loss : 0.090024 ; Train Acc : 0.106 ; Test Loss : 0.089996 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 9 ; Train Loss : 0.090037 ; Train Acc : 0.102 ; Test Loss : 0.090016 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 10 ; Train Loss : 0.090044 ; Train Acc : 0.106 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 11 ; Train Loss : 0.089994 ; Train Acc : 0.101 ; Test Loss : 0.090010 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 12 ; Train Loss : 0.090009 ; Train Acc : 0.107 ; Test Loss : 0.089992 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 13 ; Train Loss : 0.090015 ; Train Acc : 0.099 ; Test Loss : 0.089993 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 14 ; Train Loss : 0.090030 ; Train Acc : 0.108 ; Test Loss : 0.090016 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 15 ; Train Loss : 0.090024 ; Train Acc : 0.108 ; Test Loss : 0.090009 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 16 ; Train Loss : 0.090009 ; Train Acc : 0.102 ; Test Loss : 0.089990 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 17 ; Train Loss : 0.090005 ; Train Acc : 0.106 ; Test Loss : 0.090023 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 18 ; Train Loss : 0.090007 ; Train Acc : 0.103 ; Test Loss : 0.089991 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 19 ; Train Loss : 0.090015 ; Train Acc : 0.099 ; Test Loss : 0.090020 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 20 ; Train Loss : 0.089988 ; Train Acc : 0.104 ; Test Loss : 0.090003 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 21 ; Train Loss : 0.090024 ; Train Acc : 0.100 ; Test Loss : 0.090016 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 22 ; Train Loss : 0.090042 ; Train Acc : 0.101 ; Test Loss : 0.090004 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 23 ; Train Loss : 0.090011 ; Train Acc : 0.105 ; Test Loss : 0.090004 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 24 ; Train Loss : 0.090018 ; Train Acc : 0.103 ; Test Loss : 0.090008 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 25 ; Train Loss : 0.089999 ; Train Acc : 0.108 ; Test Loss : 0.090018 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 26 ; Train Loss : 0.090012 ; Train Acc : 0.108 ; Test Loss : 0.090007 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 27 ; Train Loss : 0.089999 ; Train Acc : 0.098 ; Test Loss : 0.090006 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 28 ; Train Loss : 0.089993 ; Train Acc : 0.105 ; Test Loss : 0.090020 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 29 ; Train Loss : 0.089993 ; Train Acc : 0.101 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 30 ; Train Loss : 0.090006 ; Train Acc : 0.103 ; Test Loss : 0.090001 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 31 ; Train Loss : 0.090001 ; Train Acc : 0.108 ; Test Loss : 0.090016 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 32 ; Train Loss : 0.090002 ; Train Acc : 0.109 ; Test Loss : 0.090004 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 33 ; Train Loss : 0.090007 ; Train Acc : 0.098 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 34 ; Train Loss : 0.089997 ; Train Acc : 0.100 ; Test Loss : 0.090013 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 35 ; Train Loss : 0.089999 ; Train Acc : 0.111 ; Test Loss : 0.089999 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 36 ; Train Loss : 0.089973 ; Train Acc : 0.110 ; Test Loss : 0.090011 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 37 ; Train Loss : 0.089998 ; Train Acc : 0.105 ; Test Loss : 0.090005 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 38 ; Train Loss : 0.089995 ; Train Acc : 0.101 ; Test Loss : 0.089998 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 39 ; Train Loss : 0.089893 ; Train Acc : 0.114 ; Test Loss : 0.089823 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 40 ; Train Loss : 0.089448 ; Train Acc : 0.125 ; Test Loss : 0.088378 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 41 ; Train Loss : 0.087142 ; Train Acc : 0.162 ; Test Loss : 0.085773 ; Test Acc : 0.188 ; LR : 0.045\n",
      "Epoch : 42 ; Train Loss : 0.084920 ; Train Acc : 0.185 ; Test Loss : 0.083814 ; Test Acc : 0.188 ; LR : 0.045\n",
      "Epoch : 43 ; Train Loss : 0.083852 ; Train Acc : 0.191 ; Test Loss : 0.083158 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 44 ; Train Loss : 0.083528 ; Train Acc : 0.195 ; Test Loss : 0.082555 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 45 ; Train Loss : 0.083223 ; Train Acc : 0.200 ; Test Loss : 0.082398 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 46 ; Train Loss : 0.082574 ; Train Acc : 0.210 ; Test Loss : 0.081904 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 47 ; Train Loss : 0.082133 ; Train Acc : 0.215 ; Test Loss : 0.081755 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 48 ; Train Loss : 0.081938 ; Train Acc : 0.207 ; Test Loss : 0.081414 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 49 ; Train Loss : 0.081633 ; Train Acc : 0.201 ; Test Loss : 0.081242 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 50 ; Train Loss : 0.081549 ; Train Acc : 0.213 ; Test Loss : 0.081316 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 51 ; Train Loss : 0.081343 ; Train Acc : 0.207 ; Test Loss : 0.081023 ; Test Acc : 0.188 ; LR : 0.041\n",
      "Epoch : 52 ; Train Loss : 0.080901 ; Train Acc : 0.215 ; Test Loss : 0.080711 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 53 ; Train Loss : 0.080756 ; Train Acc : 0.209 ; Test Loss : 0.080708 ; Test Acc : 0.188 ; LR : 0.041\n",
      "Epoch : 54 ; Train Loss : 0.080419 ; Train Acc : 0.214 ; Test Loss : 0.080333 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 55 ; Train Loss : 0.080692 ; Train Acc : 0.213 ; Test Loss : 0.080244 ; Test Acc : 0.188 ; LR : 0.041\n",
      "Epoch : 56 ; Train Loss : 0.080477 ; Train Acc : 0.211 ; Test Loss : 0.080276 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 57 ; Train Loss : 0.080463 ; Train Acc : 0.215 ; Test Loss : 0.080154 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 58 ; Train Loss : 0.080362 ; Train Acc : 0.213 ; Test Loss : 0.080096 ; Test Acc : 0.188 ; LR : 0.041\n",
      "Epoch : 59 ; Train Loss : 0.080478 ; Train Acc : 0.209 ; Test Loss : 0.080069 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 60 ; Train Loss : 0.080399 ; Train Acc : 0.207 ; Test Loss : 0.080014 ; Test Acc : 0.188 ; LR : 0.041\n",
      "Epoch : 61 ; Train Loss : 0.080448 ; Train Acc : 0.204 ; Test Loss : 0.080030 ; Test Acc : 0.188 ; LR : 0.041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 62 ; Train Loss : 0.080314 ; Train Acc : 0.204 ; Test Loss : 0.080021 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 63 ; Train Loss : 0.080174 ; Train Acc : 0.211 ; Test Loss : 0.080059 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 64 ; Train Loss : 0.080377 ; Train Acc : 0.208 ; Test Loss : 0.079949 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 65 ; Train Loss : 0.080163 ; Train Acc : 0.211 ; Test Loss : 0.079946 ; Test Acc : 0.188 ; LR : 0.041\n",
      "Epoch : 66 ; Train Loss : 0.080341 ; Train Acc : 0.207 ; Test Loss : 0.079919 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 67 ; Train Loss : 0.080228 ; Train Acc : 0.214 ; Test Loss : 0.079969 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 68 ; Train Loss : 0.080028 ; Train Acc : 0.210 ; Test Loss : 0.079918 ; Test Acc : 0.188 ; LR : 0.041\n",
      "Epoch : 69 ; Train Loss : 0.080296 ; Train Acc : 0.215 ; Test Loss : 0.079958 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 70 ; Train Loss : 0.080252 ; Train Acc : 0.214 ; Test Loss : 0.079921 ; Test Acc : 0.188 ; LR : 0.041\n",
      "Epoch : 71 ; Train Loss : 0.080223 ; Train Acc : 0.212 ; Test Loss : 0.079874 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 72 ; Train Loss : 0.080227 ; Train Acc : 0.215 ; Test Loss : 0.079851 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 73 ; Train Loss : 0.080065 ; Train Acc : 0.208 ; Test Loss : 0.079848 ; Test Acc : 0.188 ; LR : 0.041\n",
      "Epoch : 74 ; Train Loss : 0.080289 ; Train Acc : 0.207 ; Test Loss : 0.079854 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 75 ; Train Loss : 0.080103 ; Train Acc : 0.205 ; Test Loss : 0.079818 ; Test Acc : 0.188 ; LR : 0.041\n",
      "Epoch : 76 ; Train Loss : 0.080004 ; Train Acc : 0.212 ; Test Loss : 0.079825 ; Test Acc : 0.188 ; LR : 0.041\n",
      "Epoch : 77 ; Train Loss : 0.080059 ; Train Acc : 0.213 ; Test Loss : 0.079881 ; Test Acc : 0.188 ; LR : 0.041\n",
      "Epoch : 78 ; Train Loss : 0.079954 ; Train Acc : 0.213 ; Test Loss : 0.079809 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 79 ; Train Loss : 0.079933 ; Train Acc : 0.213 ; Test Loss : 0.079811 ; Test Acc : 0.188 ; LR : 0.041\n",
      "Epoch : 80 ; Train Loss : 0.079694 ; Train Acc : 0.205 ; Test Loss : 0.079806 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 81 ; Train Loss : 0.079938 ; Train Acc : 0.207 ; Test Loss : 0.079784 ; Test Acc : 0.188 ; LR : 0.041\n",
      "Epoch : 82 ; Train Loss : 0.079957 ; Train Acc : 0.204 ; Test Loss : 0.079786 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 83 ; Train Loss : 0.080019 ; Train Acc : 0.205 ; Test Loss : 0.079834 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 84 ; Train Loss : 0.079985 ; Train Acc : 0.212 ; Test Loss : 0.079741 ; Test Acc : 0.188 ; LR : 0.041\n",
      "Epoch : 85 ; Train Loss : 0.080059 ; Train Acc : 0.209 ; Test Loss : 0.079778 ; Test Acc : 0.188 ; LR : 0.041\n",
      "Epoch : 86 ; Train Loss : 0.079807 ; Train Acc : 0.214 ; Test Loss : 0.079739 ; Test Acc : 0.188 ; LR : 0.041\n",
      "Epoch : 87 ; Train Loss : 0.079808 ; Train Acc : 0.207 ; Test Loss : 0.079777 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 88 ; Train Loss : 0.079917 ; Train Acc : 0.214 ; Test Loss : 0.079746 ; Test Acc : 0.188 ; LR : 0.041\n",
      "Epoch : 89 ; Train Loss : 0.079789 ; Train Acc : 0.214 ; Test Loss : 0.079768 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 90 ; Train Loss : 0.079937 ; Train Acc : 0.212 ; Test Loss : 0.079740 ; Test Acc : 0.188 ; LR : 0.041\n",
      "Epoch : 91 ; Train Loss : 0.079947 ; Train Acc : 0.208 ; Test Loss : 0.079757 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 92 ; Train Loss : 0.079777 ; Train Acc : 0.216 ; Test Loss : 0.079737 ; Test Acc : 0.188 ; LR : 0.041\n",
      "Epoch : 93 ; Train Loss : 0.079878 ; Train Acc : 0.214 ; Test Loss : 0.079751 ; Test Acc : 0.188 ; LR : 0.041\n",
      "Epoch : 94 ; Train Loss : 0.079843 ; Train Acc : 0.211 ; Test Loss : 0.079725 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 95 ; Train Loss : 0.079933 ; Train Acc : 0.215 ; Test Loss : 0.079726 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 96 ; Train Loss : 0.079757 ; Train Acc : 0.211 ; Test Loss : 0.079973 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 97 ; Train Loss : 0.080074 ; Train Acc : 0.207 ; Test Loss : 0.079773 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 98 ; Train Loss : 0.079918 ; Train Acc : 0.215 ; Test Loss : 0.079711 ; Test Acc : 0.188 ; LR : 0.041\n",
      "Epoch : 99 ; Train Loss : 0.079829 ; Train Acc : 0.208 ; Test Loss : 0.079712 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 100 ; Train Loss : 0.079795 ; Train Acc : 0.210 ; Test Loss : 0.079788 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 101 ; Train Loss : 0.079970 ; Train Acc : 0.215 ; Test Loss : 0.079719 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 102 ; Train Loss : 0.080066 ; Train Acc : 0.214 ; Test Loss : 0.079717 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 103 ; Train Loss : 0.079867 ; Train Acc : 0.216 ; Test Loss : 0.079733 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 104 ; Train Loss : 0.079778 ; Train Acc : 0.203 ; Test Loss : 0.079708 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 105 ; Train Loss : 0.079925 ; Train Acc : 0.216 ; Test Loss : 0.079713 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 106 ; Train Loss : 0.079917 ; Train Acc : 0.206 ; Test Loss : 0.079753 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 107 ; Train Loss : 0.080008 ; Train Acc : 0.215 ; Test Loss : 0.079705 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 108 ; Train Loss : 0.079946 ; Train Acc : 0.217 ; Test Loss : 0.079696 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 109 ; Train Loss : 0.080005 ; Train Acc : 0.210 ; Test Loss : 0.079804 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 110 ; Train Loss : 0.079986 ; Train Acc : 0.215 ; Test Loss : 0.079686 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 111 ; Train Loss : 0.079802 ; Train Acc : 0.214 ; Test Loss : 0.079703 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 112 ; Train Loss : 0.079919 ; Train Acc : 0.214 ; Test Loss : 0.079717 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 113 ; Train Loss : 0.079791 ; Train Acc : 0.212 ; Test Loss : 0.079751 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 114 ; Train Loss : 0.079989 ; Train Acc : 0.216 ; Test Loss : 0.079741 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 115 ; Train Loss : 0.079849 ; Train Acc : 0.212 ; Test Loss : 0.079712 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 116 ; Train Loss : 0.079776 ; Train Acc : 0.215 ; Test Loss : 0.079722 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 117 ; Train Loss : 0.079860 ; Train Acc : 0.213 ; Test Loss : 0.079672 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 118 ; Train Loss : 0.079886 ; Train Acc : 0.214 ; Test Loss : 0.079693 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 119 ; Train Loss : 0.079827 ; Train Acc : 0.213 ; Test Loss : 0.079706 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 120 ; Train Loss : 0.079855 ; Train Acc : 0.212 ; Test Loss : 0.079676 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 121 ; Train Loss : 0.079893 ; Train Acc : 0.210 ; Test Loss : 0.079671 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 122 ; Train Loss : 0.079905 ; Train Acc : 0.210 ; Test Loss : 0.079777 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 123 ; Train Loss : 0.079447 ; Train Acc : 0.215 ; Test Loss : 0.079718 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 124 ; Train Loss : 0.079787 ; Train Acc : 0.202 ; Test Loss : 0.079723 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 125 ; Train Loss : 0.079918 ; Train Acc : 0.214 ; Test Loss : 0.079752 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 126 ; Train Loss : 0.079814 ; Train Acc : 0.217 ; Test Loss : 0.079720 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 127 ; Train Loss : 0.079879 ; Train Acc : 0.218 ; Test Loss : 0.079673 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 128 ; Train Loss : 0.079832 ; Train Acc : 0.215 ; Test Loss : 0.079677 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 129 ; Train Loss : 0.079630 ; Train Acc : 0.207 ; Test Loss : 0.079727 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 130 ; Train Loss : 0.079841 ; Train Acc : 0.212 ; Test Loss : 0.079661 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 131 ; Train Loss : 0.079952 ; Train Acc : 0.205 ; Test Loss : 0.079748 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 132 ; Train Loss : 0.079742 ; Train Acc : 0.215 ; Test Loss : 0.079681 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 133 ; Train Loss : 0.080087 ; Train Acc : 0.215 ; Test Loss : 0.079659 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 134 ; Train Loss : 0.079693 ; Train Acc : 0.215 ; Test Loss : 0.079677 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 135 ; Train Loss : 0.079906 ; Train Acc : 0.216 ; Test Loss : 0.079667 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 136 ; Train Loss : 0.079720 ; Train Acc : 0.204 ; Test Loss : 0.079683 ; Test Acc : 0.188 ; LR : 0.036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 137 ; Train Loss : 0.079620 ; Train Acc : 0.204 ; Test Loss : 0.079657 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 138 ; Train Loss : 0.079893 ; Train Acc : 0.212 ; Test Loss : 0.079689 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 139 ; Train Loss : 0.079520 ; Train Acc : 0.208 ; Test Loss : 0.079654 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 140 ; Train Loss : 0.079944 ; Train Acc : 0.209 ; Test Loss : 0.079694 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 141 ; Train Loss : 0.079711 ; Train Acc : 0.217 ; Test Loss : 0.079657 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 142 ; Train Loss : 0.079844 ; Train Acc : 0.214 ; Test Loss : 0.079698 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 143 ; Train Loss : 0.079576 ; Train Acc : 0.213 ; Test Loss : 0.079702 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 144 ; Train Loss : 0.079723 ; Train Acc : 0.209 ; Test Loss : 0.079669 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 145 ; Train Loss : 0.079946 ; Train Acc : 0.212 ; Test Loss : 0.079694 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 146 ; Train Loss : 0.079905 ; Train Acc : 0.213 ; Test Loss : 0.079652 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 147 ; Train Loss : 0.079770 ; Train Acc : 0.213 ; Test Loss : 0.079670 ; Test Acc : 0.250 ; LR : 0.036\n",
      "Epoch : 148 ; Train Loss : 0.079728 ; Train Acc : 0.211 ; Test Loss : 0.079650 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 149 ; Train Loss : 0.079839 ; Train Acc : 0.210 ; Test Loss : 0.079670 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 150 ; Train Loss : 0.079862 ; Train Acc : 0.207 ; Test Loss : 0.079708 ; Test Acc : 0.188 ; LR : 0.036\n",
      "Epoch : 151 ; Train Loss : 0.079814 ; Train Acc : 0.211 ; Test Loss : 0.079655 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 152 ; Train Loss : 0.079798 ; Train Acc : 0.211 ; Test Loss : 0.079671 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 153 ; Train Loss : 0.079883 ; Train Acc : 0.210 ; Test Loss : 0.079661 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 154 ; Train Loss : 0.079855 ; Train Acc : 0.212 ; Test Loss : 0.079669 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 155 ; Train Loss : 0.079613 ; Train Acc : 0.215 ; Test Loss : 0.079664 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 156 ; Train Loss : 0.079617 ; Train Acc : 0.202 ; Test Loss : 0.079645 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 157 ; Train Loss : 0.079600 ; Train Acc : 0.215 ; Test Loss : 0.079660 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 158 ; Train Loss : 0.079785 ; Train Acc : 0.210 ; Test Loss : 0.079644 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 159 ; Train Loss : 0.079864 ; Train Acc : 0.213 ; Test Loss : 0.079663 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 160 ; Train Loss : 0.079645 ; Train Acc : 0.213 ; Test Loss : 0.079650 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 161 ; Train Loss : 0.079802 ; Train Acc : 0.212 ; Test Loss : 0.079678 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 162 ; Train Loss : 0.079737 ; Train Acc : 0.216 ; Test Loss : 0.079666 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 163 ; Train Loss : 0.079488 ; Train Acc : 0.215 ; Test Loss : 0.079682 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 164 ; Train Loss : 0.079441 ; Train Acc : 0.216 ; Test Loss : 0.079666 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 165 ; Train Loss : 0.079609 ; Train Acc : 0.211 ; Test Loss : 0.079676 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 166 ; Train Loss : 0.079648 ; Train Acc : 0.214 ; Test Loss : 0.079654 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 167 ; Train Loss : 0.079689 ; Train Acc : 0.210 ; Test Loss : 0.079648 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 168 ; Train Loss : 0.079706 ; Train Acc : 0.213 ; Test Loss : 0.079658 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 169 ; Train Loss : 0.079802 ; Train Acc : 0.210 ; Test Loss : 0.079643 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 170 ; Train Loss : 0.079725 ; Train Acc : 0.214 ; Test Loss : 0.079647 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 171 ; Train Loss : 0.079620 ; Train Acc : 0.213 ; Test Loss : 0.079639 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 172 ; Train Loss : 0.079663 ; Train Acc : 0.215 ; Test Loss : 0.079648 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 173 ; Train Loss : 0.079846 ; Train Acc : 0.215 ; Test Loss : 0.079670 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 174 ; Train Loss : 0.079735 ; Train Acc : 0.217 ; Test Loss : 0.079642 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 175 ; Train Loss : 0.079616 ; Train Acc : 0.204 ; Test Loss : 0.079657 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 176 ; Train Loss : 0.079793 ; Train Acc : 0.216 ; Test Loss : 0.079661 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 177 ; Train Loss : 0.079539 ; Train Acc : 0.215 ; Test Loss : 0.079671 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 178 ; Train Loss : 0.079840 ; Train Acc : 0.203 ; Test Loss : 0.079662 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 179 ; Train Loss : 0.079559 ; Train Acc : 0.214 ; Test Loss : 0.079679 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 180 ; Train Loss : 0.079702 ; Train Acc : 0.215 ; Test Loss : 0.079725 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 181 ; Train Loss : 0.079781 ; Train Acc : 0.216 ; Test Loss : 0.079647 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 182 ; Train Loss : 0.079753 ; Train Acc : 0.211 ; Test Loss : 0.079658 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 183 ; Train Loss : 0.079442 ; Train Acc : 0.207 ; Test Loss : 0.079652 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 184 ; Train Loss : 0.079571 ; Train Acc : 0.211 ; Test Loss : 0.079664 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 185 ; Train Loss : 0.079473 ; Train Acc : 0.211 ; Test Loss : 0.079655 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 186 ; Train Loss : 0.079747 ; Train Acc : 0.206 ; Test Loss : 0.079677 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 187 ; Train Loss : 0.079496 ; Train Acc : 0.209 ; Test Loss : 0.079652 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 188 ; Train Loss : 0.079733 ; Train Acc : 0.215 ; Test Loss : 0.079674 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 189 ; Train Loss : 0.079583 ; Train Acc : 0.200 ; Test Loss : 0.079642 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 190 ; Train Loss : 0.079571 ; Train Acc : 0.213 ; Test Loss : 0.079667 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 191 ; Train Loss : 0.079709 ; Train Acc : 0.205 ; Test Loss : 0.079671 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 192 ; Train Loss : 0.079683 ; Train Acc : 0.217 ; Test Loss : 0.079660 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 193 ; Train Loss : 0.079718 ; Train Acc : 0.216 ; Test Loss : 0.079655 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 194 ; Train Loss : 0.079680 ; Train Acc : 0.212 ; Test Loss : 0.079671 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 195 ; Train Loss : 0.079551 ; Train Acc : 0.213 ; Test Loss : 0.079780 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 196 ; Train Loss : 0.079906 ; Train Acc : 0.209 ; Test Loss : 0.079641 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 197 ; Train Loss : 0.079551 ; Train Acc : 0.214 ; Test Loss : 0.079687 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 198 ; Train Loss : 0.079558 ; Train Acc : 0.207 ; Test Loss : 0.079654 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 199 ; Train Loss : 0.079653 ; Train Acc : 0.210 ; Test Loss : 0.079647 ; Test Acc : 0.188 ; LR : 0.033\n",
      "Epoch : 200 ; Train Loss : 0.079484 ; Train Acc : 0.217 ; Test Loss : 0.079659 ; Test Acc : 0.250 ; LR : 0.033\n",
      "Epoch : 201 ; Train Loss : 0.079704 ; Train Acc : 0.215 ; Test Loss : 0.079664 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 202 ; Train Loss : 0.079766 ; Train Acc : 0.211 ; Test Loss : 0.079666 ; Test Acc : 0.250 ; LR : 0.030\n",
      "Epoch : 203 ; Train Loss : 0.079460 ; Train Acc : 0.215 ; Test Loss : 0.079665 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 204 ; Train Loss : 0.079562 ; Train Acc : 0.213 ; Test Loss : 0.079663 ; Test Acc : 0.250 ; LR : 0.030\n",
      "Epoch : 205 ; Train Loss : 0.079838 ; Train Acc : 0.215 ; Test Loss : 0.079645 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 206 ; Train Loss : 0.079770 ; Train Acc : 0.215 ; Test Loss : 0.079686 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 207 ; Train Loss : 0.079480 ; Train Acc : 0.215 ; Test Loss : 0.079666 ; Test Acc : 0.250 ; LR : 0.030\n",
      "Epoch : 208 ; Train Loss : 0.079655 ; Train Acc : 0.205 ; Test Loss : 0.079650 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 209 ; Train Loss : 0.079583 ; Train Acc : 0.214 ; Test Loss : 0.079663 ; Test Acc : 0.250 ; LR : 0.030\n",
      "Epoch : 210 ; Train Loss : 0.079587 ; Train Acc : 0.212 ; Test Loss : 0.079672 ; Test Acc : 0.250 ; LR : 0.030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 211 ; Train Loss : 0.079631 ; Train Acc : 0.205 ; Test Loss : 0.079651 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 212 ; Train Loss : 0.079781 ; Train Acc : 0.216 ; Test Loss : 0.079660 ; Test Acc : 0.250 ; LR : 0.030\n",
      "Epoch : 213 ; Train Loss : 0.079805 ; Train Acc : 0.207 ; Test Loss : 0.079644 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 214 ; Train Loss : 0.079711 ; Train Acc : 0.215 ; Test Loss : 0.079679 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 215 ; Train Loss : 0.079543 ; Train Acc : 0.217 ; Test Loss : 0.079710 ; Test Acc : 0.250 ; LR : 0.030\n",
      "Epoch : 216 ; Train Loss : 0.079443 ; Train Acc : 0.214 ; Test Loss : 0.079650 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 217 ; Train Loss : 0.079536 ; Train Acc : 0.215 ; Test Loss : 0.079660 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 218 ; Train Loss : 0.079575 ; Train Acc : 0.215 ; Test Loss : 0.079675 ; Test Acc : 0.250 ; LR : 0.030\n",
      "Epoch : 219 ; Train Loss : 0.079597 ; Train Acc : 0.207 ; Test Loss : 0.079660 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 220 ; Train Loss : 0.079670 ; Train Acc : 0.215 ; Test Loss : 0.079709 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 221 ; Train Loss : 0.079511 ; Train Acc : 0.216 ; Test Loss : 0.079651 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 222 ; Train Loss : 0.079679 ; Train Acc : 0.212 ; Test Loss : 0.079664 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 223 ; Train Loss : 0.079442 ; Train Acc : 0.216 ; Test Loss : 0.079663 ; Test Acc : 0.250 ; LR : 0.030\n",
      "Epoch : 224 ; Train Loss : 0.079569 ; Train Acc : 0.212 ; Test Loss : 0.079652 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 225 ; Train Loss : 0.079645 ; Train Acc : 0.198 ; Test Loss : 0.079647 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 226 ; Train Loss : 0.079518 ; Train Acc : 0.210 ; Test Loss : 0.079675 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 227 ; Train Loss : 0.079718 ; Train Acc : 0.216 ; Test Loss : 0.079663 ; Test Acc : 0.250 ; LR : 0.030\n",
      "Epoch : 228 ; Train Loss : 0.079710 ; Train Acc : 0.203 ; Test Loss : 0.079674 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 229 ; Train Loss : 0.079827 ; Train Acc : 0.214 ; Test Loss : 0.079657 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 230 ; Train Loss : 0.079745 ; Train Acc : 0.207 ; Test Loss : 0.079669 ; Test Acc : 0.250 ; LR : 0.030\n",
      "Epoch : 231 ; Train Loss : 0.079788 ; Train Acc : 0.217 ; Test Loss : 0.079676 ; Test Acc : 0.250 ; LR : 0.030\n",
      "Epoch : 232 ; Train Loss : 0.079495 ; Train Acc : 0.211 ; Test Loss : 0.079646 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 233 ; Train Loss : 0.079516 ; Train Acc : 0.216 ; Test Loss : 0.079675 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 234 ; Train Loss : 0.079577 ; Train Acc : 0.215 ; Test Loss : 0.079644 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 235 ; Train Loss : 0.079689 ; Train Acc : 0.211 ; Test Loss : 0.079666 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 236 ; Train Loss : 0.079676 ; Train Acc : 0.213 ; Test Loss : 0.079659 ; Test Acc : 0.250 ; LR : 0.030\n",
      "Epoch : 237 ; Train Loss : 0.079512 ; Train Acc : 0.211 ; Test Loss : 0.079670 ; Test Acc : 0.250 ; LR : 0.030\n",
      "Epoch : 238 ; Train Loss : 0.079324 ; Train Acc : 0.202 ; Test Loss : 0.079675 ; Test Acc : 0.250 ; LR : 0.030\n",
      "Epoch : 239 ; Train Loss : 0.079624 ; Train Acc : 0.217 ; Test Loss : 0.079661 ; Test Acc : 0.250 ; LR : 0.030\n",
      "Epoch : 240 ; Train Loss : 0.079647 ; Train Acc : 0.198 ; Test Loss : 0.079645 ; Test Acc : 0.250 ; LR : 0.030\n",
      "Epoch : 241 ; Train Loss : 0.079620 ; Train Acc : 0.216 ; Test Loss : 0.079707 ; Test Acc : 0.250 ; LR : 0.030\n",
      "Epoch : 242 ; Train Loss : 0.079765 ; Train Acc : 0.215 ; Test Loss : 0.079726 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 243 ; Train Loss : 0.079436 ; Train Acc : 0.215 ; Test Loss : 0.079719 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 244 ; Train Loss : 0.079649 ; Train Acc : 0.212 ; Test Loss : 0.079732 ; Test Acc : 0.250 ; LR : 0.030\n",
      "Epoch : 245 ; Train Loss : 0.079815 ; Train Acc : 0.219 ; Test Loss : 0.079684 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 246 ; Train Loss : 0.079749 ; Train Acc : 0.215 ; Test Loss : 0.079663 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 247 ; Train Loss : 0.079472 ; Train Acc : 0.216 ; Test Loss : 0.079637 ; Test Acc : 0.250 ; LR : 0.030\n",
      "Epoch : 248 ; Train Loss : 0.079443 ; Train Acc : 0.213 ; Test Loss : 0.079696 ; Test Acc : 0.188 ; LR : 0.030\n",
      "Epoch : 249 ; Train Loss : 0.079736 ; Train Acc : 0.212 ; Test Loss : 0.079659 ; Test Acc : 0.250 ; LR : 0.030\n",
      "Epoch : 250 ; Train Loss : 0.079466 ; Train Acc : 0.215 ; Test Loss : 0.079716 ; Test Acc : 0.250 ; LR : 0.030\n",
      "Epoch : 251 ; Train Loss : 0.079528 ; Train Acc : 0.217 ; Test Loss : 0.079659 ; Test Acc : 0.250 ; LR : 0.027\n",
      "Epoch : 252 ; Train Loss : 0.079757 ; Train Acc : 0.214 ; Test Loss : 0.079699 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 253 ; Train Loss : 0.079692 ; Train Acc : 0.214 ; Test Loss : 0.079662 ; Test Acc : 0.250 ; LR : 0.027\n",
      "Epoch : 254 ; Train Loss : 0.079618 ; Train Acc : 0.210 ; Test Loss : 0.079656 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 255 ; Train Loss : 0.079602 ; Train Acc : 0.215 ; Test Loss : 0.079692 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 256 ; Train Loss : 0.079636 ; Train Acc : 0.214 ; Test Loss : 0.079666 ; Test Acc : 0.250 ; LR : 0.027\n",
      "Epoch : 257 ; Train Loss : 0.079394 ; Train Acc : 0.214 ; Test Loss : 0.079662 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 258 ; Train Loss : 0.079538 ; Train Acc : 0.216 ; Test Loss : 0.079651 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 259 ; Train Loss : 0.079593 ; Train Acc : 0.216 ; Test Loss : 0.079651 ; Test Acc : 0.250 ; LR : 0.027\n",
      "Epoch : 260 ; Train Loss : 0.079723 ; Train Acc : 0.215 ; Test Loss : 0.079738 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 261 ; Train Loss : 0.079534 ; Train Acc : 0.207 ; Test Loss : 0.079693 ; Test Acc : 0.250 ; LR : 0.027\n",
      "Epoch : 262 ; Train Loss : 0.079562 ; Train Acc : 0.213 ; Test Loss : 0.079684 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 263 ; Train Loss : 0.079609 ; Train Acc : 0.216 ; Test Loss : 0.079666 ; Test Acc : 0.250 ; LR : 0.027\n",
      "Epoch : 264 ; Train Loss : 0.079556 ; Train Acc : 0.215 ; Test Loss : 0.079683 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 265 ; Train Loss : 0.079544 ; Train Acc : 0.215 ; Test Loss : 0.079664 ; Test Acc : 0.250 ; LR : 0.027\n",
      "Epoch : 266 ; Train Loss : 0.079826 ; Train Acc : 0.212 ; Test Loss : 0.079728 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 267 ; Train Loss : 0.079394 ; Train Acc : 0.214 ; Test Loss : 0.079656 ; Test Acc : 0.250 ; LR : 0.027\n",
      "Epoch : 268 ; Train Loss : 0.079614 ; Train Acc : 0.213 ; Test Loss : 0.079680 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 269 ; Train Loss : 0.079791 ; Train Acc : 0.216 ; Test Loss : 0.079710 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 270 ; Train Loss : 0.079757 ; Train Acc : 0.213 ; Test Loss : 0.079716 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 271 ; Train Loss : 0.079605 ; Train Acc : 0.213 ; Test Loss : 0.079691 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 272 ; Train Loss : 0.079725 ; Train Acc : 0.216 ; Test Loss : 0.079659 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 273 ; Train Loss : 0.079596 ; Train Acc : 0.216 ; Test Loss : 0.079692 ; Test Acc : 0.250 ; LR : 0.027\n",
      "Epoch : 274 ; Train Loss : 0.079322 ; Train Acc : 0.219 ; Test Loss : 0.079665 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 275 ; Train Loss : 0.079672 ; Train Acc : 0.212 ; Test Loss : 0.079663 ; Test Acc : 0.250 ; LR : 0.027\n",
      "Epoch : 276 ; Train Loss : 0.079447 ; Train Acc : 0.215 ; Test Loss : 0.079656 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 277 ; Train Loss : 0.079663 ; Train Acc : 0.212 ; Test Loss : 0.079653 ; Test Acc : 0.250 ; LR : 0.027\n",
      "Epoch : 278 ; Train Loss : 0.079764 ; Train Acc : 0.214 ; Test Loss : 0.079665 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 279 ; Train Loss : 0.079385 ; Train Acc : 0.211 ; Test Loss : 0.079666 ; Test Acc : 0.250 ; LR : 0.027\n",
      "Epoch : 280 ; Train Loss : 0.079393 ; Train Acc : 0.215 ; Test Loss : 0.079670 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 281 ; Train Loss : 0.079557 ; Train Acc : 0.216 ; Test Loss : 0.079663 ; Test Acc : 0.250 ; LR : 0.027\n",
      "Epoch : 282 ; Train Loss : 0.079525 ; Train Acc : 0.211 ; Test Loss : 0.079650 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 283 ; Train Loss : 0.079424 ; Train Acc : 0.216 ; Test Loss : 0.079670 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 284 ; Train Loss : 0.079571 ; Train Acc : 0.214 ; Test Loss : 0.079652 ; Test Acc : 0.250 ; LR : 0.027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 285 ; Train Loss : 0.079501 ; Train Acc : 0.217 ; Test Loss : 0.079685 ; Test Acc : 0.250 ; LR : 0.027\n",
      "Epoch : 286 ; Train Loss : 0.079356 ; Train Acc : 0.214 ; Test Loss : 0.079671 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 287 ; Train Loss : 0.079643 ; Train Acc : 0.214 ; Test Loss : 0.079677 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 288 ; Train Loss : 0.079631 ; Train Acc : 0.212 ; Test Loss : 0.079679 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 289 ; Train Loss : 0.079753 ; Train Acc : 0.215 ; Test Loss : 0.079680 ; Test Acc : 0.250 ; LR : 0.027\n",
      "Epoch : 290 ; Train Loss : 0.079558 ; Train Acc : 0.212 ; Test Loss : 0.079685 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 291 ; Train Loss : 0.079483 ; Train Acc : 0.213 ; Test Loss : 0.079666 ; Test Acc : 0.250 ; LR : 0.027\n",
      "Epoch : 292 ; Train Loss : 0.079743 ; Train Acc : 0.216 ; Test Loss : 0.079715 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 293 ; Train Loss : 0.079657 ; Train Acc : 0.216 ; Test Loss : 0.079757 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 294 ; Train Loss : 0.079487 ; Train Acc : 0.205 ; Test Loss : 0.079675 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 295 ; Train Loss : 0.079503 ; Train Acc : 0.206 ; Test Loss : 0.079655 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 296 ; Train Loss : 0.079393 ; Train Acc : 0.208 ; Test Loss : 0.079663 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 297 ; Train Loss : 0.079470 ; Train Acc : 0.215 ; Test Loss : 0.079703 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 298 ; Train Loss : 0.079617 ; Train Acc : 0.214 ; Test Loss : 0.079703 ; Test Acc : 0.250 ; LR : 0.027\n",
      "Epoch : 299 ; Train Loss : 0.079432 ; Train Acc : 0.215 ; Test Loss : 0.079674 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 300 ; Train Loss : 0.079534 ; Train Acc : 0.215 ; Test Loss : 0.079679 ; Test Acc : 0.188 ; LR : 0.027\n",
      "Epoch : 301 ; Train Loss : 0.079456 ; Train Acc : 0.214 ; Test Loss : 0.079695 ; Test Acc : 0.250 ; LR : 0.024\n",
      "Epoch : 302 ; Train Loss : 0.079522 ; Train Acc : 0.212 ; Test Loss : 0.079685 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 303 ; Train Loss : 0.079567 ; Train Acc : 0.215 ; Test Loss : 0.079672 ; Test Acc : 0.250 ; LR : 0.024\n",
      "Epoch : 304 ; Train Loss : 0.079621 ; Train Acc : 0.217 ; Test Loss : 0.079701 ; Test Acc : 0.250 ; LR : 0.024\n",
      "Epoch : 305 ; Train Loss : 0.079489 ; Train Acc : 0.212 ; Test Loss : 0.079690 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 306 ; Train Loss : 0.079367 ; Train Acc : 0.216 ; Test Loss : 0.079686 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 307 ; Train Loss : 0.079686 ; Train Acc : 0.214 ; Test Loss : 0.079659 ; Test Acc : 0.250 ; LR : 0.024\n",
      "Epoch : 308 ; Train Loss : 0.079356 ; Train Acc : 0.210 ; Test Loss : 0.079689 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 309 ; Train Loss : 0.079374 ; Train Acc : 0.216 ; Test Loss : 0.079668 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 310 ; Train Loss : 0.079563 ; Train Acc : 0.213 ; Test Loss : 0.079678 ; Test Acc : 0.250 ; LR : 0.024\n",
      "Epoch : 311 ; Train Loss : 0.079610 ; Train Acc : 0.211 ; Test Loss : 0.079687 ; Test Acc : 0.250 ; LR : 0.024\n",
      "Epoch : 312 ; Train Loss : 0.079321 ; Train Acc : 0.214 ; Test Loss : 0.079697 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 313 ; Train Loss : 0.079552 ; Train Acc : 0.208 ; Test Loss : 0.079678 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 314 ; Train Loss : 0.079567 ; Train Acc : 0.218 ; Test Loss : 0.079692 ; Test Acc : 0.250 ; LR : 0.024\n",
      "Epoch : 315 ; Train Loss : 0.079505 ; Train Acc : 0.205 ; Test Loss : 0.079677 ; Test Acc : 0.250 ; LR : 0.024\n",
      "Epoch : 316 ; Train Loss : 0.079565 ; Train Acc : 0.209 ; Test Loss : 0.079662 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 317 ; Train Loss : 0.079605 ; Train Acc : 0.213 ; Test Loss : 0.079689 ; Test Acc : 0.250 ; LR : 0.024\n",
      "Epoch : 318 ; Train Loss : 0.079288 ; Train Acc : 0.217 ; Test Loss : 0.079693 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 319 ; Train Loss : 0.079421 ; Train Acc : 0.215 ; Test Loss : 0.079669 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 320 ; Train Loss : 0.079555 ; Train Acc : 0.206 ; Test Loss : 0.079683 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 321 ; Train Loss : 0.079488 ; Train Acc : 0.216 ; Test Loss : 0.079738 ; Test Acc : 0.250 ; LR : 0.024\n",
      "Epoch : 322 ; Train Loss : 0.079399 ; Train Acc : 0.216 ; Test Loss : 0.079708 ; Test Acc : 0.250 ; LR : 0.024\n",
      "Epoch : 323 ; Train Loss : 0.079679 ; Train Acc : 0.219 ; Test Loss : 0.079678 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 324 ; Train Loss : 0.079551 ; Train Acc : 0.209 ; Test Loss : 0.079676 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 325 ; Train Loss : 0.079596 ; Train Acc : 0.215 ; Test Loss : 0.079690 ; Test Acc : 0.250 ; LR : 0.024\n",
      "Epoch : 326 ; Train Loss : 0.079241 ; Train Acc : 0.218 ; Test Loss : 0.079695 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 327 ; Train Loss : 0.079747 ; Train Acc : 0.211 ; Test Loss : 0.079715 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 328 ; Train Loss : 0.079556 ; Train Acc : 0.209 ; Test Loss : 0.079692 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 329 ; Train Loss : 0.079542 ; Train Acc : 0.216 ; Test Loss : 0.079688 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 330 ; Train Loss : 0.079475 ; Train Acc : 0.214 ; Test Loss : 0.079727 ; Test Acc : 0.250 ; LR : 0.024\n",
      "Epoch : 331 ; Train Loss : 0.079535 ; Train Acc : 0.216 ; Test Loss : 0.079685 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 332 ; Train Loss : 0.079600 ; Train Acc : 0.216 ; Test Loss : 0.079681 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 333 ; Train Loss : 0.079556 ; Train Acc : 0.216 ; Test Loss : 0.079677 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 334 ; Train Loss : 0.079606 ; Train Acc : 0.212 ; Test Loss : 0.079691 ; Test Acc : 0.250 ; LR : 0.024\n",
      "Epoch : 335 ; Train Loss : 0.079538 ; Train Acc : 0.217 ; Test Loss : 0.079724 ; Test Acc : 0.250 ; LR : 0.024\n",
      "Epoch : 336 ; Train Loss : 0.079511 ; Train Acc : 0.217 ; Test Loss : 0.079693 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 337 ; Train Loss : 0.079476 ; Train Acc : 0.216 ; Test Loss : 0.079670 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 338 ; Train Loss : 0.079480 ; Train Acc : 0.214 ; Test Loss : 0.079788 ; Test Acc : 0.250 ; LR : 0.024\n",
      "Epoch : 339 ; Train Loss : 0.079447 ; Train Acc : 0.214 ; Test Loss : 0.079693 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 340 ; Train Loss : 0.079498 ; Train Acc : 0.214 ; Test Loss : 0.079682 ; Test Acc : 0.250 ; LR : 0.024\n",
      "Epoch : 341 ; Train Loss : 0.079692 ; Train Acc : 0.216 ; Test Loss : 0.079678 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 342 ; Train Loss : 0.079541 ; Train Acc : 0.209 ; Test Loss : 0.079695 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 343 ; Train Loss : 0.079674 ; Train Acc : 0.213 ; Test Loss : 0.079712 ; Test Acc : 0.250 ; LR : 0.024\n",
      "Epoch : 344 ; Train Loss : 0.079495 ; Train Acc : 0.210 ; Test Loss : 0.079674 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 345 ; Train Loss : 0.079409 ; Train Acc : 0.216 ; Test Loss : 0.079683 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 346 ; Train Loss : 0.079537 ; Train Acc : 0.216 ; Test Loss : 0.079694 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 347 ; Train Loss : 0.079354 ; Train Acc : 0.214 ; Test Loss : 0.079712 ; Test Acc : 0.250 ; LR : 0.024\n",
      "Epoch : 348 ; Train Loss : 0.079647 ; Train Acc : 0.218 ; Test Loss : 0.079691 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 349 ; Train Loss : 0.079606 ; Train Acc : 0.203 ; Test Loss : 0.079696 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 350 ; Train Loss : 0.079400 ; Train Acc : 0.216 ; Test Loss : 0.079693 ; Test Acc : 0.188 ; LR : 0.024\n",
      "Epoch : 351 ; Train Loss : 0.079657 ; Train Acc : 0.203 ; Test Loss : 0.079700 ; Test Acc : 0.250 ; LR : 0.022\n",
      "Epoch : 352 ; Train Loss : 0.079440 ; Train Acc : 0.217 ; Test Loss : 0.079704 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 353 ; Train Loss : 0.079513 ; Train Acc : 0.210 ; Test Loss : 0.079717 ; Test Acc : 0.250 ; LR : 0.022\n",
      "Epoch : 354 ; Train Loss : 0.079514 ; Train Acc : 0.214 ; Test Loss : 0.079678 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 355 ; Train Loss : 0.079360 ; Train Acc : 0.206 ; Test Loss : 0.079716 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 356 ; Train Loss : 0.079539 ; Train Acc : 0.214 ; Test Loss : 0.079721 ; Test Acc : 0.250 ; LR : 0.022\n",
      "Epoch : 357 ; Train Loss : 0.079347 ; Train Acc : 0.210 ; Test Loss : 0.079735 ; Test Acc : 0.250 ; LR : 0.022\n",
      "Epoch : 358 ; Train Loss : 0.079470 ; Train Acc : 0.212 ; Test Loss : 0.079677 ; Test Acc : 0.188 ; LR : 0.022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 359 ; Train Loss : 0.079455 ; Train Acc : 0.214 ; Test Loss : 0.079688 ; Test Acc : 0.250 ; LR : 0.022\n",
      "Epoch : 360 ; Train Loss : 0.079530 ; Train Acc : 0.213 ; Test Loss : 0.079749 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 361 ; Train Loss : 0.079479 ; Train Acc : 0.216 ; Test Loss : 0.079686 ; Test Acc : 0.250 ; LR : 0.022\n",
      "Epoch : 362 ; Train Loss : 0.079456 ; Train Acc : 0.217 ; Test Loss : 0.079685 ; Test Acc : 0.250 ; LR : 0.022\n",
      "Epoch : 363 ; Train Loss : 0.079504 ; Train Acc : 0.216 ; Test Loss : 0.079671 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 364 ; Train Loss : 0.079588 ; Train Acc : 0.213 ; Test Loss : 0.079685 ; Test Acc : 0.250 ; LR : 0.022\n",
      "Epoch : 365 ; Train Loss : 0.079210 ; Train Acc : 0.214 ; Test Loss : 0.079679 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 366 ; Train Loss : 0.079593 ; Train Acc : 0.212 ; Test Loss : 0.079698 ; Test Acc : 0.250 ; LR : 0.022\n",
      "Epoch : 367 ; Train Loss : 0.079499 ; Train Acc : 0.213 ; Test Loss : 0.079702 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 368 ; Train Loss : 0.079517 ; Train Acc : 0.216 ; Test Loss : 0.079683 ; Test Acc : 0.250 ; LR : 0.022\n",
      "Epoch : 369 ; Train Loss : 0.079576 ; Train Acc : 0.211 ; Test Loss : 0.079686 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 370 ; Train Loss : 0.079513 ; Train Acc : 0.208 ; Test Loss : 0.079687 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 371 ; Train Loss : 0.079645 ; Train Acc : 0.216 ; Test Loss : 0.079682 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 372 ; Train Loss : 0.079385 ; Train Acc : 0.216 ; Test Loss : 0.079679 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 373 ; Train Loss : 0.079346 ; Train Acc : 0.210 ; Test Loss : 0.079688 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 374 ; Train Loss : 0.079314 ; Train Acc : 0.213 ; Test Loss : 0.079687 ; Test Acc : 0.250 ; LR : 0.022\n",
      "Epoch : 375 ; Train Loss : 0.079599 ; Train Acc : 0.208 ; Test Loss : 0.079710 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 376 ; Train Loss : 0.079525 ; Train Acc : 0.216 ; Test Loss : 0.079697 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 377 ; Train Loss : 0.079536 ; Train Acc : 0.215 ; Test Loss : 0.079691 ; Test Acc : 0.250 ; LR : 0.022\n",
      "Epoch : 378 ; Train Loss : 0.079549 ; Train Acc : 0.217 ; Test Loss : 0.079701 ; Test Acc : 0.250 ; LR : 0.022\n",
      "Epoch : 379 ; Train Loss : 0.079387 ; Train Acc : 0.213 ; Test Loss : 0.079719 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 380 ; Train Loss : 0.079402 ; Train Acc : 0.215 ; Test Loss : 0.079675 ; Test Acc : 0.250 ; LR : 0.022\n",
      "Epoch : 381 ; Train Loss : 0.079662 ; Train Acc : 0.208 ; Test Loss : 0.079690 ; Test Acc : 0.250 ; LR : 0.022\n",
      "Epoch : 382 ; Train Loss : 0.079499 ; Train Acc : 0.217 ; Test Loss : 0.079677 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 383 ; Train Loss : 0.079467 ; Train Acc : 0.217 ; Test Loss : 0.079700 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 384 ; Train Loss : 0.079327 ; Train Acc : 0.210 ; Test Loss : 0.079696 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 385 ; Train Loss : 0.079267 ; Train Acc : 0.215 ; Test Loss : 0.079713 ; Test Acc : 0.250 ; LR : 0.022\n",
      "Epoch : 386 ; Train Loss : 0.079638 ; Train Acc : 0.216 ; Test Loss : 0.079700 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 387 ; Train Loss : 0.079510 ; Train Acc : 0.216 ; Test Loss : 0.079714 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 388 ; Train Loss : 0.079345 ; Train Acc : 0.209 ; Test Loss : 0.079746 ; Test Acc : 0.250 ; LR : 0.022\n",
      "Epoch : 389 ; Train Loss : 0.079495 ; Train Acc : 0.217 ; Test Loss : 0.079677 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 390 ; Train Loss : 0.079447 ; Train Acc : 0.217 ; Test Loss : 0.079694 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 391 ; Train Loss : 0.079506 ; Train Acc : 0.216 ; Test Loss : 0.079700 ; Test Acc : 0.250 ; LR : 0.022\n",
      "Epoch : 392 ; Train Loss : 0.079596 ; Train Acc : 0.210 ; Test Loss : 0.079703 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 393 ; Train Loss : 0.079448 ; Train Acc : 0.214 ; Test Loss : 0.079694 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 394 ; Train Loss : 0.079512 ; Train Acc : 0.211 ; Test Loss : 0.079688 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 395 ; Train Loss : 0.079454 ; Train Acc : 0.215 ; Test Loss : 0.079689 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 396 ; Train Loss : 0.079197 ; Train Acc : 0.216 ; Test Loss : 0.079704 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 397 ; Train Loss : 0.079433 ; Train Acc : 0.209 ; Test Loss : 0.079721 ; Test Acc : 0.250 ; LR : 0.022\n",
      "Epoch : 398 ; Train Loss : 0.079511 ; Train Acc : 0.217 ; Test Loss : 0.079737 ; Test Acc : 0.250 ; LR : 0.022\n",
      "Epoch : 399 ; Train Loss : 0.079329 ; Train Acc : 0.217 ; Test Loss : 0.079671 ; Test Acc : 0.188 ; LR : 0.022\n",
      "Epoch : 400 ; Train Loss : 0.079350 ; Train Acc : 0.210 ; Test Loss : 0.079802 ; Test Acc : 0.250 ; LR : 0.022\n",
      "Epoch : 401 ; Train Loss : 0.079491 ; Train Acc : 0.214 ; Test Loss : 0.079733 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 402 ; Train Loss : 0.079587 ; Train Acc : 0.207 ; Test Loss : 0.079713 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 403 ; Train Loss : 0.079439 ; Train Acc : 0.217 ; Test Loss : 0.079693 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 404 ; Train Loss : 0.079501 ; Train Acc : 0.216 ; Test Loss : 0.079696 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 405 ; Train Loss : 0.079501 ; Train Acc : 0.215 ; Test Loss : 0.079712 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 406 ; Train Loss : 0.079285 ; Train Acc : 0.216 ; Test Loss : 0.079682 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 407 ; Train Loss : 0.079568 ; Train Acc : 0.213 ; Test Loss : 0.079704 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 408 ; Train Loss : 0.079513 ; Train Acc : 0.217 ; Test Loss : 0.079730 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 409 ; Train Loss : 0.079383 ; Train Acc : 0.214 ; Test Loss : 0.079718 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 410 ; Train Loss : 0.079448 ; Train Acc : 0.216 ; Test Loss : 0.079708 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 411 ; Train Loss : 0.079575 ; Train Acc : 0.217 ; Test Loss : 0.079765 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 412 ; Train Loss : 0.079498 ; Train Acc : 0.216 ; Test Loss : 0.079838 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 413 ; Train Loss : 0.079488 ; Train Acc : 0.209 ; Test Loss : 0.079693 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 414 ; Train Loss : 0.079554 ; Train Acc : 0.212 ; Test Loss : 0.079681 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 415 ; Train Loss : 0.079437 ; Train Acc : 0.214 ; Test Loss : 0.079695 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 416 ; Train Loss : 0.079595 ; Train Acc : 0.213 ; Test Loss : 0.079711 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 417 ; Train Loss : 0.079563 ; Train Acc : 0.209 ; Test Loss : 0.079681 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 418 ; Train Loss : 0.079323 ; Train Acc : 0.215 ; Test Loss : 0.079698 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 419 ; Train Loss : 0.079450 ; Train Acc : 0.216 ; Test Loss : 0.079687 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 420 ; Train Loss : 0.079572 ; Train Acc : 0.215 ; Test Loss : 0.079703 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 421 ; Train Loss : 0.079512 ; Train Acc : 0.216 ; Test Loss : 0.079697 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 422 ; Train Loss : 0.079366 ; Train Acc : 0.213 ; Test Loss : 0.079715 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 423 ; Train Loss : 0.079566 ; Train Acc : 0.216 ; Test Loss : 0.079743 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 424 ; Train Loss : 0.079516 ; Train Acc : 0.214 ; Test Loss : 0.079700 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 425 ; Train Loss : 0.079435 ; Train Acc : 0.215 ; Test Loss : 0.079706 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 426 ; Train Loss : 0.079397 ; Train Acc : 0.216 ; Test Loss : 0.079692 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 427 ; Train Loss : 0.079502 ; Train Acc : 0.217 ; Test Loss : 0.079688 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 428 ; Train Loss : 0.079482 ; Train Acc : 0.216 ; Test Loss : 0.079708 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 429 ; Train Loss : 0.079477 ; Train Acc : 0.216 ; Test Loss : 0.079775 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 430 ; Train Loss : 0.079696 ; Train Acc : 0.216 ; Test Loss : 0.079795 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 431 ; Train Loss : 0.079446 ; Train Acc : 0.216 ; Test Loss : 0.079698 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 432 ; Train Loss : 0.079447 ; Train Acc : 0.213 ; Test Loss : 0.079689 ; Test Acc : 0.188 ; LR : 0.019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 433 ; Train Loss : 0.079634 ; Train Acc : 0.216 ; Test Loss : 0.079683 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 434 ; Train Loss : 0.079578 ; Train Acc : 0.216 ; Test Loss : 0.079719 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 435 ; Train Loss : 0.079426 ; Train Acc : 0.217 ; Test Loss : 0.079688 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 436 ; Train Loss : 0.079674 ; Train Acc : 0.212 ; Test Loss : 0.079700 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 437 ; Train Loss : 0.079211 ; Train Acc : 0.207 ; Test Loss : 0.079717 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 438 ; Train Loss : 0.079305 ; Train Acc : 0.214 ; Test Loss : 0.079753 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 439 ; Train Loss : 0.079521 ; Train Acc : 0.212 ; Test Loss : 0.079707 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 440 ; Train Loss : 0.079229 ; Train Acc : 0.217 ; Test Loss : 0.079695 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 441 ; Train Loss : 0.079486 ; Train Acc : 0.212 ; Test Loss : 0.079680 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 442 ; Train Loss : 0.079382 ; Train Acc : 0.210 ; Test Loss : 0.079718 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 443 ; Train Loss : 0.079495 ; Train Acc : 0.212 ; Test Loss : 0.079701 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 444 ; Train Loss : 0.079569 ; Train Acc : 0.216 ; Test Loss : 0.079699 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 445 ; Train Loss : 0.079451 ; Train Acc : 0.217 ; Test Loss : 0.079699 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 446 ; Train Loss : 0.079357 ; Train Acc : 0.213 ; Test Loss : 0.079685 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 447 ; Train Loss : 0.079437 ; Train Acc : 0.212 ; Test Loss : 0.079714 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 448 ; Train Loss : 0.079401 ; Train Acc : 0.216 ; Test Loss : 0.079691 ; Test Acc : 0.188 ; LR : 0.019\n",
      "Epoch : 449 ; Train Loss : 0.079468 ; Train Acc : 0.219 ; Test Loss : 0.079706 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 450 ; Train Loss : 0.079483 ; Train Acc : 0.217 ; Test Loss : 0.079705 ; Test Acc : 0.250 ; LR : 0.019\n",
      "Epoch : 451 ; Train Loss : 0.079371 ; Train Acc : 0.215 ; Test Loss : 0.079690 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 452 ; Train Loss : 0.079535 ; Train Acc : 0.207 ; Test Loss : 0.079695 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 453 ; Train Loss : 0.079573 ; Train Acc : 0.215 ; Test Loss : 0.079739 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 454 ; Train Loss : 0.079432 ; Train Acc : 0.212 ; Test Loss : 0.079693 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 455 ; Train Loss : 0.079437 ; Train Acc : 0.216 ; Test Loss : 0.079702 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 456 ; Train Loss : 0.079427 ; Train Acc : 0.216 ; Test Loss : 0.079707 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 457 ; Train Loss : 0.079385 ; Train Acc : 0.216 ; Test Loss : 0.079701 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 458 ; Train Loss : 0.079495 ; Train Acc : 0.213 ; Test Loss : 0.079706 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 459 ; Train Loss : 0.079362 ; Train Acc : 0.217 ; Test Loss : 0.079692 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 460 ; Train Loss : 0.079337 ; Train Acc : 0.217 ; Test Loss : 0.079710 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 461 ; Train Loss : 0.079479 ; Train Acc : 0.212 ; Test Loss : 0.079692 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 462 ; Train Loss : 0.079416 ; Train Acc : 0.209 ; Test Loss : 0.079741 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 463 ; Train Loss : 0.079438 ; Train Acc : 0.209 ; Test Loss : 0.079700 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 464 ; Train Loss : 0.079348 ; Train Acc : 0.214 ; Test Loss : 0.079703 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 465 ; Train Loss : 0.079607 ; Train Acc : 0.216 ; Test Loss : 0.079697 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 466 ; Train Loss : 0.079337 ; Train Acc : 0.217 ; Test Loss : 0.079691 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 467 ; Train Loss : 0.079276 ; Train Acc : 0.217 ; Test Loss : 0.079738 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 468 ; Train Loss : 0.079602 ; Train Acc : 0.216 ; Test Loss : 0.079698 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 469 ; Train Loss : 0.079410 ; Train Acc : 0.216 ; Test Loss : 0.079699 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 470 ; Train Loss : 0.079430 ; Train Acc : 0.216 ; Test Loss : 0.079708 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 471 ; Train Loss : 0.079297 ; Train Acc : 0.213 ; Test Loss : 0.079700 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 472 ; Train Loss : 0.079535 ; Train Acc : 0.214 ; Test Loss : 0.079700 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 473 ; Train Loss : 0.079169 ; Train Acc : 0.215 ; Test Loss : 0.079698 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 474 ; Train Loss : 0.079539 ; Train Acc : 0.217 ; Test Loss : 0.079696 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 475 ; Train Loss : 0.079339 ; Train Acc : 0.216 ; Test Loss : 0.079698 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 476 ; Train Loss : 0.079443 ; Train Acc : 0.216 ; Test Loss : 0.079721 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 477 ; Train Loss : 0.079421 ; Train Acc : 0.215 ; Test Loss : 0.079706 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 478 ; Train Loss : 0.079537 ; Train Acc : 0.209 ; Test Loss : 0.079715 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 479 ; Train Loss : 0.079362 ; Train Acc : 0.216 ; Test Loss : 0.079694 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 480 ; Train Loss : 0.079333 ; Train Acc : 0.214 ; Test Loss : 0.079795 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 481 ; Train Loss : 0.079626 ; Train Acc : 0.211 ; Test Loss : 0.079748 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 482 ; Train Loss : 0.079324 ; Train Acc : 0.208 ; Test Loss : 0.079766 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 483 ; Train Loss : 0.079623 ; Train Acc : 0.218 ; Test Loss : 0.079732 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 484 ; Train Loss : 0.079467 ; Train Acc : 0.205 ; Test Loss : 0.079706 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 485 ; Train Loss : 0.079423 ; Train Acc : 0.216 ; Test Loss : 0.079704 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 486 ; Train Loss : 0.079542 ; Train Acc : 0.221 ; Test Loss : 0.079693 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 487 ; Train Loss : 0.079289 ; Train Acc : 0.217 ; Test Loss : 0.079707 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 488 ; Train Loss : 0.079299 ; Train Acc : 0.217 ; Test Loss : 0.079713 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 489 ; Train Loss : 0.079548 ; Train Acc : 0.209 ; Test Loss : 0.079702 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 490 ; Train Loss : 0.079349 ; Train Acc : 0.217 ; Test Loss : 0.079705 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 491 ; Train Loss : 0.079371 ; Train Acc : 0.217 ; Test Loss : 0.079710 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 492 ; Train Loss : 0.079426 ; Train Acc : 0.216 ; Test Loss : 0.079841 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 493 ; Train Loss : 0.079302 ; Train Acc : 0.210 ; Test Loss : 0.079730 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 494 ; Train Loss : 0.079475 ; Train Acc : 0.217 ; Test Loss : 0.079693 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 495 ; Train Loss : 0.079460 ; Train Acc : 0.217 ; Test Loss : 0.079746 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 496 ; Train Loss : 0.079564 ; Train Acc : 0.216 ; Test Loss : 0.079705 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 497 ; Train Loss : 0.079341 ; Train Acc : 0.212 ; Test Loss : 0.079700 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 498 ; Train Loss : 0.079209 ; Train Acc : 0.207 ; Test Loss : 0.079704 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 499 ; Train Loss : 0.079343 ; Train Acc : 0.210 ; Test Loss : 0.079716 ; Test Acc : 0.250 ; LR : 0.017\n",
      "Epoch : 500 ; Train Loss : 0.079564 ; Train Acc : 0.217 ; Test Loss : 0.079707 ; Test Acc : 0.188 ; LR : 0.017\n",
      "Epoch : 501 ; Train Loss : 0.079218 ; Train Acc : 0.216 ; Test Loss : 0.079706 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 502 ; Train Loss : 0.079535 ; Train Acc : 0.217 ; Test Loss : 0.079701 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 503 ; Train Loss : 0.079457 ; Train Acc : 0.217 ; Test Loss : 0.079693 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 504 ; Train Loss : 0.079399 ; Train Acc : 0.217 ; Test Loss : 0.079709 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 505 ; Train Loss : 0.079536 ; Train Acc : 0.217 ; Test Loss : 0.079710 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 506 ; Train Loss : 0.079539 ; Train Acc : 0.212 ; Test Loss : 0.079756 ; Test Acc : 0.250 ; LR : 0.016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 507 ; Train Loss : 0.079552 ; Train Acc : 0.213 ; Test Loss : 0.079700 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 508 ; Train Loss : 0.079352 ; Train Acc : 0.215 ; Test Loss : 0.079703 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 509 ; Train Loss : 0.079373 ; Train Acc : 0.210 ; Test Loss : 0.079709 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 510 ; Train Loss : 0.079342 ; Train Acc : 0.214 ; Test Loss : 0.079711 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 511 ; Train Loss : 0.079283 ; Train Acc : 0.204 ; Test Loss : 0.079703 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 512 ; Train Loss : 0.079473 ; Train Acc : 0.212 ; Test Loss : 0.079725 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 513 ; Train Loss : 0.079532 ; Train Acc : 0.214 ; Test Loss : 0.079700 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 514 ; Train Loss : 0.079330 ; Train Acc : 0.216 ; Test Loss : 0.079705 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 515 ; Train Loss : 0.079212 ; Train Acc : 0.216 ; Test Loss : 0.079703 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 516 ; Train Loss : 0.079266 ; Train Acc : 0.217 ; Test Loss : 0.079705 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 517 ; Train Loss : 0.079360 ; Train Acc : 0.212 ; Test Loss : 0.079698 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 518 ; Train Loss : 0.079586 ; Train Acc : 0.216 ; Test Loss : 0.079714 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 519 ; Train Loss : 0.079490 ; Train Acc : 0.211 ; Test Loss : 0.079726 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 520 ; Train Loss : 0.079524 ; Train Acc : 0.213 ; Test Loss : 0.079703 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 521 ; Train Loss : 0.079396 ; Train Acc : 0.217 ; Test Loss : 0.079714 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 522 ; Train Loss : 0.079273 ; Train Acc : 0.214 ; Test Loss : 0.079705 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 523 ; Train Loss : 0.079424 ; Train Acc : 0.214 ; Test Loss : 0.079719 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 524 ; Train Loss : 0.079457 ; Train Acc : 0.219 ; Test Loss : 0.079702 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 525 ; Train Loss : 0.079519 ; Train Acc : 0.217 ; Test Loss : 0.079697 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 526 ; Train Loss : 0.079480 ; Train Acc : 0.212 ; Test Loss : 0.079738 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 527 ; Train Loss : 0.079414 ; Train Acc : 0.216 ; Test Loss : 0.079718 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 528 ; Train Loss : 0.079333 ; Train Acc : 0.217 ; Test Loss : 0.079700 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 529 ; Train Loss : 0.079460 ; Train Acc : 0.217 ; Test Loss : 0.079710 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 530 ; Train Loss : 0.079258 ; Train Acc : 0.213 ; Test Loss : 0.079715 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 531 ; Train Loss : 0.079533 ; Train Acc : 0.215 ; Test Loss : 0.079706 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 532 ; Train Loss : 0.079482 ; Train Acc : 0.208 ; Test Loss : 0.079703 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 533 ; Train Loss : 0.079474 ; Train Acc : 0.215 ; Test Loss : 0.079694 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 534 ; Train Loss : 0.079525 ; Train Acc : 0.211 ; Test Loss : 0.079711 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 535 ; Train Loss : 0.079188 ; Train Acc : 0.216 ; Test Loss : 0.079698 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 536 ; Train Loss : 0.079207 ; Train Acc : 0.217 ; Test Loss : 0.079709 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 537 ; Train Loss : 0.079540 ; Train Acc : 0.216 ; Test Loss : 0.079726 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 538 ; Train Loss : 0.079459 ; Train Acc : 0.211 ; Test Loss : 0.079705 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 539 ; Train Loss : 0.079279 ; Train Acc : 0.216 ; Test Loss : 0.079706 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 540 ; Train Loss : 0.079575 ; Train Acc : 0.216 ; Test Loss : 0.079718 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 541 ; Train Loss : 0.079486 ; Train Acc : 0.210 ; Test Loss : 0.079697 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 542 ; Train Loss : 0.079594 ; Train Acc : 0.216 ; Test Loss : 0.079717 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 543 ; Train Loss : 0.079392 ; Train Acc : 0.217 ; Test Loss : 0.079703 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 544 ; Train Loss : 0.079468 ; Train Acc : 0.204 ; Test Loss : 0.079702 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 545 ; Train Loss : 0.079329 ; Train Acc : 0.217 ; Test Loss : 0.079703 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 546 ; Train Loss : 0.079474 ; Train Acc : 0.215 ; Test Loss : 0.079710 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 547 ; Train Loss : 0.079270 ; Train Acc : 0.216 ; Test Loss : 0.079694 ; Test Acc : 0.188 ; LR : 0.016\n",
      "Epoch : 548 ; Train Loss : 0.079331 ; Train Acc : 0.211 ; Test Loss : 0.079708 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 549 ; Train Loss : 0.079454 ; Train Acc : 0.212 ; Test Loss : 0.079711 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 550 ; Train Loss : 0.079337 ; Train Acc : 0.216 ; Test Loss : 0.079764 ; Test Acc : 0.250 ; LR : 0.016\n",
      "Epoch : 551 ; Train Loss : 0.079352 ; Train Acc : 0.215 ; Test Loss : 0.079712 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 552 ; Train Loss : 0.079184 ; Train Acc : 0.217 ; Test Loss : 0.079697 ; Test Acc : 0.250 ; LR : 0.014\n",
      "Epoch : 553 ; Train Loss : 0.079601 ; Train Acc : 0.216 ; Test Loss : 0.079707 ; Test Acc : 0.250 ; LR : 0.014\n",
      "Epoch : 554 ; Train Loss : 0.079397 ; Train Acc : 0.217 ; Test Loss : 0.079697 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 555 ; Train Loss : 0.079316 ; Train Acc : 0.213 ; Test Loss : 0.079701 ; Test Acc : 0.250 ; LR : 0.014\n",
      "Epoch : 556 ; Train Loss : 0.079517 ; Train Acc : 0.213 ; Test Loss : 0.079703 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 557 ; Train Loss : 0.079575 ; Train Acc : 0.212 ; Test Loss : 0.079710 ; Test Acc : 0.250 ; LR : 0.014\n",
      "Epoch : 558 ; Train Loss : 0.079472 ; Train Acc : 0.211 ; Test Loss : 0.079702 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 559 ; Train Loss : 0.079383 ; Train Acc : 0.217 ; Test Loss : 0.079713 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 560 ; Train Loss : 0.079461 ; Train Acc : 0.206 ; Test Loss : 0.079713 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 561 ; Train Loss : 0.079447 ; Train Acc : 0.213 ; Test Loss : 0.079708 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 562 ; Train Loss : 0.079386 ; Train Acc : 0.217 ; Test Loss : 0.079697 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 563 ; Train Loss : 0.079476 ; Train Acc : 0.214 ; Test Loss : 0.079718 ; Test Acc : 0.250 ; LR : 0.014\n",
      "Epoch : 564 ; Train Loss : 0.079400 ; Train Acc : 0.211 ; Test Loss : 0.079700 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 565 ; Train Loss : 0.079438 ; Train Acc : 0.217 ; Test Loss : 0.079700 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 566 ; Train Loss : 0.079441 ; Train Acc : 0.217 ; Test Loss : 0.079729 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 567 ; Train Loss : 0.079462 ; Train Acc : 0.217 ; Test Loss : 0.079721 ; Test Acc : 0.250 ; LR : 0.014\n",
      "Epoch : 568 ; Train Loss : 0.079347 ; Train Acc : 0.207 ; Test Loss : 0.079719 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 569 ; Train Loss : 0.079332 ; Train Acc : 0.211 ; Test Loss : 0.079752 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 570 ; Train Loss : 0.079313 ; Train Acc : 0.211 ; Test Loss : 0.079699 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 571 ; Train Loss : 0.079271 ; Train Acc : 0.216 ; Test Loss : 0.079734 ; Test Acc : 0.250 ; LR : 0.014\n",
      "Epoch : 572 ; Train Loss : 0.079451 ; Train Acc : 0.217 ; Test Loss : 0.079747 ; Test Acc : 0.250 ; LR : 0.014\n",
      "Epoch : 573 ; Train Loss : 0.079352 ; Train Acc : 0.206 ; Test Loss : 0.079779 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 574 ; Train Loss : 0.079554 ; Train Acc : 0.217 ; Test Loss : 0.079725 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 575 ; Train Loss : 0.079291 ; Train Acc : 0.217 ; Test Loss : 0.079732 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 576 ; Train Loss : 0.079591 ; Train Acc : 0.218 ; Test Loss : 0.079722 ; Test Acc : 0.250 ; LR : 0.014\n",
      "Epoch : 577 ; Train Loss : 0.079444 ; Train Acc : 0.217 ; Test Loss : 0.079727 ; Test Acc : 0.250 ; LR : 0.014\n",
      "Epoch : 578 ; Train Loss : 0.079407 ; Train Acc : 0.216 ; Test Loss : 0.079722 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 579 ; Train Loss : 0.079590 ; Train Acc : 0.217 ; Test Loss : 0.079717 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 580 ; Train Loss : 0.079520 ; Train Acc : 0.211 ; Test Loss : 0.079718 ; Test Acc : 0.250 ; LR : 0.014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 581 ; Train Loss : 0.079298 ; Train Acc : 0.217 ; Test Loss : 0.079715 ; Test Acc : 0.250 ; LR : 0.014\n",
      "Epoch : 582 ; Train Loss : 0.079531 ; Train Acc : 0.214 ; Test Loss : 0.079717 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 583 ; Train Loss : 0.079258 ; Train Acc : 0.214 ; Test Loss : 0.079713 ; Test Acc : 0.250 ; LR : 0.014\n",
      "Epoch : 584 ; Train Loss : 0.079515 ; Train Acc : 0.218 ; Test Loss : 0.079716 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 585 ; Train Loss : 0.079381 ; Train Acc : 0.217 ; Test Loss : 0.079730 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 586 ; Train Loss : 0.079393 ; Train Acc : 0.217 ; Test Loss : 0.079736 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 587 ; Train Loss : 0.079393 ; Train Acc : 0.217 ; Test Loss : 0.079740 ; Test Acc : 0.250 ; LR : 0.014\n",
      "Epoch : 588 ; Train Loss : 0.079378 ; Train Acc : 0.217 ; Test Loss : 0.079715 ; Test Acc : 0.250 ; LR : 0.014\n",
      "Epoch : 589 ; Train Loss : 0.079519 ; Train Acc : 0.209 ; Test Loss : 0.079720 ; Test Acc : 0.250 ; LR : 0.014\n",
      "Epoch : 590 ; Train Loss : 0.079194 ; Train Acc : 0.210 ; Test Loss : 0.079720 ; Test Acc : 0.250 ; LR : 0.014\n",
      "Epoch : 591 ; Train Loss : 0.079447 ; Train Acc : 0.215 ; Test Loss : 0.079712 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 592 ; Train Loss : 0.079260 ; Train Acc : 0.217 ; Test Loss : 0.079724 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 593 ; Train Loss : 0.079333 ; Train Acc : 0.217 ; Test Loss : 0.079716 ; Test Acc : 0.250 ; LR : 0.014\n",
      "Epoch : 594 ; Train Loss : 0.079280 ; Train Acc : 0.213 ; Test Loss : 0.079723 ; Test Acc : 0.250 ; LR : 0.014\n",
      "Epoch : 595 ; Train Loss : 0.079163 ; Train Acc : 0.214 ; Test Loss : 0.079739 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 596 ; Train Loss : 0.079324 ; Train Acc : 0.217 ; Test Loss : 0.079755 ; Test Acc : 0.250 ; LR : 0.014\n",
      "Epoch : 597 ; Train Loss : 0.079600 ; Train Acc : 0.214 ; Test Loss : 0.079702 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 598 ; Train Loss : 0.079447 ; Train Acc : 0.216 ; Test Loss : 0.079720 ; Test Acc : 0.250 ; LR : 0.014\n",
      "Epoch : 599 ; Train Loss : 0.079214 ; Train Acc : 0.213 ; Test Loss : 0.079714 ; Test Acc : 0.188 ; LR : 0.014\n",
      "Epoch : 600 ; Train Loss : 0.079382 ; Train Acc : 0.210 ; Test Loss : 0.079707 ; Test Acc : 0.250 ; LR : 0.014\n",
      "\n",
      "Hidden Neurons : 10 ; Train Loss : 0.079577 ; Train Acc : 0.217 ; Test Loss : 0.079719 ; Test Acc : 0.188\n",
      "\n",
      "\n",
      "Epoch : 1 ; Train Loss : 0.132467 ; Train Acc : 0.121 ; Test Loss : 0.091179 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 2 ; Train Loss : 0.091071 ; Train Acc : 0.098 ; Test Loss : 0.090043 ; Test Acc : 0.062 ; LR : 0.045\n",
      "Epoch : 3 ; Train Loss : 0.089896 ; Train Acc : 0.113 ; Test Loss : 0.089495 ; Test Acc : 0.188 ; LR : 0.045\n",
      "Epoch : 4 ; Train Loss : 0.088542 ; Train Acc : 0.152 ; Test Loss : 0.087701 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 5 ; Train Loss : 0.086528 ; Train Acc : 0.191 ; Test Loss : 0.086230 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 6 ; Train Loss : 0.085153 ; Train Acc : 0.213 ; Test Loss : 0.084987 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 7 ; Train Loss : 0.083777 ; Train Acc : 0.242 ; Test Loss : 0.083344 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 8 ; Train Loss : 0.081607 ; Train Acc : 0.263 ; Test Loss : 0.080919 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 9 ; Train Loss : 0.079814 ; Train Acc : 0.281 ; Test Loss : 0.079179 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 10 ; Train Loss : 0.078113 ; Train Acc : 0.298 ; Test Loss : 0.078062 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 11 ; Train Loss : 0.077205 ; Train Acc : 0.297 ; Test Loss : 0.077495 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 12 ; Train Loss : 0.076757 ; Train Acc : 0.297 ; Test Loss : 0.077150 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 13 ; Train Loss : 0.075789 ; Train Acc : 0.301 ; Test Loss : 0.077199 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 14 ; Train Loss : 0.075897 ; Train Acc : 0.297 ; Test Loss : 0.077047 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 15 ; Train Loss : 0.075636 ; Train Acc : 0.289 ; Test Loss : 0.076282 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 16 ; Train Loss : 0.075473 ; Train Acc : 0.302 ; Test Loss : 0.076182 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 17 ; Train Loss : 0.075029 ; Train Acc : 0.292 ; Test Loss : 0.076326 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 18 ; Train Loss : 0.075090 ; Train Acc : 0.300 ; Test Loss : 0.076039 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 19 ; Train Loss : 0.075088 ; Train Acc : 0.302 ; Test Loss : 0.075927 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 20 ; Train Loss : 0.075027 ; Train Acc : 0.301 ; Test Loss : 0.075778 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 21 ; Train Loss : 0.074711 ; Train Acc : 0.296 ; Test Loss : 0.075789 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 22 ; Train Loss : 0.074781 ; Train Acc : 0.304 ; Test Loss : 0.075750 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 23 ; Train Loss : 0.074586 ; Train Acc : 0.304 ; Test Loss : 0.075800 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 24 ; Train Loss : 0.074386 ; Train Acc : 0.303 ; Test Loss : 0.075764 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 25 ; Train Loss : 0.074409 ; Train Acc : 0.291 ; Test Loss : 0.075676 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 26 ; Train Loss : 0.074535 ; Train Acc : 0.289 ; Test Loss : 0.075803 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 27 ; Train Loss : 0.074481 ; Train Acc : 0.298 ; Test Loss : 0.075737 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 28 ; Train Loss : 0.074378 ; Train Acc : 0.300 ; Test Loss : 0.075633 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 29 ; Train Loss : 0.074253 ; Train Acc : 0.302 ; Test Loss : 0.075512 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 30 ; Train Loss : 0.074217 ; Train Acc : 0.302 ; Test Loss : 0.075501 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 31 ; Train Loss : 0.074282 ; Train Acc : 0.297 ; Test Loss : 0.075565 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 32 ; Train Loss : 0.074399 ; Train Acc : 0.299 ; Test Loss : 0.075541 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 33 ; Train Loss : 0.074240 ; Train Acc : 0.304 ; Test Loss : 0.075608 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 34 ; Train Loss : 0.074055 ; Train Acc : 0.300 ; Test Loss : 0.075381 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 35 ; Train Loss : 0.074047 ; Train Acc : 0.300 ; Test Loss : 0.075399 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 36 ; Train Loss : 0.073967 ; Train Acc : 0.300 ; Test Loss : 0.075398 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 37 ; Train Loss : 0.074172 ; Train Acc : 0.302 ; Test Loss : 0.075511 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 38 ; Train Loss : 0.073966 ; Train Acc : 0.299 ; Test Loss : 0.075614 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 39 ; Train Loss : 0.074234 ; Train Acc : 0.302 ; Test Loss : 0.075388 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 40 ; Train Loss : 0.074050 ; Train Acc : 0.301 ; Test Loss : 0.075351 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 41 ; Train Loss : 0.074155 ; Train Acc : 0.299 ; Test Loss : 0.075350 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 42 ; Train Loss : 0.074011 ; Train Acc : 0.300 ; Test Loss : 0.075632 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 43 ; Train Loss : 0.073995 ; Train Acc : 0.306 ; Test Loss : 0.075516 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 44 ; Train Loss : 0.073846 ; Train Acc : 0.301 ; Test Loss : 0.075827 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 45 ; Train Loss : 0.074038 ; Train Acc : 0.289 ; Test Loss : 0.075324 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 46 ; Train Loss : 0.074000 ; Train Acc : 0.302 ; Test Loss : 0.075335 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 47 ; Train Loss : 0.073809 ; Train Acc : 0.303 ; Test Loss : 0.075411 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 48 ; Train Loss : 0.073796 ; Train Acc : 0.294 ; Test Loss : 0.075373 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 49 ; Train Loss : 0.073585 ; Train Acc : 0.302 ; Test Loss : 0.075366 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 50 ; Train Loss : 0.073481 ; Train Acc : 0.301 ; Test Loss : 0.075327 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 51 ; Train Loss : 0.073539 ; Train Acc : 0.300 ; Test Loss : 0.075298 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 52 ; Train Loss : 0.073874 ; Train Acc : 0.302 ; Test Loss : 0.075388 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 53 ; Train Loss : 0.073700 ; Train Acc : 0.302 ; Test Loss : 0.075599 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 54 ; Train Loss : 0.073683 ; Train Acc : 0.302 ; Test Loss : 0.075233 ; Test Acc : 0.375 ; LR : 0.041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 55 ; Train Loss : 0.073630 ; Train Acc : 0.305 ; Test Loss : 0.075426 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 56 ; Train Loss : 0.073688 ; Train Acc : 0.303 ; Test Loss : 0.075567 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 57 ; Train Loss : 0.073658 ; Train Acc : 0.298 ; Test Loss : 0.075239 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 58 ; Train Loss : 0.073610 ; Train Acc : 0.300 ; Test Loss : 0.075333 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 59 ; Train Loss : 0.073626 ; Train Acc : 0.303 ; Test Loss : 0.075563 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 60 ; Train Loss : 0.073694 ; Train Acc : 0.301 ; Test Loss : 0.075270 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 61 ; Train Loss : 0.073718 ; Train Acc : 0.301 ; Test Loss : 0.075282 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 62 ; Train Loss : 0.073750 ; Train Acc : 0.300 ; Test Loss : 0.075399 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 63 ; Train Loss : 0.073462 ; Train Acc : 0.302 ; Test Loss : 0.075272 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 64 ; Train Loss : 0.073633 ; Train Acc : 0.302 ; Test Loss : 0.075279 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 65 ; Train Loss : 0.073591 ; Train Acc : 0.301 ; Test Loss : 0.075446 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 66 ; Train Loss : 0.073591 ; Train Acc : 0.304 ; Test Loss : 0.075250 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 67 ; Train Loss : 0.073567 ; Train Acc : 0.304 ; Test Loss : 0.075306 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 68 ; Train Loss : 0.073312 ; Train Acc : 0.306 ; Test Loss : 0.075434 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 69 ; Train Loss : 0.073515 ; Train Acc : 0.303 ; Test Loss : 0.075280 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 70 ; Train Loss : 0.073491 ; Train Acc : 0.297 ; Test Loss : 0.075417 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 71 ; Train Loss : 0.073427 ; Train Acc : 0.303 ; Test Loss : 0.075269 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 72 ; Train Loss : 0.073307 ; Train Acc : 0.294 ; Test Loss : 0.075294 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 73 ; Train Loss : 0.073491 ; Train Acc : 0.306 ; Test Loss : 0.075342 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 74 ; Train Loss : 0.073349 ; Train Acc : 0.306 ; Test Loss : 0.075291 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 75 ; Train Loss : 0.073364 ; Train Acc : 0.305 ; Test Loss : 0.075409 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 76 ; Train Loss : 0.073535 ; Train Acc : 0.296 ; Test Loss : 0.075427 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 77 ; Train Loss : 0.073475 ; Train Acc : 0.305 ; Test Loss : 0.075324 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 78 ; Train Loss : 0.073181 ; Train Acc : 0.305 ; Test Loss : 0.075369 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 79 ; Train Loss : 0.073278 ; Train Acc : 0.302 ; Test Loss : 0.075500 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 80 ; Train Loss : 0.073498 ; Train Acc : 0.291 ; Test Loss : 0.075382 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 81 ; Train Loss : 0.073345 ; Train Acc : 0.307 ; Test Loss : 0.075600 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 82 ; Train Loss : 0.073120 ; Train Acc : 0.306 ; Test Loss : 0.075453 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 83 ; Train Loss : 0.073329 ; Train Acc : 0.303 ; Test Loss : 0.075349 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 84 ; Train Loss : 0.073016 ; Train Acc : 0.300 ; Test Loss : 0.075374 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 85 ; Train Loss : 0.073124 ; Train Acc : 0.304 ; Test Loss : 0.075292 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 86 ; Train Loss : 0.073284 ; Train Acc : 0.303 ; Test Loss : 0.075322 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 87 ; Train Loss : 0.073140 ; Train Acc : 0.298 ; Test Loss : 0.075415 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 88 ; Train Loss : 0.073249 ; Train Acc : 0.303 ; Test Loss : 0.075316 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 89 ; Train Loss : 0.073163 ; Train Acc : 0.304 ; Test Loss : 0.075395 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 90 ; Train Loss : 0.073417 ; Train Acc : 0.292 ; Test Loss : 0.075443 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 91 ; Train Loss : 0.073337 ; Train Acc : 0.304 ; Test Loss : 0.075306 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 92 ; Train Loss : 0.073095 ; Train Acc : 0.304 ; Test Loss : 0.075302 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 93 ; Train Loss : 0.073341 ; Train Acc : 0.304 ; Test Loss : 0.075310 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 94 ; Train Loss : 0.073008 ; Train Acc : 0.305 ; Test Loss : 0.075351 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 95 ; Train Loss : 0.073098 ; Train Acc : 0.305 ; Test Loss : 0.075432 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 96 ; Train Loss : 0.073078 ; Train Acc : 0.286 ; Test Loss : 0.075320 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 97 ; Train Loss : 0.072914 ; Train Acc : 0.296 ; Test Loss : 0.075378 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 98 ; Train Loss : 0.073106 ; Train Acc : 0.304 ; Test Loss : 0.075372 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 99 ; Train Loss : 0.073092 ; Train Acc : 0.304 ; Test Loss : 0.075428 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 100 ; Train Loss : 0.073103 ; Train Acc : 0.296 ; Test Loss : 0.075430 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 101 ; Train Loss : 0.072923 ; Train Acc : 0.304 ; Test Loss : 0.075431 ; Test Acc : 0.375 ; LR : 0.036\n",
      "Epoch : 102 ; Train Loss : 0.073065 ; Train Acc : 0.310 ; Test Loss : 0.075180 ; Test Acc : 0.375 ; LR : 0.036\n",
      "Epoch : 103 ; Train Loss : 0.072898 ; Train Acc : 0.320 ; Test Loss : 0.074874 ; Test Acc : 0.375 ; LR : 0.036\n",
      "Epoch : 104 ; Train Loss : 0.072492 ; Train Acc : 0.323 ; Test Loss : 0.073855 ; Test Acc : 0.375 ; LR : 0.036\n",
      "Epoch : 105 ; Train Loss : 0.071414 ; Train Acc : 0.326 ; Test Loss : 0.073236 ; Test Acc : 0.375 ; LR : 0.036\n",
      "Epoch : 106 ; Train Loss : 0.070857 ; Train Acc : 0.342 ; Test Loss : 0.072466 ; Test Acc : 0.375 ; LR : 0.036\n",
      "Epoch : 107 ; Train Loss : 0.070258 ; Train Acc : 0.351 ; Test Loss : 0.072272 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 108 ; Train Loss : 0.070039 ; Train Acc : 0.351 ; Test Loss : 0.071801 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 109 ; Train Loss : 0.069528 ; Train Acc : 0.355 ; Test Loss : 0.071738 ; Test Acc : 0.375 ; LR : 0.036\n",
      "Epoch : 110 ; Train Loss : 0.069197 ; Train Acc : 0.357 ; Test Loss : 0.071335 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 111 ; Train Loss : 0.068925 ; Train Acc : 0.363 ; Test Loss : 0.071188 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 112 ; Train Loss : 0.068571 ; Train Acc : 0.366 ; Test Loss : 0.071032 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 113 ; Train Loss : 0.068556 ; Train Acc : 0.368 ; Test Loss : 0.070886 ; Test Acc : 0.375 ; LR : 0.036\n",
      "Epoch : 114 ; Train Loss : 0.068136 ; Train Acc : 0.371 ; Test Loss : 0.070760 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 115 ; Train Loss : 0.068486 ; Train Acc : 0.372 ; Test Loss : 0.070813 ; Test Acc : 0.375 ; LR : 0.036\n",
      "Epoch : 116 ; Train Loss : 0.068257 ; Train Acc : 0.363 ; Test Loss : 0.070538 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 117 ; Train Loss : 0.067917 ; Train Acc : 0.367 ; Test Loss : 0.070485 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 118 ; Train Loss : 0.067725 ; Train Acc : 0.372 ; Test Loss : 0.070551 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 119 ; Train Loss : 0.067955 ; Train Acc : 0.372 ; Test Loss : 0.070279 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 120 ; Train Loss : 0.067351 ; Train Acc : 0.375 ; Test Loss : 0.070699 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 121 ; Train Loss : 0.067386 ; Train Acc : 0.371 ; Test Loss : 0.070501 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 122 ; Train Loss : 0.067588 ; Train Acc : 0.366 ; Test Loss : 0.070238 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 123 ; Train Loss : 0.067292 ; Train Acc : 0.376 ; Test Loss : 0.070102 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 124 ; Train Loss : 0.067198 ; Train Acc : 0.377 ; Test Loss : 0.070106 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 125 ; Train Loss : 0.067133 ; Train Acc : 0.376 ; Test Loss : 0.070269 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 126 ; Train Loss : 0.066946 ; Train Acc : 0.371 ; Test Loss : 0.070021 ; Test Acc : 0.375 ; LR : 0.036\n",
      "Epoch : 127 ; Train Loss : 0.066833 ; Train Acc : 0.376 ; Test Loss : 0.070050 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 128 ; Train Loss : 0.067122 ; Train Acc : 0.377 ; Test Loss : 0.070044 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 129 ; Train Loss : 0.067084 ; Train Acc : 0.367 ; Test Loss : 0.069988 ; Test Acc : 0.438 ; LR : 0.036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 130 ; Train Loss : 0.066799 ; Train Acc : 0.378 ; Test Loss : 0.070039 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 131 ; Train Loss : 0.066638 ; Train Acc : 0.377 ; Test Loss : 0.070444 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 132 ; Train Loss : 0.066751 ; Train Acc : 0.377 ; Test Loss : 0.070099 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 133 ; Train Loss : 0.066990 ; Train Acc : 0.372 ; Test Loss : 0.070028 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 134 ; Train Loss : 0.066844 ; Train Acc : 0.372 ; Test Loss : 0.070248 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 135 ; Train Loss : 0.066707 ; Train Acc : 0.379 ; Test Loss : 0.070077 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 136 ; Train Loss : 0.066819 ; Train Acc : 0.379 ; Test Loss : 0.070229 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 137 ; Train Loss : 0.066554 ; Train Acc : 0.379 ; Test Loss : 0.069842 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 138 ; Train Loss : 0.066339 ; Train Acc : 0.379 ; Test Loss : 0.069850 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 139 ; Train Loss : 0.066492 ; Train Acc : 0.379 ; Test Loss : 0.069885 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 140 ; Train Loss : 0.066534 ; Train Acc : 0.379 ; Test Loss : 0.070155 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 141 ; Train Loss : 0.066403 ; Train Acc : 0.380 ; Test Loss : 0.070096 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 142 ; Train Loss : 0.066360 ; Train Acc : 0.370 ; Test Loss : 0.069866 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 143 ; Train Loss : 0.066358 ; Train Acc : 0.379 ; Test Loss : 0.070219 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 144 ; Train Loss : 0.066208 ; Train Acc : 0.383 ; Test Loss : 0.069845 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 145 ; Train Loss : 0.066227 ; Train Acc : 0.378 ; Test Loss : 0.069824 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 146 ; Train Loss : 0.066178 ; Train Acc : 0.379 ; Test Loss : 0.069820 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 147 ; Train Loss : 0.066564 ; Train Acc : 0.370 ; Test Loss : 0.069790 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 148 ; Train Loss : 0.066454 ; Train Acc : 0.379 ; Test Loss : 0.069960 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 149 ; Train Loss : 0.066165 ; Train Acc : 0.380 ; Test Loss : 0.070056 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 150 ; Train Loss : 0.066578 ; Train Acc : 0.369 ; Test Loss : 0.069945 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 151 ; Train Loss : 0.066344 ; Train Acc : 0.379 ; Test Loss : 0.069771 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 152 ; Train Loss : 0.066432 ; Train Acc : 0.376 ; Test Loss : 0.069956 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 153 ; Train Loss : 0.066205 ; Train Acc : 0.380 ; Test Loss : 0.069805 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 154 ; Train Loss : 0.066125 ; Train Acc : 0.372 ; Test Loss : 0.069873 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 155 ; Train Loss : 0.065979 ; Train Acc : 0.380 ; Test Loss : 0.069909 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 156 ; Train Loss : 0.066236 ; Train Acc : 0.380 ; Test Loss : 0.069963 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 157 ; Train Loss : 0.066492 ; Train Acc : 0.378 ; Test Loss : 0.069887 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 158 ; Train Loss : 0.066243 ; Train Acc : 0.379 ; Test Loss : 0.069910 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 159 ; Train Loss : 0.066078 ; Train Acc : 0.380 ; Test Loss : 0.069876 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 160 ; Train Loss : 0.066216 ; Train Acc : 0.380 ; Test Loss : 0.069733 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 161 ; Train Loss : 0.066312 ; Train Acc : 0.382 ; Test Loss : 0.069741 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 162 ; Train Loss : 0.065980 ; Train Acc : 0.381 ; Test Loss : 0.069848 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 163 ; Train Loss : 0.066125 ; Train Acc : 0.380 ; Test Loss : 0.069869 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 164 ; Train Loss : 0.066137 ; Train Acc : 0.380 ; Test Loss : 0.069811 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 165 ; Train Loss : 0.066113 ; Train Acc : 0.375 ; Test Loss : 0.069752 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 166 ; Train Loss : 0.066100 ; Train Acc : 0.381 ; Test Loss : 0.069868 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 167 ; Train Loss : 0.066189 ; Train Acc : 0.380 ; Test Loss : 0.070205 ; Test Acc : 0.375 ; LR : 0.033\n",
      "Epoch : 168 ; Train Loss : 0.066206 ; Train Acc : 0.376 ; Test Loss : 0.070263 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 169 ; Train Loss : 0.066045 ; Train Acc : 0.380 ; Test Loss : 0.070124 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 170 ; Train Loss : 0.066129 ; Train Acc : 0.380 ; Test Loss : 0.070156 ; Test Acc : 0.375 ; LR : 0.033\n",
      "Epoch : 171 ; Train Loss : 0.065792 ; Train Acc : 0.375 ; Test Loss : 0.070039 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 172 ; Train Loss : 0.065721 ; Train Acc : 0.382 ; Test Loss : 0.069927 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 173 ; Train Loss : 0.065869 ; Train Acc : 0.381 ; Test Loss : 0.069887 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 174 ; Train Loss : 0.065481 ; Train Acc : 0.383 ; Test Loss : 0.069866 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 175 ; Train Loss : 0.065797 ; Train Acc : 0.382 ; Test Loss : 0.069877 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 176 ; Train Loss : 0.065872 ; Train Acc : 0.382 ; Test Loss : 0.070188 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 177 ; Train Loss : 0.065931 ; Train Acc : 0.378 ; Test Loss : 0.070107 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 178 ; Train Loss : 0.066001 ; Train Acc : 0.382 ; Test Loss : 0.070091 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 179 ; Train Loss : 0.065553 ; Train Acc : 0.383 ; Test Loss : 0.069915 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 180 ; Train Loss : 0.065763 ; Train Acc : 0.382 ; Test Loss : 0.070150 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 181 ; Train Loss : 0.065966 ; Train Acc : 0.377 ; Test Loss : 0.070077 ; Test Acc : 0.375 ; LR : 0.033\n",
      "Epoch : 182 ; Train Loss : 0.065630 ; Train Acc : 0.378 ; Test Loss : 0.069918 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 183 ; Train Loss : 0.065702 ; Train Acc : 0.382 ; Test Loss : 0.070063 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 184 ; Train Loss : 0.065825 ; Train Acc : 0.383 ; Test Loss : 0.070105 ; Test Acc : 0.375 ; LR : 0.033\n",
      "Epoch : 185 ; Train Loss : 0.065755 ; Train Acc : 0.383 ; Test Loss : 0.069944 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 186 ; Train Loss : 0.065250 ; Train Acc : 0.383 ; Test Loss : 0.069996 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 187 ; Train Loss : 0.065710 ; Train Acc : 0.376 ; Test Loss : 0.070119 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 188 ; Train Loss : 0.065646 ; Train Acc : 0.382 ; Test Loss : 0.070031 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 189 ; Train Loss : 0.065547 ; Train Acc : 0.379 ; Test Loss : 0.070104 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 190 ; Train Loss : 0.065780 ; Train Acc : 0.381 ; Test Loss : 0.069994 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 191 ; Train Loss : 0.065503 ; Train Acc : 0.383 ; Test Loss : 0.070154 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 192 ; Train Loss : 0.065638 ; Train Acc : 0.382 ; Test Loss : 0.070057 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 193 ; Train Loss : 0.065530 ; Train Acc : 0.384 ; Test Loss : 0.070033 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 194 ; Train Loss : 0.065504 ; Train Acc : 0.376 ; Test Loss : 0.070012 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 195 ; Train Loss : 0.065524 ; Train Acc : 0.383 ; Test Loss : 0.070420 ; Test Acc : 0.375 ; LR : 0.033\n",
      "Epoch : 196 ; Train Loss : 0.065510 ; Train Acc : 0.376 ; Test Loss : 0.070133 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 197 ; Train Loss : 0.065521 ; Train Acc : 0.380 ; Test Loss : 0.070041 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 198 ; Train Loss : 0.065518 ; Train Acc : 0.384 ; Test Loss : 0.070253 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 199 ; Train Loss : 0.065524 ; Train Acc : 0.382 ; Test Loss : 0.070150 ; Test Acc : 0.375 ; LR : 0.033\n",
      "Epoch : 200 ; Train Loss : 0.065616 ; Train Acc : 0.382 ; Test Loss : 0.070202 ; Test Acc : 0.375 ; LR : 0.033\n",
      "Epoch : 201 ; Train Loss : 0.065599 ; Train Acc : 0.373 ; Test Loss : 0.070211 ; Test Acc : 0.375 ; LR : 0.030\n",
      "Epoch : 202 ; Train Loss : 0.065502 ; Train Acc : 0.383 ; Test Loss : 0.070152 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 203 ; Train Loss : 0.065568 ; Train Acc : 0.375 ; Test Loss : 0.070198 ; Test Acc : 0.375 ; LR : 0.030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 204 ; Train Loss : 0.065263 ; Train Acc : 0.384 ; Test Loss : 0.070220 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 205 ; Train Loss : 0.065403 ; Train Acc : 0.375 ; Test Loss : 0.070193 ; Test Acc : 0.312 ; LR : 0.030\n",
      "Epoch : 206 ; Train Loss : 0.065425 ; Train Acc : 0.383 ; Test Loss : 0.070100 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 207 ; Train Loss : 0.065478 ; Train Acc : 0.384 ; Test Loss : 0.070225 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 208 ; Train Loss : 0.065427 ; Train Acc : 0.385 ; Test Loss : 0.070176 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 209 ; Train Loss : 0.065124 ; Train Acc : 0.377 ; Test Loss : 0.070408 ; Test Acc : 0.375 ; LR : 0.030\n",
      "Epoch : 210 ; Train Loss : 0.065440 ; Train Acc : 0.384 ; Test Loss : 0.070325 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 211 ; Train Loss : 0.065178 ; Train Acc : 0.373 ; Test Loss : 0.070297 ; Test Acc : 0.375 ; LR : 0.030\n",
      "Epoch : 212 ; Train Loss : 0.065445 ; Train Acc : 0.384 ; Test Loss : 0.070295 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 213 ; Train Loss : 0.065500 ; Train Acc : 0.382 ; Test Loss : 0.070410 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 214 ; Train Loss : 0.065340 ; Train Acc : 0.378 ; Test Loss : 0.070598 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 215 ; Train Loss : 0.065405 ; Train Acc : 0.384 ; Test Loss : 0.070314 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 216 ; Train Loss : 0.065836 ; Train Acc : 0.378 ; Test Loss : 0.070070 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 217 ; Train Loss : 0.065035 ; Train Acc : 0.383 ; Test Loss : 0.070111 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 218 ; Train Loss : 0.065042 ; Train Acc : 0.384 ; Test Loss : 0.070269 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 219 ; Train Loss : 0.065473 ; Train Acc : 0.384 ; Test Loss : 0.070163 ; Test Acc : 0.375 ; LR : 0.030\n",
      "Epoch : 220 ; Train Loss : 0.065373 ; Train Acc : 0.383 ; Test Loss : 0.070290 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 221 ; Train Loss : 0.065130 ; Train Acc : 0.377 ; Test Loss : 0.070471 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 222 ; Train Loss : 0.065234 ; Train Acc : 0.377 ; Test Loss : 0.070204 ; Test Acc : 0.375 ; LR : 0.030\n",
      "Epoch : 223 ; Train Loss : 0.065523 ; Train Acc : 0.383 ; Test Loss : 0.070690 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 224 ; Train Loss : 0.065015 ; Train Acc : 0.386 ; Test Loss : 0.070365 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 225 ; Train Loss : 0.065377 ; Train Acc : 0.384 ; Test Loss : 0.070310 ; Test Acc : 0.375 ; LR : 0.030\n",
      "Epoch : 226 ; Train Loss : 0.065119 ; Train Acc : 0.384 ; Test Loss : 0.070515 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 227 ; Train Loss : 0.065424 ; Train Acc : 0.372 ; Test Loss : 0.070238 ; Test Acc : 0.375 ; LR : 0.030\n",
      "Epoch : 228 ; Train Loss : 0.065167 ; Train Acc : 0.377 ; Test Loss : 0.070216 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 229 ; Train Loss : 0.065190 ; Train Acc : 0.386 ; Test Loss : 0.070261 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 230 ; Train Loss : 0.065489 ; Train Acc : 0.384 ; Test Loss : 0.070256 ; Test Acc : 0.375 ; LR : 0.030\n",
      "Epoch : 231 ; Train Loss : 0.065408 ; Train Acc : 0.376 ; Test Loss : 0.070543 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 232 ; Train Loss : 0.065179 ; Train Acc : 0.386 ; Test Loss : 0.070257 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 233 ; Train Loss : 0.065208 ; Train Acc : 0.384 ; Test Loss : 0.070400 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 234 ; Train Loss : 0.064816 ; Train Acc : 0.385 ; Test Loss : 0.070666 ; Test Acc : 0.375 ; LR : 0.030\n",
      "Epoch : 235 ; Train Loss : 0.065169 ; Train Acc : 0.385 ; Test Loss : 0.070299 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 236 ; Train Loss : 0.065012 ; Train Acc : 0.382 ; Test Loss : 0.070386 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 237 ; Train Loss : 0.065121 ; Train Acc : 0.385 ; Test Loss : 0.070405 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 238 ; Train Loss : 0.065017 ; Train Acc : 0.386 ; Test Loss : 0.070665 ; Test Acc : 0.375 ; LR : 0.030\n",
      "Epoch : 239 ; Train Loss : 0.065551 ; Train Acc : 0.383 ; Test Loss : 0.070584 ; Test Acc : 0.375 ; LR : 0.030\n",
      "Epoch : 240 ; Train Loss : 0.064947 ; Train Acc : 0.387 ; Test Loss : 0.070429 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 241 ; Train Loss : 0.065529 ; Train Acc : 0.384 ; Test Loss : 0.070340 ; Test Acc : 0.375 ; LR : 0.030\n",
      "Epoch : 242 ; Train Loss : 0.065101 ; Train Acc : 0.385 ; Test Loss : 0.070324 ; Test Acc : 0.375 ; LR : 0.030\n",
      "Epoch : 243 ; Train Loss : 0.065169 ; Train Acc : 0.376 ; Test Loss : 0.070382 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 244 ; Train Loss : 0.065105 ; Train Acc : 0.386 ; Test Loss : 0.070702 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 245 ; Train Loss : 0.065148 ; Train Acc : 0.386 ; Test Loss : 0.070441 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 246 ; Train Loss : 0.065332 ; Train Acc : 0.377 ; Test Loss : 0.070371 ; Test Acc : 0.375 ; LR : 0.030\n",
      "Epoch : 247 ; Train Loss : 0.065037 ; Train Acc : 0.385 ; Test Loss : 0.070608 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 248 ; Train Loss : 0.064714 ; Train Acc : 0.386 ; Test Loss : 0.070408 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 249 ; Train Loss : 0.064691 ; Train Acc : 0.385 ; Test Loss : 0.070597 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 250 ; Train Loss : 0.065129 ; Train Acc : 0.385 ; Test Loss : 0.070550 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 251 ; Train Loss : 0.065231 ; Train Acc : 0.386 ; Test Loss : 0.070490 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 252 ; Train Loss : 0.065230 ; Train Acc : 0.386 ; Test Loss : 0.070985 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 253 ; Train Loss : 0.065257 ; Train Acc : 0.385 ; Test Loss : 0.070651 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 254 ; Train Loss : 0.065456 ; Train Acc : 0.384 ; Test Loss : 0.070815 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 255 ; Train Loss : 0.065214 ; Train Acc : 0.382 ; Test Loss : 0.070955 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 256 ; Train Loss : 0.065178 ; Train Acc : 0.387 ; Test Loss : 0.070682 ; Test Acc : 0.375 ; LR : 0.027\n",
      "Epoch : 257 ; Train Loss : 0.065065 ; Train Acc : 0.385 ; Test Loss : 0.070437 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 258 ; Train Loss : 0.065289 ; Train Acc : 0.386 ; Test Loss : 0.070415 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 259 ; Train Loss : 0.064890 ; Train Acc : 0.380 ; Test Loss : 0.070486 ; Test Acc : 0.375 ; LR : 0.027\n",
      "Epoch : 260 ; Train Loss : 0.064942 ; Train Acc : 0.386 ; Test Loss : 0.070674 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 261 ; Train Loss : 0.064947 ; Train Acc : 0.386 ; Test Loss : 0.070877 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 262 ; Train Loss : 0.064896 ; Train Acc : 0.386 ; Test Loss : 0.070718 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 263 ; Train Loss : 0.065211 ; Train Acc : 0.385 ; Test Loss : 0.070529 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 264 ; Train Loss : 0.064977 ; Train Acc : 0.382 ; Test Loss : 0.070729 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 265 ; Train Loss : 0.065049 ; Train Acc : 0.387 ; Test Loss : 0.070834 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 266 ; Train Loss : 0.064887 ; Train Acc : 0.386 ; Test Loss : 0.070566 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 267 ; Train Loss : 0.065137 ; Train Acc : 0.386 ; Test Loss : 0.070419 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 268 ; Train Loss : 0.064943 ; Train Acc : 0.386 ; Test Loss : 0.070571 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 269 ; Train Loss : 0.064928 ; Train Acc : 0.387 ; Test Loss : 0.070657 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 270 ; Train Loss : 0.064744 ; Train Acc : 0.378 ; Test Loss : 0.070451 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 271 ; Train Loss : 0.064931 ; Train Acc : 0.385 ; Test Loss : 0.070600 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 272 ; Train Loss : 0.064658 ; Train Acc : 0.387 ; Test Loss : 0.070486 ; Test Acc : 0.375 ; LR : 0.027\n",
      "Epoch : 273 ; Train Loss : 0.064960 ; Train Acc : 0.387 ; Test Loss : 0.070871 ; Test Acc : 0.375 ; LR : 0.027\n",
      "Epoch : 274 ; Train Loss : 0.064872 ; Train Acc : 0.378 ; Test Loss : 0.070545 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 275 ; Train Loss : 0.065022 ; Train Acc : 0.384 ; Test Loss : 0.070842 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 276 ; Train Loss : 0.064919 ; Train Acc : 0.377 ; Test Loss : 0.070614 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 277 ; Train Loss : 0.064600 ; Train Acc : 0.384 ; Test Loss : 0.070669 ; Test Acc : 0.438 ; LR : 0.027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 278 ; Train Loss : 0.065233 ; Train Acc : 0.384 ; Test Loss : 0.070615 ; Test Acc : 0.375 ; LR : 0.027\n",
      "Epoch : 279 ; Train Loss : 0.065065 ; Train Acc : 0.379 ; Test Loss : 0.070826 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 280 ; Train Loss : 0.064942 ; Train Acc : 0.387 ; Test Loss : 0.070604 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 281 ; Train Loss : 0.064639 ; Train Acc : 0.386 ; Test Loss : 0.070620 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 282 ; Train Loss : 0.064816 ; Train Acc : 0.387 ; Test Loss : 0.070726 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 283 ; Train Loss : 0.064828 ; Train Acc : 0.382 ; Test Loss : 0.070945 ; Test Acc : 0.375 ; LR : 0.027\n",
      "Epoch : 284 ; Train Loss : 0.065143 ; Train Acc : 0.382 ; Test Loss : 0.071257 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 285 ; Train Loss : 0.064927 ; Train Acc : 0.381 ; Test Loss : 0.071019 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 286 ; Train Loss : 0.065165 ; Train Acc : 0.386 ; Test Loss : 0.070624 ; Test Acc : 0.375 ; LR : 0.027\n",
      "Epoch : 287 ; Train Loss : 0.064880 ; Train Acc : 0.388 ; Test Loss : 0.070898 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 288 ; Train Loss : 0.064859 ; Train Acc : 0.386 ; Test Loss : 0.070585 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 289 ; Train Loss : 0.064795 ; Train Acc : 0.381 ; Test Loss : 0.070935 ; Test Acc : 0.375 ; LR : 0.027\n",
      "Epoch : 290 ; Train Loss : 0.064775 ; Train Acc : 0.382 ; Test Loss : 0.070607 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 291 ; Train Loss : 0.065026 ; Train Acc : 0.383 ; Test Loss : 0.070698 ; Test Acc : 0.375 ; LR : 0.027\n",
      "Epoch : 292 ; Train Loss : 0.064662 ; Train Acc : 0.388 ; Test Loss : 0.070771 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 293 ; Train Loss : 0.064836 ; Train Acc : 0.386 ; Test Loss : 0.071036 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 294 ; Train Loss : 0.065069 ; Train Acc : 0.386 ; Test Loss : 0.070984 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 295 ; Train Loss : 0.065179 ; Train Acc : 0.383 ; Test Loss : 0.070759 ; Test Acc : 0.375 ; LR : 0.027\n",
      "Epoch : 296 ; Train Loss : 0.064633 ; Train Acc : 0.386 ; Test Loss : 0.070650 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 297 ; Train Loss : 0.064989 ; Train Acc : 0.387 ; Test Loss : 0.071074 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 298 ; Train Loss : 0.064932 ; Train Acc : 0.379 ; Test Loss : 0.070734 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 299 ; Train Loss : 0.064592 ; Train Acc : 0.386 ; Test Loss : 0.070685 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 300 ; Train Loss : 0.064551 ; Train Acc : 0.387 ; Test Loss : 0.070784 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 301 ; Train Loss : 0.065045 ; Train Acc : 0.377 ; Test Loss : 0.070858 ; Test Acc : 0.375 ; LR : 0.024\n",
      "Epoch : 302 ; Train Loss : 0.064953 ; Train Acc : 0.388 ; Test Loss : 0.070704 ; Test Acc : 0.375 ; LR : 0.024\n",
      "Epoch : 303 ; Train Loss : 0.064922 ; Train Acc : 0.378 ; Test Loss : 0.070820 ; Test Acc : 0.375 ; LR : 0.024\n",
      "Epoch : 304 ; Train Loss : 0.064741 ; Train Acc : 0.386 ; Test Loss : 0.071242 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 305 ; Train Loss : 0.064568 ; Train Acc : 0.385 ; Test Loss : 0.070747 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 306 ; Train Loss : 0.064680 ; Train Acc : 0.388 ; Test Loss : 0.070919 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 307 ; Train Loss : 0.064567 ; Train Acc : 0.386 ; Test Loss : 0.070733 ; Test Acc : 0.375 ; LR : 0.024\n",
      "Epoch : 308 ; Train Loss : 0.064664 ; Train Acc : 0.385 ; Test Loss : 0.070700 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 309 ; Train Loss : 0.064457 ; Train Acc : 0.387 ; Test Loss : 0.070866 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 310 ; Train Loss : 0.064669 ; Train Acc : 0.387 ; Test Loss : 0.070827 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 311 ; Train Loss : 0.064606 ; Train Acc : 0.387 ; Test Loss : 0.070820 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 312 ; Train Loss : 0.064757 ; Train Acc : 0.377 ; Test Loss : 0.070912 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 313 ; Train Loss : 0.064673 ; Train Acc : 0.385 ; Test Loss : 0.071040 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 314 ; Train Loss : 0.064789 ; Train Acc : 0.381 ; Test Loss : 0.070873 ; Test Acc : 0.375 ; LR : 0.024\n",
      "Epoch : 315 ; Train Loss : 0.064555 ; Train Acc : 0.384 ; Test Loss : 0.070796 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 316 ; Train Loss : 0.064719 ; Train Acc : 0.379 ; Test Loss : 0.070905 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 317 ; Train Loss : 0.064720 ; Train Acc : 0.383 ; Test Loss : 0.070859 ; Test Acc : 0.375 ; LR : 0.024\n",
      "Epoch : 318 ; Train Loss : 0.064840 ; Train Acc : 0.382 ; Test Loss : 0.070938 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 319 ; Train Loss : 0.064914 ; Train Acc : 0.386 ; Test Loss : 0.071300 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 320 ; Train Loss : 0.064533 ; Train Acc : 0.388 ; Test Loss : 0.071037 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 321 ; Train Loss : 0.064852 ; Train Acc : 0.386 ; Test Loss : 0.071498 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 322 ; Train Loss : 0.064775 ; Train Acc : 0.387 ; Test Loss : 0.070956 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 323 ; Train Loss : 0.064854 ; Train Acc : 0.385 ; Test Loss : 0.071045 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 324 ; Train Loss : 0.064856 ; Train Acc : 0.387 ; Test Loss : 0.071008 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 325 ; Train Loss : 0.064490 ; Train Acc : 0.386 ; Test Loss : 0.071114 ; Test Acc : 0.375 ; LR : 0.024\n",
      "Epoch : 326 ; Train Loss : 0.064728 ; Train Acc : 0.385 ; Test Loss : 0.071202 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 327 ; Train Loss : 0.064759 ; Train Acc : 0.387 ; Test Loss : 0.070837 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 328 ; Train Loss : 0.064765 ; Train Acc : 0.382 ; Test Loss : 0.071392 ; Test Acc : 0.375 ; LR : 0.024\n",
      "Epoch : 329 ; Train Loss : 0.064783 ; Train Acc : 0.384 ; Test Loss : 0.071273 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 330 ; Train Loss : 0.064726 ; Train Acc : 0.383 ; Test Loss : 0.071360 ; Test Acc : 0.375 ; LR : 0.024\n",
      "Epoch : 331 ; Train Loss : 0.064716 ; Train Acc : 0.383 ; Test Loss : 0.070818 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 332 ; Train Loss : 0.064522 ; Train Acc : 0.385 ; Test Loss : 0.070854 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 333 ; Train Loss : 0.064661 ; Train Acc : 0.387 ; Test Loss : 0.071264 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 334 ; Train Loss : 0.064631 ; Train Acc : 0.384 ; Test Loss : 0.070861 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 335 ; Train Loss : 0.064758 ; Train Acc : 0.388 ; Test Loss : 0.071006 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 336 ; Train Loss : 0.064635 ; Train Acc : 0.387 ; Test Loss : 0.071061 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 337 ; Train Loss : 0.064581 ; Train Acc : 0.381 ; Test Loss : 0.070946 ; Test Acc : 0.375 ; LR : 0.024\n",
      "Epoch : 338 ; Train Loss : 0.064709 ; Train Acc : 0.385 ; Test Loss : 0.070928 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 339 ; Train Loss : 0.064683 ; Train Acc : 0.387 ; Test Loss : 0.070935 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 340 ; Train Loss : 0.064645 ; Train Acc : 0.383 ; Test Loss : 0.071088 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 341 ; Train Loss : 0.064597 ; Train Acc : 0.385 ; Test Loss : 0.071060 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 342 ; Train Loss : 0.064545 ; Train Acc : 0.386 ; Test Loss : 0.070934 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 343 ; Train Loss : 0.064674 ; Train Acc : 0.385 ; Test Loss : 0.071475 ; Test Acc : 0.375 ; LR : 0.024\n",
      "Epoch : 344 ; Train Loss : 0.064944 ; Train Acc : 0.386 ; Test Loss : 0.071056 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 345 ; Train Loss : 0.064384 ; Train Acc : 0.387 ; Test Loss : 0.070998 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 346 ; Train Loss : 0.064679 ; Train Acc : 0.388 ; Test Loss : 0.071482 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 347 ; Train Loss : 0.064605 ; Train Acc : 0.388 ; Test Loss : 0.071058 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 348 ; Train Loss : 0.064627 ; Train Acc : 0.385 ; Test Loss : 0.070968 ; Test Acc : 0.375 ; LR : 0.024\n",
      "Epoch : 349 ; Train Loss : 0.064591 ; Train Acc : 0.386 ; Test Loss : 0.071257 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 350 ; Train Loss : 0.064782 ; Train Acc : 0.387 ; Test Loss : 0.071200 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 351 ; Train Loss : 0.064776 ; Train Acc : 0.381 ; Test Loss : 0.070973 ; Test Acc : 0.438 ; LR : 0.022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 352 ; Train Loss : 0.064946 ; Train Acc : 0.385 ; Test Loss : 0.071645 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 353 ; Train Loss : 0.064587 ; Train Acc : 0.385 ; Test Loss : 0.071016 ; Test Acc : 0.375 ; LR : 0.022\n",
      "Epoch : 354 ; Train Loss : 0.064394 ; Train Acc : 0.378 ; Test Loss : 0.071349 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 355 ; Train Loss : 0.064708 ; Train Acc : 0.385 ; Test Loss : 0.071020 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 356 ; Train Loss : 0.064462 ; Train Acc : 0.384 ; Test Loss : 0.070926 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 357 ; Train Loss : 0.064281 ; Train Acc : 0.380 ; Test Loss : 0.071005 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 358 ; Train Loss : 0.064569 ; Train Acc : 0.384 ; Test Loss : 0.071121 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 359 ; Train Loss : 0.064763 ; Train Acc : 0.376 ; Test Loss : 0.070988 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 360 ; Train Loss : 0.064273 ; Train Acc : 0.388 ; Test Loss : 0.070956 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 361 ; Train Loss : 0.064570 ; Train Acc : 0.386 ; Test Loss : 0.071194 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 362 ; Train Loss : 0.064430 ; Train Acc : 0.386 ; Test Loss : 0.071005 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 363 ; Train Loss : 0.064592 ; Train Acc : 0.384 ; Test Loss : 0.071389 ; Test Acc : 0.375 ; LR : 0.022\n",
      "Epoch : 364 ; Train Loss : 0.064753 ; Train Acc : 0.386 ; Test Loss : 0.071074 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 365 ; Train Loss : 0.064317 ; Train Acc : 0.387 ; Test Loss : 0.071199 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 366 ; Train Loss : 0.064417 ; Train Acc : 0.386 ; Test Loss : 0.071008 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 367 ; Train Loss : 0.064535 ; Train Acc : 0.379 ; Test Loss : 0.071077 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 368 ; Train Loss : 0.064450 ; Train Acc : 0.378 ; Test Loss : 0.071057 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 369 ; Train Loss : 0.064559 ; Train Acc : 0.387 ; Test Loss : 0.071155 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 370 ; Train Loss : 0.064799 ; Train Acc : 0.387 ; Test Loss : 0.071096 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 371 ; Train Loss : 0.064225 ; Train Acc : 0.389 ; Test Loss : 0.071175 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 372 ; Train Loss : 0.064433 ; Train Acc : 0.386 ; Test Loss : 0.071077 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 373 ; Train Loss : 0.064329 ; Train Acc : 0.388 ; Test Loss : 0.071135 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 374 ; Train Loss : 0.064691 ; Train Acc : 0.386 ; Test Loss : 0.072048 ; Test Acc : 0.375 ; LR : 0.022\n",
      "Epoch : 375 ; Train Loss : 0.064309 ; Train Acc : 0.381 ; Test Loss : 0.071138 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 376 ; Train Loss : 0.064401 ; Train Acc : 0.381 ; Test Loss : 0.071465 ; Test Acc : 0.375 ; LR : 0.022\n",
      "Epoch : 377 ; Train Loss : 0.064467 ; Train Acc : 0.387 ; Test Loss : 0.071127 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 378 ; Train Loss : 0.064479 ; Train Acc : 0.387 ; Test Loss : 0.071240 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 379 ; Train Loss : 0.064388 ; Train Acc : 0.382 ; Test Loss : 0.071167 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 380 ; Train Loss : 0.064495 ; Train Acc : 0.388 ; Test Loss : 0.071255 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 381 ; Train Loss : 0.064453 ; Train Acc : 0.385 ; Test Loss : 0.071163 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 382 ; Train Loss : 0.064845 ; Train Acc : 0.386 ; Test Loss : 0.071411 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 383 ; Train Loss : 0.064779 ; Train Acc : 0.387 ; Test Loss : 0.071428 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 384 ; Train Loss : 0.064585 ; Train Acc : 0.384 ; Test Loss : 0.071177 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 385 ; Train Loss : 0.064482 ; Train Acc : 0.386 ; Test Loss : 0.071253 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 386 ; Train Loss : 0.064529 ; Train Acc : 0.387 ; Test Loss : 0.071275 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 387 ; Train Loss : 0.064231 ; Train Acc : 0.386 ; Test Loss : 0.071122 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 388 ; Train Loss : 0.064477 ; Train Acc : 0.388 ; Test Loss : 0.071348 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 389 ; Train Loss : 0.064518 ; Train Acc : 0.387 ; Test Loss : 0.071254 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 390 ; Train Loss : 0.064232 ; Train Acc : 0.387 ; Test Loss : 0.071584 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 391 ; Train Loss : 0.064535 ; Train Acc : 0.387 ; Test Loss : 0.071520 ; Test Acc : 0.375 ; LR : 0.022\n",
      "Epoch : 392 ; Train Loss : 0.064551 ; Train Acc : 0.380 ; Test Loss : 0.071228 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 393 ; Train Loss : 0.064554 ; Train Acc : 0.386 ; Test Loss : 0.071421 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 394 ; Train Loss : 0.064184 ; Train Acc : 0.386 ; Test Loss : 0.071232 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 395 ; Train Loss : 0.064205 ; Train Acc : 0.387 ; Test Loss : 0.071253 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 396 ; Train Loss : 0.064374 ; Train Acc : 0.380 ; Test Loss : 0.071332 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 397 ; Train Loss : 0.064442 ; Train Acc : 0.382 ; Test Loss : 0.071175 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 398 ; Train Loss : 0.064468 ; Train Acc : 0.388 ; Test Loss : 0.071342 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 399 ; Train Loss : 0.064426 ; Train Acc : 0.388 ; Test Loss : 0.071249 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 400 ; Train Loss : 0.064591 ; Train Acc : 0.388 ; Test Loss : 0.071271 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 401 ; Train Loss : 0.064479 ; Train Acc : 0.388 ; Test Loss : 0.071333 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 402 ; Train Loss : 0.064367 ; Train Acc : 0.387 ; Test Loss : 0.071511 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 403 ; Train Loss : 0.064442 ; Train Acc : 0.387 ; Test Loss : 0.071630 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 404 ; Train Loss : 0.064336 ; Train Acc : 0.388 ; Test Loss : 0.071337 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 405 ; Train Loss : 0.064272 ; Train Acc : 0.387 ; Test Loss : 0.071807 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 406 ; Train Loss : 0.064402 ; Train Acc : 0.380 ; Test Loss : 0.071276 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 407 ; Train Loss : 0.064347 ; Train Acc : 0.388 ; Test Loss : 0.072106 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 408 ; Train Loss : 0.064681 ; Train Acc : 0.384 ; Test Loss : 0.071248 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 409 ; Train Loss : 0.064654 ; Train Acc : 0.387 ; Test Loss : 0.071661 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 410 ; Train Loss : 0.064520 ; Train Acc : 0.387 ; Test Loss : 0.071587 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 411 ; Train Loss : 0.064390 ; Train Acc : 0.379 ; Test Loss : 0.071610 ; Test Acc : 0.375 ; LR : 0.019\n",
      "Epoch : 412 ; Train Loss : 0.064294 ; Train Acc : 0.386 ; Test Loss : 0.071343 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 413 ; Train Loss : 0.064307 ; Train Acc : 0.388 ; Test Loss : 0.071278 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 414 ; Train Loss : 0.064556 ; Train Acc : 0.388 ; Test Loss : 0.071470 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 415 ; Train Loss : 0.064206 ; Train Acc : 0.387 ; Test Loss : 0.071364 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 416 ; Train Loss : 0.064487 ; Train Acc : 0.381 ; Test Loss : 0.071320 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 417 ; Train Loss : 0.064500 ; Train Acc : 0.387 ; Test Loss : 0.071461 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 418 ; Train Loss : 0.064519 ; Train Acc : 0.388 ; Test Loss : 0.071262 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 419 ; Train Loss : 0.064192 ; Train Acc : 0.386 ; Test Loss : 0.071386 ; Test Acc : 0.375 ; LR : 0.019\n",
      "Epoch : 420 ; Train Loss : 0.064400 ; Train Acc : 0.383 ; Test Loss : 0.071561 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 421 ; Train Loss : 0.064365 ; Train Acc : 0.386 ; Test Loss : 0.071314 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 422 ; Train Loss : 0.064360 ; Train Acc : 0.383 ; Test Loss : 0.071441 ; Test Acc : 0.375 ; LR : 0.019\n",
      "Epoch : 423 ; Train Loss : 0.064336 ; Train Acc : 0.377 ; Test Loss : 0.071277 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 424 ; Train Loss : 0.064148 ; Train Acc : 0.387 ; Test Loss : 0.071758 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 425 ; Train Loss : 0.064564 ; Train Acc : 0.377 ; Test Loss : 0.071299 ; Test Acc : 0.438 ; LR : 0.019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 426 ; Train Loss : 0.064323 ; Train Acc : 0.387 ; Test Loss : 0.071462 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 427 ; Train Loss : 0.064132 ; Train Acc : 0.386 ; Test Loss : 0.071407 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 428 ; Train Loss : 0.064495 ; Train Acc : 0.386 ; Test Loss : 0.071372 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 429 ; Train Loss : 0.064237 ; Train Acc : 0.388 ; Test Loss : 0.071739 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 430 ; Train Loss : 0.064599 ; Train Acc : 0.381 ; Test Loss : 0.071371 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 431 ; Train Loss : 0.064052 ; Train Acc : 0.387 ; Test Loss : 0.071493 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 432 ; Train Loss : 0.064128 ; Train Acc : 0.388 ; Test Loss : 0.071711 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 433 ; Train Loss : 0.064526 ; Train Acc : 0.386 ; Test Loss : 0.071510 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 434 ; Train Loss : 0.064407 ; Train Acc : 0.383 ; Test Loss : 0.071579 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 435 ; Train Loss : 0.064320 ; Train Acc : 0.382 ; Test Loss : 0.071504 ; Test Acc : 0.375 ; LR : 0.019\n",
      "Epoch : 436 ; Train Loss : 0.064206 ; Train Acc : 0.389 ; Test Loss : 0.071483 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 437 ; Train Loss : 0.064360 ; Train Acc : 0.384 ; Test Loss : 0.071484 ; Test Acc : 0.375 ; LR : 0.019\n",
      "Epoch : 438 ; Train Loss : 0.064352 ; Train Acc : 0.382 ; Test Loss : 0.071425 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 439 ; Train Loss : 0.064331 ; Train Acc : 0.385 ; Test Loss : 0.071404 ; Test Acc : 0.375 ; LR : 0.019\n",
      "Epoch : 440 ; Train Loss : 0.064330 ; Train Acc : 0.384 ; Test Loss : 0.071435 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 441 ; Train Loss : 0.064432 ; Train Acc : 0.381 ; Test Loss : 0.071508 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 442 ; Train Loss : 0.064427 ; Train Acc : 0.388 ; Test Loss : 0.071612 ; Test Acc : 0.375 ; LR : 0.019\n",
      "Epoch : 443 ; Train Loss : 0.064231 ; Train Acc : 0.386 ; Test Loss : 0.071411 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 444 ; Train Loss : 0.064468 ; Train Acc : 0.379 ; Test Loss : 0.071814 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 445 ; Train Loss : 0.064363 ; Train Acc : 0.378 ; Test Loss : 0.071340 ; Test Acc : 0.375 ; LR : 0.019\n",
      "Epoch : 446 ; Train Loss : 0.064231 ; Train Acc : 0.389 ; Test Loss : 0.071507 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 447 ; Train Loss : 0.064159 ; Train Acc : 0.387 ; Test Loss : 0.071718 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 448 ; Train Loss : 0.064561 ; Train Acc : 0.380 ; Test Loss : 0.071975 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 449 ; Train Loss : 0.064355 ; Train Acc : 0.387 ; Test Loss : 0.071360 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 450 ; Train Loss : 0.064523 ; Train Acc : 0.387 ; Test Loss : 0.071505 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 451 ; Train Loss : 0.064215 ; Train Acc : 0.388 ; Test Loss : 0.071375 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 452 ; Train Loss : 0.064232 ; Train Acc : 0.386 ; Test Loss : 0.071407 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 453 ; Train Loss : 0.064272 ; Train Acc : 0.389 ; Test Loss : 0.071339 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 454 ; Train Loss : 0.064461 ; Train Acc : 0.386 ; Test Loss : 0.071541 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 455 ; Train Loss : 0.064740 ; Train Acc : 0.387 ; Test Loss : 0.071442 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 456 ; Train Loss : 0.064056 ; Train Acc : 0.388 ; Test Loss : 0.071433 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 457 ; Train Loss : 0.064382 ; Train Acc : 0.388 ; Test Loss : 0.071512 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 458 ; Train Loss : 0.064581 ; Train Acc : 0.387 ; Test Loss : 0.071527 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 459 ; Train Loss : 0.064388 ; Train Acc : 0.388 ; Test Loss : 0.071658 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 460 ; Train Loss : 0.064504 ; Train Acc : 0.387 ; Test Loss : 0.071357 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 461 ; Train Loss : 0.064486 ; Train Acc : 0.382 ; Test Loss : 0.072095 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 462 ; Train Loss : 0.064425 ; Train Acc : 0.387 ; Test Loss : 0.071389 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 463 ; Train Loss : 0.064328 ; Train Acc : 0.386 ; Test Loss : 0.071553 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 464 ; Train Loss : 0.064329 ; Train Acc : 0.387 ; Test Loss : 0.071512 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 465 ; Train Loss : 0.064314 ; Train Acc : 0.387 ; Test Loss : 0.071454 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 466 ; Train Loss : 0.063910 ; Train Acc : 0.387 ; Test Loss : 0.071493 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 467 ; Train Loss : 0.063762 ; Train Acc : 0.387 ; Test Loss : 0.071460 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 468 ; Train Loss : 0.064153 ; Train Acc : 0.387 ; Test Loss : 0.071635 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 469 ; Train Loss : 0.064525 ; Train Acc : 0.387 ; Test Loss : 0.071615 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 470 ; Train Loss : 0.064515 ; Train Acc : 0.388 ; Test Loss : 0.072048 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 471 ; Train Loss : 0.064788 ; Train Acc : 0.382 ; Test Loss : 0.072100 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 472 ; Train Loss : 0.063967 ; Train Acc : 0.385 ; Test Loss : 0.071522 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 473 ; Train Loss : 0.064203 ; Train Acc : 0.387 ; Test Loss : 0.071847 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 474 ; Train Loss : 0.064289 ; Train Acc : 0.387 ; Test Loss : 0.071549 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 475 ; Train Loss : 0.064113 ; Train Acc : 0.381 ; Test Loss : 0.071468 ; Test Acc : 0.375 ; LR : 0.017\n",
      "Epoch : 476 ; Train Loss : 0.064008 ; Train Acc : 0.386 ; Test Loss : 0.071742 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 477 ; Train Loss : 0.064147 ; Train Acc : 0.388 ; Test Loss : 0.071517 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 478 ; Train Loss : 0.064376 ; Train Acc : 0.387 ; Test Loss : 0.071643 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 479 ; Train Loss : 0.064219 ; Train Acc : 0.378 ; Test Loss : 0.071632 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 480 ; Train Loss : 0.064324 ; Train Acc : 0.388 ; Test Loss : 0.071542 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 481 ; Train Loss : 0.064235 ; Train Acc : 0.388 ; Test Loss : 0.071689 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 482 ; Train Loss : 0.064012 ; Train Acc : 0.388 ; Test Loss : 0.071598 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 483 ; Train Loss : 0.064320 ; Train Acc : 0.386 ; Test Loss : 0.071564 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 484 ; Train Loss : 0.064396 ; Train Acc : 0.388 ; Test Loss : 0.071575 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 485 ; Train Loss : 0.063939 ; Train Acc : 0.387 ; Test Loss : 0.071728 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 486 ; Train Loss : 0.064192 ; Train Acc : 0.388 ; Test Loss : 0.071843 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 487 ; Train Loss : 0.064407 ; Train Acc : 0.377 ; Test Loss : 0.071645 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 488 ; Train Loss : 0.064100 ; Train Acc : 0.387 ; Test Loss : 0.071640 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 489 ; Train Loss : 0.064029 ; Train Acc : 0.385 ; Test Loss : 0.071827 ; Test Acc : 0.375 ; LR : 0.017\n",
      "Epoch : 490 ; Train Loss : 0.064413 ; Train Acc : 0.387 ; Test Loss : 0.071624 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 491 ; Train Loss : 0.064213 ; Train Acc : 0.386 ; Test Loss : 0.071701 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 492 ; Train Loss : 0.064015 ; Train Acc : 0.382 ; Test Loss : 0.071904 ; Test Acc : 0.375 ; LR : 0.017\n",
      "Epoch : 493 ; Train Loss : 0.064154 ; Train Acc : 0.386 ; Test Loss : 0.071620 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 494 ; Train Loss : 0.064226 ; Train Acc : 0.387 ; Test Loss : 0.071694 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 495 ; Train Loss : 0.064122 ; Train Acc : 0.388 ; Test Loss : 0.071806 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 496 ; Train Loss : 0.064230 ; Train Acc : 0.380 ; Test Loss : 0.071633 ; Test Acc : 0.375 ; LR : 0.017\n",
      "Epoch : 497 ; Train Loss : 0.064224 ; Train Acc : 0.388 ; Test Loss : 0.071762 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 498 ; Train Loss : 0.064182 ; Train Acc : 0.388 ; Test Loss : 0.071772 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 499 ; Train Loss : 0.064148 ; Train Acc : 0.388 ; Test Loss : 0.071607 ; Test Acc : 0.438 ; LR : 0.017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 500 ; Train Loss : 0.064161 ; Train Acc : 0.387 ; Test Loss : 0.071692 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 501 ; Train Loss : 0.064119 ; Train Acc : 0.387 ; Test Loss : 0.072340 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 502 ; Train Loss : 0.064216 ; Train Acc : 0.388 ; Test Loss : 0.071855 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 503 ; Train Loss : 0.064118 ; Train Acc : 0.388 ; Test Loss : 0.071753 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 504 ; Train Loss : 0.064019 ; Train Acc : 0.389 ; Test Loss : 0.071860 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 505 ; Train Loss : 0.064254 ; Train Acc : 0.387 ; Test Loss : 0.071760 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 506 ; Train Loss : 0.063999 ; Train Acc : 0.387 ; Test Loss : 0.071707 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 507 ; Train Loss : 0.064333 ; Train Acc : 0.386 ; Test Loss : 0.071629 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 508 ; Train Loss : 0.064340 ; Train Acc : 0.388 ; Test Loss : 0.072035 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 509 ; Train Loss : 0.064162 ; Train Acc : 0.388 ; Test Loss : 0.071707 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 510 ; Train Loss : 0.063897 ; Train Acc : 0.378 ; Test Loss : 0.071655 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 511 ; Train Loss : 0.064119 ; Train Acc : 0.380 ; Test Loss : 0.071880 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 512 ; Train Loss : 0.064067 ; Train Acc : 0.387 ; Test Loss : 0.071789 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 513 ; Train Loss : 0.064047 ; Train Acc : 0.387 ; Test Loss : 0.071841 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 514 ; Train Loss : 0.064393 ; Train Acc : 0.381 ; Test Loss : 0.071963 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 515 ; Train Loss : 0.064045 ; Train Acc : 0.387 ; Test Loss : 0.071783 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 516 ; Train Loss : 0.063965 ; Train Acc : 0.387 ; Test Loss : 0.071795 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 517 ; Train Loss : 0.064082 ; Train Acc : 0.388 ; Test Loss : 0.071950 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 518 ; Train Loss : 0.064086 ; Train Acc : 0.387 ; Test Loss : 0.071777 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 519 ; Train Loss : 0.064087 ; Train Acc : 0.376 ; Test Loss : 0.071804 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 520 ; Train Loss : 0.063919 ; Train Acc : 0.383 ; Test Loss : 0.071847 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 521 ; Train Loss : 0.064297 ; Train Acc : 0.388 ; Test Loss : 0.071913 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 522 ; Train Loss : 0.064525 ; Train Acc : 0.384 ; Test Loss : 0.072083 ; Test Acc : 0.375 ; LR : 0.016\n",
      "Epoch : 523 ; Train Loss : 0.064302 ; Train Acc : 0.388 ; Test Loss : 0.071791 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 524 ; Train Loss : 0.064061 ; Train Acc : 0.388 ; Test Loss : 0.071942 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 525 ; Train Loss : 0.064021 ; Train Acc : 0.388 ; Test Loss : 0.071774 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 526 ; Train Loss : 0.064333 ; Train Acc : 0.388 ; Test Loss : 0.071748 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 527 ; Train Loss : 0.064024 ; Train Acc : 0.388 ; Test Loss : 0.071849 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 528 ; Train Loss : 0.063959 ; Train Acc : 0.383 ; Test Loss : 0.071729 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 529 ; Train Loss : 0.064243 ; Train Acc : 0.387 ; Test Loss : 0.071749 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 530 ; Train Loss : 0.064101 ; Train Acc : 0.386 ; Test Loss : 0.072020 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 531 ; Train Loss : 0.064202 ; Train Acc : 0.381 ; Test Loss : 0.071745 ; Test Acc : 0.375 ; LR : 0.016\n",
      "Epoch : 532 ; Train Loss : 0.064283 ; Train Acc : 0.385 ; Test Loss : 0.072215 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 533 ; Train Loss : 0.064432 ; Train Acc : 0.386 ; Test Loss : 0.072019 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 534 ; Train Loss : 0.064109 ; Train Acc : 0.388 ; Test Loss : 0.071967 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 535 ; Train Loss : 0.063971 ; Train Acc : 0.389 ; Test Loss : 0.071754 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 536 ; Train Loss : 0.064044 ; Train Acc : 0.388 ; Test Loss : 0.071819 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 537 ; Train Loss : 0.064037 ; Train Acc : 0.383 ; Test Loss : 0.071825 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 538 ; Train Loss : 0.064283 ; Train Acc : 0.379 ; Test Loss : 0.072021 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 539 ; Train Loss : 0.064249 ; Train Acc : 0.388 ; Test Loss : 0.071932 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 540 ; Train Loss : 0.064241 ; Train Acc : 0.385 ; Test Loss : 0.071915 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 541 ; Train Loss : 0.064137 ; Train Acc : 0.385 ; Test Loss : 0.072302 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 542 ; Train Loss : 0.063951 ; Train Acc : 0.386 ; Test Loss : 0.071922 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 543 ; Train Loss : 0.064215 ; Train Acc : 0.388 ; Test Loss : 0.071783 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 544 ; Train Loss : 0.064078 ; Train Acc : 0.383 ; Test Loss : 0.071809 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 545 ; Train Loss : 0.063854 ; Train Acc : 0.389 ; Test Loss : 0.072700 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 546 ; Train Loss : 0.064383 ; Train Acc : 0.382 ; Test Loss : 0.071851 ; Test Acc : 0.375 ; LR : 0.016\n",
      "Epoch : 547 ; Train Loss : 0.064352 ; Train Acc : 0.379 ; Test Loss : 0.071881 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 548 ; Train Loss : 0.064390 ; Train Acc : 0.383 ; Test Loss : 0.072080 ; Test Acc : 0.375 ; LR : 0.016\n",
      "Epoch : 549 ; Train Loss : 0.064014 ; Train Acc : 0.386 ; Test Loss : 0.072126 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 550 ; Train Loss : 0.064047 ; Train Acc : 0.388 ; Test Loss : 0.071772 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 551 ; Train Loss : 0.064159 ; Train Acc : 0.388 ; Test Loss : 0.071865 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 552 ; Train Loss : 0.064087 ; Train Acc : 0.389 ; Test Loss : 0.072402 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 553 ; Train Loss : 0.064151 ; Train Acc : 0.382 ; Test Loss : 0.071768 ; Test Acc : 0.375 ; LR : 0.014\n",
      "Epoch : 554 ; Train Loss : 0.063879 ; Train Acc : 0.382 ; Test Loss : 0.071975 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 555 ; Train Loss : 0.064023 ; Train Acc : 0.387 ; Test Loss : 0.072082 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 556 ; Train Loss : 0.064284 ; Train Acc : 0.387 ; Test Loss : 0.072022 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 557 ; Train Loss : 0.063859 ; Train Acc : 0.380 ; Test Loss : 0.071918 ; Test Acc : 0.375 ; LR : 0.014\n",
      "Epoch : 558 ; Train Loss : 0.063923 ; Train Acc : 0.385 ; Test Loss : 0.071868 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 559 ; Train Loss : 0.064002 ; Train Acc : 0.389 ; Test Loss : 0.071982 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 560 ; Train Loss : 0.064313 ; Train Acc : 0.387 ; Test Loss : 0.071859 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 561 ; Train Loss : 0.064109 ; Train Acc : 0.387 ; Test Loss : 0.072038 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 562 ; Train Loss : 0.064029 ; Train Acc : 0.387 ; Test Loss : 0.072180 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 563 ; Train Loss : 0.064232 ; Train Acc : 0.383 ; Test Loss : 0.072149 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 564 ; Train Loss : 0.063936 ; Train Acc : 0.389 ; Test Loss : 0.071961 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 565 ; Train Loss : 0.064038 ; Train Acc : 0.388 ; Test Loss : 0.071928 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 566 ; Train Loss : 0.064002 ; Train Acc : 0.378 ; Test Loss : 0.072216 ; Test Acc : 0.375 ; LR : 0.014\n",
      "Epoch : 567 ; Train Loss : 0.064499 ; Train Acc : 0.388 ; Test Loss : 0.071952 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 568 ; Train Loss : 0.063960 ; Train Acc : 0.388 ; Test Loss : 0.072168 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 569 ; Train Loss : 0.064206 ; Train Acc : 0.388 ; Test Loss : 0.072005 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 570 ; Train Loss : 0.064074 ; Train Acc : 0.382 ; Test Loss : 0.071918 ; Test Acc : 0.375 ; LR : 0.014\n",
      "Epoch : 571 ; Train Loss : 0.063910 ; Train Acc : 0.387 ; Test Loss : 0.071957 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 572 ; Train Loss : 0.064009 ; Train Acc : 0.389 ; Test Loss : 0.071913 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 573 ; Train Loss : 0.063805 ; Train Acc : 0.387 ; Test Loss : 0.072141 ; Test Acc : 0.438 ; LR : 0.014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 574 ; Train Loss : 0.064003 ; Train Acc : 0.388 ; Test Loss : 0.071902 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 575 ; Train Loss : 0.064195 ; Train Acc : 0.389 ; Test Loss : 0.072183 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 576 ; Train Loss : 0.064350 ; Train Acc : 0.388 ; Test Loss : 0.072222 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 577 ; Train Loss : 0.064097 ; Train Acc : 0.387 ; Test Loss : 0.072129 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 578 ; Train Loss : 0.063851 ; Train Acc : 0.384 ; Test Loss : 0.072176 ; Test Acc : 0.375 ; LR : 0.014\n",
      "Epoch : 579 ; Train Loss : 0.064157 ; Train Acc : 0.387 ; Test Loss : 0.071966 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 580 ; Train Loss : 0.064018 ; Train Acc : 0.388 ; Test Loss : 0.072029 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 581 ; Train Loss : 0.064013 ; Train Acc : 0.387 ; Test Loss : 0.071930 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 582 ; Train Loss : 0.064372 ; Train Acc : 0.388 ; Test Loss : 0.072038 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 583 ; Train Loss : 0.064094 ; Train Acc : 0.386 ; Test Loss : 0.071959 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 584 ; Train Loss : 0.064197 ; Train Acc : 0.387 ; Test Loss : 0.071953 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 585 ; Train Loss : 0.064168 ; Train Acc : 0.389 ; Test Loss : 0.072091 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 586 ; Train Loss : 0.063586 ; Train Acc : 0.387 ; Test Loss : 0.071912 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 587 ; Train Loss : 0.064109 ; Train Acc : 0.388 ; Test Loss : 0.071980 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 588 ; Train Loss : 0.063889 ; Train Acc : 0.387 ; Test Loss : 0.072071 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 589 ; Train Loss : 0.064038 ; Train Acc : 0.387 ; Test Loss : 0.072270 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 590 ; Train Loss : 0.063831 ; Train Acc : 0.382 ; Test Loss : 0.071953 ; Test Acc : 0.375 ; LR : 0.014\n",
      "Epoch : 591 ; Train Loss : 0.063893 ; Train Acc : 0.388 ; Test Loss : 0.072044 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 592 ; Train Loss : 0.063989 ; Train Acc : 0.388 ; Test Loss : 0.072130 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 593 ; Train Loss : 0.063846 ; Train Acc : 0.381 ; Test Loss : 0.072013 ; Test Acc : 0.375 ; LR : 0.014\n",
      "Epoch : 594 ; Train Loss : 0.064082 ; Train Acc : 0.387 ; Test Loss : 0.072148 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 595 ; Train Loss : 0.063974 ; Train Acc : 0.388 ; Test Loss : 0.072366 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 596 ; Train Loss : 0.063916 ; Train Acc : 0.384 ; Test Loss : 0.072134 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 597 ; Train Loss : 0.064135 ; Train Acc : 0.389 ; Test Loss : 0.072372 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 598 ; Train Loss : 0.063912 ; Train Acc : 0.388 ; Test Loss : 0.072135 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 599 ; Train Loss : 0.064333 ; Train Acc : 0.384 ; Test Loss : 0.072288 ; Test Acc : 0.375 ; LR : 0.014\n",
      "Epoch : 600 ; Train Loss : 0.064246 ; Train Acc : 0.386 ; Test Loss : 0.072368 ; Test Acc : 0.375 ; LR : 0.014\n",
      "\n",
      "Hidden Neurons : 20 ; Train Loss : 0.064058 ; Train Acc : 0.379 ; Test Loss : 0.072214 ; Test Acc : 0.438\n",
      "\n",
      "\n",
      "Epoch : 1 ; Train Loss : 0.156411 ; Train Acc : 0.115 ; Test Loss : 0.089972 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 2 ; Train Loss : 0.088857 ; Train Acc : 0.149 ; Test Loss : 0.086497 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 3 ; Train Loss : 0.085118 ; Train Acc : 0.206 ; Test Loss : 0.084249 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 4 ; Train Loss : 0.083489 ; Train Acc : 0.217 ; Test Loss : 0.083203 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 5 ; Train Loss : 0.082894 ; Train Acc : 0.222 ; Test Loss : 0.082979 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 6 ; Train Loss : 0.082485 ; Train Acc : 0.223 ; Test Loss : 0.082612 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 7 ; Train Loss : 0.082326 ; Train Acc : 0.223 ; Test Loss : 0.082449 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 8 ; Train Loss : 0.082165 ; Train Acc : 0.213 ; Test Loss : 0.082390 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 9 ; Train Loss : 0.081988 ; Train Acc : 0.219 ; Test Loss : 0.082293 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 10 ; Train Loss : 0.081858 ; Train Acc : 0.218 ; Test Loss : 0.082214 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 11 ; Train Loss : 0.081762 ; Train Acc : 0.218 ; Test Loss : 0.082056 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 12 ; Train Loss : 0.081752 ; Train Acc : 0.212 ; Test Loss : 0.082066 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 13 ; Train Loss : 0.081688 ; Train Acc : 0.217 ; Test Loss : 0.082517 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 14 ; Train Loss : 0.081708 ; Train Acc : 0.222 ; Test Loss : 0.081993 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 15 ; Train Loss : 0.081543 ; Train Acc : 0.214 ; Test Loss : 0.082224 ; Test Acc : 0.188 ; LR : 0.045\n",
      "Epoch : 16 ; Train Loss : 0.081754 ; Train Acc : 0.223 ; Test Loss : 0.081968 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 17 ; Train Loss : 0.081407 ; Train Acc : 0.218 ; Test Loss : 0.081911 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 18 ; Train Loss : 0.081530 ; Train Acc : 0.225 ; Test Loss : 0.081825 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 19 ; Train Loss : 0.081574 ; Train Acc : 0.215 ; Test Loss : 0.081922 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 20 ; Train Loss : 0.081597 ; Train Acc : 0.210 ; Test Loss : 0.081879 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 21 ; Train Loss : 0.081273 ; Train Acc : 0.218 ; Test Loss : 0.081858 ; Test Acc : 0.188 ; LR : 0.045\n",
      "Epoch : 22 ; Train Loss : 0.081346 ; Train Acc : 0.213 ; Test Loss : 0.081701 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 23 ; Train Loss : 0.081332 ; Train Acc : 0.221 ; Test Loss : 0.081749 ; Test Acc : 0.188 ; LR : 0.045\n",
      "Epoch : 24 ; Train Loss : 0.081334 ; Train Acc : 0.234 ; Test Loss : 0.081725 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 25 ; Train Loss : 0.081038 ; Train Acc : 0.260 ; Test Loss : 0.081176 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 26 ; Train Loss : 0.079838 ; Train Acc : 0.288 ; Test Loss : 0.079756 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 27 ; Train Loss : 0.077819 ; Train Acc : 0.330 ; Test Loss : 0.077442 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 28 ; Train Loss : 0.075683 ; Train Acc : 0.310 ; Test Loss : 0.076081 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 29 ; Train Loss : 0.074974 ; Train Acc : 0.303 ; Test Loss : 0.075200 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 30 ; Train Loss : 0.074327 ; Train Acc : 0.292 ; Test Loss : 0.074663 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 31 ; Train Loss : 0.073805 ; Train Acc : 0.305 ; Test Loss : 0.074455 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 32 ; Train Loss : 0.073616 ; Train Acc : 0.305 ; Test Loss : 0.074329 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 33 ; Train Loss : 0.073701 ; Train Acc : 0.303 ; Test Loss : 0.074154 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 34 ; Train Loss : 0.073469 ; Train Acc : 0.308 ; Test Loss : 0.074150 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 35 ; Train Loss : 0.073218 ; Train Acc : 0.311 ; Test Loss : 0.073827 ; Test Acc : 0.188 ; LR : 0.045\n",
      "Epoch : 36 ; Train Loss : 0.073019 ; Train Acc : 0.319 ; Test Loss : 0.073597 ; Test Acc : 0.188 ; LR : 0.045\n",
      "Epoch : 37 ; Train Loss : 0.072613 ; Train Acc : 0.330 ; Test Loss : 0.073134 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 38 ; Train Loss : 0.072347 ; Train Acc : 0.345 ; Test Loss : 0.072782 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 39 ; Train Loss : 0.071610 ; Train Acc : 0.367 ; Test Loss : 0.072158 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 40 ; Train Loss : 0.071113 ; Train Acc : 0.385 ; Test Loss : 0.071481 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 41 ; Train Loss : 0.070725 ; Train Acc : 0.390 ; Test Loss : 0.071231 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 42 ; Train Loss : 0.070074 ; Train Acc : 0.415 ; Test Loss : 0.070804 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 43 ; Train Loss : 0.069902 ; Train Acc : 0.405 ; Test Loss : 0.070728 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 44 ; Train Loss : 0.069647 ; Train Acc : 0.409 ; Test Loss : 0.071267 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 45 ; Train Loss : 0.069793 ; Train Acc : 0.405 ; Test Loss : 0.070678 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 46 ; Train Loss : 0.069569 ; Train Acc : 0.416 ; Test Loss : 0.070117 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 47 ; Train Loss : 0.069069 ; Train Acc : 0.417 ; Test Loss : 0.070324 ; Test Acc : 0.312 ; LR : 0.045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 48 ; Train Loss : 0.069087 ; Train Acc : 0.420 ; Test Loss : 0.070626 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 49 ; Train Loss : 0.069089 ; Train Acc : 0.422 ; Test Loss : 0.069886 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 50 ; Train Loss : 0.068806 ; Train Acc : 0.423 ; Test Loss : 0.069711 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 51 ; Train Loss : 0.068439 ; Train Acc : 0.428 ; Test Loss : 0.069428 ; Test Acc : 0.312 ; LR : 0.041\n",
      "Epoch : 52 ; Train Loss : 0.068366 ; Train Acc : 0.440 ; Test Loss : 0.069366 ; Test Acc : 0.312 ; LR : 0.041\n",
      "Epoch : 53 ; Train Loss : 0.068416 ; Train Acc : 0.439 ; Test Loss : 0.069163 ; Test Acc : 0.312 ; LR : 0.041\n",
      "Epoch : 54 ; Train Loss : 0.068134 ; Train Acc : 0.436 ; Test Loss : 0.068941 ; Test Acc : 0.312 ; LR : 0.041\n",
      "Epoch : 55 ; Train Loss : 0.067772 ; Train Acc : 0.444 ; Test Loss : 0.069452 ; Test Acc : 0.312 ; LR : 0.041\n",
      "Epoch : 56 ; Train Loss : 0.068003 ; Train Acc : 0.448 ; Test Loss : 0.068835 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 57 ; Train Loss : 0.067517 ; Train Acc : 0.438 ; Test Loss : 0.068628 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 58 ; Train Loss : 0.067519 ; Train Acc : 0.434 ; Test Loss : 0.068384 ; Test Acc : 0.312 ; LR : 0.041\n",
      "Epoch : 59 ; Train Loss : 0.067332 ; Train Acc : 0.430 ; Test Loss : 0.068558 ; Test Acc : 0.438 ; LR : 0.041\n",
      "Epoch : 60 ; Train Loss : 0.067358 ; Train Acc : 0.440 ; Test Loss : 0.068758 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 61 ; Train Loss : 0.067334 ; Train Acc : 0.422 ; Test Loss : 0.068451 ; Test Acc : 0.312 ; LR : 0.041\n",
      "Epoch : 62 ; Train Loss : 0.067254 ; Train Acc : 0.430 ; Test Loss : 0.068239 ; Test Acc : 0.250 ; LR : 0.041\n",
      "Epoch : 63 ; Train Loss : 0.067111 ; Train Acc : 0.427 ; Test Loss : 0.068131 ; Test Acc : 0.312 ; LR : 0.041\n",
      "Epoch : 64 ; Train Loss : 0.066958 ; Train Acc : 0.435 ; Test Loss : 0.067938 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 65 ; Train Loss : 0.066828 ; Train Acc : 0.439 ; Test Loss : 0.068023 ; Test Acc : 0.438 ; LR : 0.041\n",
      "Epoch : 66 ; Train Loss : 0.066722 ; Train Acc : 0.427 ; Test Loss : 0.067989 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 67 ; Train Loss : 0.066827 ; Train Acc : 0.420 ; Test Loss : 0.068237 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 68 ; Train Loss : 0.066682 ; Train Acc : 0.433 ; Test Loss : 0.068241 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 69 ; Train Loss : 0.066807 ; Train Acc : 0.417 ; Test Loss : 0.068226 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 70 ; Train Loss : 0.066812 ; Train Acc : 0.423 ; Test Loss : 0.068197 ; Test Acc : 0.312 ; LR : 0.041\n",
      "Epoch : 71 ; Train Loss : 0.066761 ; Train Acc : 0.421 ; Test Loss : 0.067972 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 72 ; Train Loss : 0.066906 ; Train Acc : 0.408 ; Test Loss : 0.068422 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 73 ; Train Loss : 0.066618 ; Train Acc : 0.411 ; Test Loss : 0.067749 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 74 ; Train Loss : 0.066160 ; Train Acc : 0.428 ; Test Loss : 0.068265 ; Test Acc : 0.438 ; LR : 0.041\n",
      "Epoch : 75 ; Train Loss : 0.066463 ; Train Acc : 0.415 ; Test Loss : 0.067889 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 76 ; Train Loss : 0.066499 ; Train Acc : 0.402 ; Test Loss : 0.067835 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 77 ; Train Loss : 0.066425 ; Train Acc : 0.411 ; Test Loss : 0.068253 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 78 ; Train Loss : 0.066507 ; Train Acc : 0.417 ; Test Loss : 0.067561 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 79 ; Train Loss : 0.066192 ; Train Acc : 0.411 ; Test Loss : 0.067461 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 80 ; Train Loss : 0.066115 ; Train Acc : 0.409 ; Test Loss : 0.067653 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 81 ; Train Loss : 0.066208 ; Train Acc : 0.419 ; Test Loss : 0.067955 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 82 ; Train Loss : 0.066134 ; Train Acc : 0.413 ; Test Loss : 0.067428 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 83 ; Train Loss : 0.066134 ; Train Acc : 0.410 ; Test Loss : 0.067406 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 84 ; Train Loss : 0.066300 ; Train Acc : 0.416 ; Test Loss : 0.067412 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 85 ; Train Loss : 0.066408 ; Train Acc : 0.407 ; Test Loss : 0.067492 ; Test Acc : 0.438 ; LR : 0.041\n",
      "Epoch : 86 ; Train Loss : 0.065930 ; Train Acc : 0.411 ; Test Loss : 0.067205 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 87 ; Train Loss : 0.065562 ; Train Acc : 0.411 ; Test Loss : 0.067318 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 88 ; Train Loss : 0.066024 ; Train Acc : 0.404 ; Test Loss : 0.067127 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 89 ; Train Loss : 0.065870 ; Train Acc : 0.405 ; Test Loss : 0.067449 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 90 ; Train Loss : 0.066061 ; Train Acc : 0.413 ; Test Loss : 0.067398 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 91 ; Train Loss : 0.065956 ; Train Acc : 0.404 ; Test Loss : 0.067149 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 92 ; Train Loss : 0.065807 ; Train Acc : 0.404 ; Test Loss : 0.067231 ; Test Acc : 0.312 ; LR : 0.041\n",
      "Epoch : 93 ; Train Loss : 0.065518 ; Train Acc : 0.409 ; Test Loss : 0.067070 ; Test Acc : 0.438 ; LR : 0.041\n",
      "Epoch : 94 ; Train Loss : 0.065699 ; Train Acc : 0.407 ; Test Loss : 0.067206 ; Test Acc : 0.438 ; LR : 0.041\n",
      "Epoch : 95 ; Train Loss : 0.065559 ; Train Acc : 0.404 ; Test Loss : 0.066977 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 96 ; Train Loss : 0.065780 ; Train Acc : 0.402 ; Test Loss : 0.067196 ; Test Acc : 0.312 ; LR : 0.041\n",
      "Epoch : 97 ; Train Loss : 0.065831 ; Train Acc : 0.398 ; Test Loss : 0.067264 ; Test Acc : 0.312 ; LR : 0.041\n",
      "Epoch : 98 ; Train Loss : 0.065681 ; Train Acc : 0.414 ; Test Loss : 0.066904 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 99 ; Train Loss : 0.065149 ; Train Acc : 0.402 ; Test Loss : 0.066953 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 100 ; Train Loss : 0.065621 ; Train Acc : 0.396 ; Test Loss : 0.066929 ; Test Acc : 0.375 ; LR : 0.041\n",
      "Epoch : 101 ; Train Loss : 0.065390 ; Train Acc : 0.393 ; Test Loss : 0.066754 ; Test Acc : 0.375 ; LR : 0.036\n",
      "Epoch : 102 ; Train Loss : 0.065350 ; Train Acc : 0.395 ; Test Loss : 0.066700 ; Test Acc : 0.312 ; LR : 0.036\n",
      "Epoch : 103 ; Train Loss : 0.065058 ; Train Acc : 0.400 ; Test Loss : 0.066699 ; Test Acc : 0.375 ; LR : 0.036\n",
      "Epoch : 104 ; Train Loss : 0.065008 ; Train Acc : 0.404 ; Test Loss : 0.066656 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 105 ; Train Loss : 0.065077 ; Train Acc : 0.404 ; Test Loss : 0.066613 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 106 ; Train Loss : 0.064733 ; Train Acc : 0.406 ; Test Loss : 0.066431 ; Test Acc : 0.375 ; LR : 0.036\n",
      "Epoch : 107 ; Train Loss : 0.065091 ; Train Acc : 0.410 ; Test Loss : 0.066512 ; Test Acc : 0.375 ; LR : 0.036\n",
      "Epoch : 108 ; Train Loss : 0.064988 ; Train Acc : 0.398 ; Test Loss : 0.066545 ; Test Acc : 0.375 ; LR : 0.036\n",
      "Epoch : 109 ; Train Loss : 0.064887 ; Train Acc : 0.399 ; Test Loss : 0.066357 ; Test Acc : 0.375 ; LR : 0.036\n",
      "Epoch : 110 ; Train Loss : 0.064671 ; Train Acc : 0.399 ; Test Loss : 0.066477 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 111 ; Train Loss : 0.064257 ; Train Acc : 0.413 ; Test Loss : 0.066121 ; Test Acc : 0.312 ; LR : 0.036\n",
      "Epoch : 112 ; Train Loss : 0.063695 ; Train Acc : 0.440 ; Test Loss : 0.064777 ; Test Acc : 0.375 ; LR : 0.036\n",
      "Epoch : 113 ; Train Loss : 0.061771 ; Train Acc : 0.464 ; Test Loss : 0.062342 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 114 ; Train Loss : 0.059232 ; Train Acc : 0.484 ; Test Loss : 0.061104 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 115 ; Train Loss : 0.058475 ; Train Acc : 0.492 ; Test Loss : 0.060601 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 116 ; Train Loss : 0.057872 ; Train Acc : 0.493 ; Test Loss : 0.060104 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 117 ; Train Loss : 0.057817 ; Train Acc : 0.493 ; Test Loss : 0.059946 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 118 ; Train Loss : 0.057550 ; Train Acc : 0.492 ; Test Loss : 0.059521 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 119 ; Train Loss : 0.057235 ; Train Acc : 0.496 ; Test Loss : 0.059741 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 120 ; Train Loss : 0.057362 ; Train Acc : 0.495 ; Test Loss : 0.059511 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 121 ; Train Loss : 0.057149 ; Train Acc : 0.495 ; Test Loss : 0.059345 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 122 ; Train Loss : 0.056743 ; Train Acc : 0.495 ; Test Loss : 0.059252 ; Test Acc : 0.500 ; LR : 0.036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 123 ; Train Loss : 0.056936 ; Train Acc : 0.496 ; Test Loss : 0.059486 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 124 ; Train Loss : 0.057241 ; Train Acc : 0.494 ; Test Loss : 0.059259 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 125 ; Train Loss : 0.057106 ; Train Acc : 0.497 ; Test Loss : 0.059118 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 126 ; Train Loss : 0.056488 ; Train Acc : 0.498 ; Test Loss : 0.059170 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 127 ; Train Loss : 0.056686 ; Train Acc : 0.498 ; Test Loss : 0.059261 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 128 ; Train Loss : 0.056596 ; Train Acc : 0.497 ; Test Loss : 0.059219 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 129 ; Train Loss : 0.056706 ; Train Acc : 0.499 ; Test Loss : 0.059160 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 130 ; Train Loss : 0.056388 ; Train Acc : 0.498 ; Test Loss : 0.059112 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 131 ; Train Loss : 0.056325 ; Train Acc : 0.499 ; Test Loss : 0.059268 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 132 ; Train Loss : 0.056680 ; Train Acc : 0.499 ; Test Loss : 0.059093 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 133 ; Train Loss : 0.056203 ; Train Acc : 0.498 ; Test Loss : 0.059101 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 134 ; Train Loss : 0.056354 ; Train Acc : 0.498 ; Test Loss : 0.058968 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 135 ; Train Loss : 0.056404 ; Train Acc : 0.499 ; Test Loss : 0.059105 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 136 ; Train Loss : 0.056390 ; Train Acc : 0.499 ; Test Loss : 0.058974 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 137 ; Train Loss : 0.056246 ; Train Acc : 0.499 ; Test Loss : 0.058951 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 138 ; Train Loss : 0.056140 ; Train Acc : 0.498 ; Test Loss : 0.058869 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 139 ; Train Loss : 0.056229 ; Train Acc : 0.500 ; Test Loss : 0.058772 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 140 ; Train Loss : 0.055873 ; Train Acc : 0.499 ; Test Loss : 0.058946 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 141 ; Train Loss : 0.056292 ; Train Acc : 0.500 ; Test Loss : 0.058931 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 142 ; Train Loss : 0.056191 ; Train Acc : 0.500 ; Test Loss : 0.058856 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 143 ; Train Loss : 0.056132 ; Train Acc : 0.500 ; Test Loss : 0.058821 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 144 ; Train Loss : 0.055941 ; Train Acc : 0.500 ; Test Loss : 0.059008 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 145 ; Train Loss : 0.056059 ; Train Acc : 0.499 ; Test Loss : 0.058821 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 146 ; Train Loss : 0.055926 ; Train Acc : 0.500 ; Test Loss : 0.058820 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 147 ; Train Loss : 0.056180 ; Train Acc : 0.502 ; Test Loss : 0.058824 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 148 ; Train Loss : 0.055963 ; Train Acc : 0.499 ; Test Loss : 0.058754 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 149 ; Train Loss : 0.055963 ; Train Acc : 0.500 ; Test Loss : 0.058768 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 150 ; Train Loss : 0.055994 ; Train Acc : 0.500 ; Test Loss : 0.058788 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 151 ; Train Loss : 0.055894 ; Train Acc : 0.500 ; Test Loss : 0.058797 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 152 ; Train Loss : 0.055831 ; Train Acc : 0.499 ; Test Loss : 0.058948 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 153 ; Train Loss : 0.056017 ; Train Acc : 0.500 ; Test Loss : 0.058871 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 154 ; Train Loss : 0.055968 ; Train Acc : 0.500 ; Test Loss : 0.059031 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 155 ; Train Loss : 0.056170 ; Train Acc : 0.500 ; Test Loss : 0.058802 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 156 ; Train Loss : 0.055911 ; Train Acc : 0.501 ; Test Loss : 0.058985 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 157 ; Train Loss : 0.055747 ; Train Acc : 0.500 ; Test Loss : 0.058807 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 158 ; Train Loss : 0.055876 ; Train Acc : 0.500 ; Test Loss : 0.058777 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 159 ; Train Loss : 0.055756 ; Train Acc : 0.501 ; Test Loss : 0.058840 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 160 ; Train Loss : 0.055633 ; Train Acc : 0.501 ; Test Loss : 0.058774 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 161 ; Train Loss : 0.055689 ; Train Acc : 0.500 ; Test Loss : 0.059083 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 162 ; Train Loss : 0.055827 ; Train Acc : 0.499 ; Test Loss : 0.058812 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 163 ; Train Loss : 0.055394 ; Train Acc : 0.500 ; Test Loss : 0.058744 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 164 ; Train Loss : 0.055662 ; Train Acc : 0.501 ; Test Loss : 0.058739 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 165 ; Train Loss : 0.055735 ; Train Acc : 0.498 ; Test Loss : 0.058680 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 166 ; Train Loss : 0.055874 ; Train Acc : 0.499 ; Test Loss : 0.058793 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 167 ; Train Loss : 0.055740 ; Train Acc : 0.501 ; Test Loss : 0.058869 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 168 ; Train Loss : 0.055810 ; Train Acc : 0.500 ; Test Loss : 0.058886 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 169 ; Train Loss : 0.055506 ; Train Acc : 0.500 ; Test Loss : 0.059007 ; Test Acc : 0.375 ; LR : 0.033\n",
      "Epoch : 170 ; Train Loss : 0.055697 ; Train Acc : 0.501 ; Test Loss : 0.058745 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 171 ; Train Loss : 0.055481 ; Train Acc : 0.501 ; Test Loss : 0.058845 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 172 ; Train Loss : 0.055513 ; Train Acc : 0.502 ; Test Loss : 0.058838 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 173 ; Train Loss : 0.055468 ; Train Acc : 0.501 ; Test Loss : 0.058865 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 174 ; Train Loss : 0.055380 ; Train Acc : 0.503 ; Test Loss : 0.058763 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 175 ; Train Loss : 0.055076 ; Train Acc : 0.501 ; Test Loss : 0.058702 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 176 ; Train Loss : 0.054943 ; Train Acc : 0.503 ; Test Loss : 0.058784 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 177 ; Train Loss : 0.055251 ; Train Acc : 0.503 ; Test Loss : 0.059134 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 178 ; Train Loss : 0.055574 ; Train Acc : 0.503 ; Test Loss : 0.058762 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 179 ; Train Loss : 0.055429 ; Train Acc : 0.508 ; Test Loss : 0.058412 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 180 ; Train Loss : 0.054502 ; Train Acc : 0.536 ; Test Loss : 0.057082 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 181 ; Train Loss : 0.052394 ; Train Acc : 0.562 ; Test Loss : 0.055378 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 182 ; Train Loss : 0.050671 ; Train Acc : 0.576 ; Test Loss : 0.053712 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 183 ; Train Loss : 0.049584 ; Train Acc : 0.585 ; Test Loss : 0.052822 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 184 ; Train Loss : 0.048867 ; Train Acc : 0.592 ; Test Loss : 0.052457 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 185 ; Train Loss : 0.048233 ; Train Acc : 0.594 ; Test Loss : 0.052167 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 186 ; Train Loss : 0.047959 ; Train Acc : 0.596 ; Test Loss : 0.051944 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 187 ; Train Loss : 0.047838 ; Train Acc : 0.595 ; Test Loss : 0.051760 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 188 ; Train Loss : 0.047706 ; Train Acc : 0.595 ; Test Loss : 0.051746 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 189 ; Train Loss : 0.047644 ; Train Acc : 0.597 ; Test Loss : 0.051772 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 190 ; Train Loss : 0.047597 ; Train Acc : 0.597 ; Test Loss : 0.051522 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 191 ; Train Loss : 0.047525 ; Train Acc : 0.596 ; Test Loss : 0.051415 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 192 ; Train Loss : 0.047172 ; Train Acc : 0.598 ; Test Loss : 0.051510 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 193 ; Train Loss : 0.047239 ; Train Acc : 0.599 ; Test Loss : 0.051392 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 194 ; Train Loss : 0.047213 ; Train Acc : 0.598 ; Test Loss : 0.051529 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 195 ; Train Loss : 0.046961 ; Train Acc : 0.599 ; Test Loss : 0.051690 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 196 ; Train Loss : 0.046746 ; Train Acc : 0.599 ; Test Loss : 0.051667 ; Test Acc : 0.562 ; LR : 0.033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 197 ; Train Loss : 0.046776 ; Train Acc : 0.598 ; Test Loss : 0.051510 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 198 ; Train Loss : 0.046951 ; Train Acc : 0.599 ; Test Loss : 0.051315 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 199 ; Train Loss : 0.046681 ; Train Acc : 0.598 ; Test Loss : 0.051310 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 200 ; Train Loss : 0.046991 ; Train Acc : 0.600 ; Test Loss : 0.051396 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 201 ; Train Loss : 0.046810 ; Train Acc : 0.601 ; Test Loss : 0.051395 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 202 ; Train Loss : 0.046829 ; Train Acc : 0.599 ; Test Loss : 0.051314 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 203 ; Train Loss : 0.046817 ; Train Acc : 0.598 ; Test Loss : 0.051666 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 204 ; Train Loss : 0.046636 ; Train Acc : 0.598 ; Test Loss : 0.051295 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 205 ; Train Loss : 0.046728 ; Train Acc : 0.600 ; Test Loss : 0.051394 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 206 ; Train Loss : 0.047088 ; Train Acc : 0.598 ; Test Loss : 0.051269 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 207 ; Train Loss : 0.046616 ; Train Acc : 0.600 ; Test Loss : 0.051254 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 208 ; Train Loss : 0.046456 ; Train Acc : 0.600 ; Test Loss : 0.051320 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 209 ; Train Loss : 0.046502 ; Train Acc : 0.601 ; Test Loss : 0.051233 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 210 ; Train Loss : 0.046429 ; Train Acc : 0.601 ; Test Loss : 0.051254 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 211 ; Train Loss : 0.046564 ; Train Acc : 0.601 ; Test Loss : 0.051420 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 212 ; Train Loss : 0.046957 ; Train Acc : 0.601 ; Test Loss : 0.051591 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 213 ; Train Loss : 0.046682 ; Train Acc : 0.600 ; Test Loss : 0.051308 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 214 ; Train Loss : 0.046238 ; Train Acc : 0.600 ; Test Loss : 0.051364 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 215 ; Train Loss : 0.046441 ; Train Acc : 0.602 ; Test Loss : 0.051206 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 216 ; Train Loss : 0.046207 ; Train Acc : 0.600 ; Test Loss : 0.051237 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 217 ; Train Loss : 0.046514 ; Train Acc : 0.601 ; Test Loss : 0.051435 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 218 ; Train Loss : 0.046413 ; Train Acc : 0.600 ; Test Loss : 0.051373 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 219 ; Train Loss : 0.046209 ; Train Acc : 0.601 ; Test Loss : 0.051405 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 220 ; Train Loss : 0.046358 ; Train Acc : 0.600 ; Test Loss : 0.051297 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 221 ; Train Loss : 0.046092 ; Train Acc : 0.601 ; Test Loss : 0.051278 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 222 ; Train Loss : 0.046102 ; Train Acc : 0.600 ; Test Loss : 0.051252 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 223 ; Train Loss : 0.046217 ; Train Acc : 0.601 ; Test Loss : 0.051281 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 224 ; Train Loss : 0.046136 ; Train Acc : 0.601 ; Test Loss : 0.051259 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 225 ; Train Loss : 0.046558 ; Train Acc : 0.600 ; Test Loss : 0.051249 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 226 ; Train Loss : 0.046510 ; Train Acc : 0.601 ; Test Loss : 0.051300 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 227 ; Train Loss : 0.046190 ; Train Acc : 0.602 ; Test Loss : 0.051430 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 228 ; Train Loss : 0.046008 ; Train Acc : 0.601 ; Test Loss : 0.051372 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 229 ; Train Loss : 0.046160 ; Train Acc : 0.601 ; Test Loss : 0.051466 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 230 ; Train Loss : 0.046262 ; Train Acc : 0.600 ; Test Loss : 0.051330 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 231 ; Train Loss : 0.046185 ; Train Acc : 0.601 ; Test Loss : 0.051441 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 232 ; Train Loss : 0.046098 ; Train Acc : 0.602 ; Test Loss : 0.051372 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 233 ; Train Loss : 0.046033 ; Train Acc : 0.601 ; Test Loss : 0.051295 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 234 ; Train Loss : 0.046127 ; Train Acc : 0.601 ; Test Loss : 0.051472 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 235 ; Train Loss : 0.045783 ; Train Acc : 0.602 ; Test Loss : 0.051445 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 236 ; Train Loss : 0.045957 ; Train Acc : 0.602 ; Test Loss : 0.051300 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 237 ; Train Loss : 0.045933 ; Train Acc : 0.600 ; Test Loss : 0.051650 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 238 ; Train Loss : 0.045941 ; Train Acc : 0.602 ; Test Loss : 0.051321 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 239 ; Train Loss : 0.045897 ; Train Acc : 0.602 ; Test Loss : 0.051407 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 240 ; Train Loss : 0.045697 ; Train Acc : 0.602 ; Test Loss : 0.051345 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 241 ; Train Loss : 0.045897 ; Train Acc : 0.602 ; Test Loss : 0.051499 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 242 ; Train Loss : 0.045998 ; Train Acc : 0.602 ; Test Loss : 0.051431 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 243 ; Train Loss : 0.045939 ; Train Acc : 0.602 ; Test Loss : 0.051406 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 244 ; Train Loss : 0.045842 ; Train Acc : 0.603 ; Test Loss : 0.051535 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 245 ; Train Loss : 0.045688 ; Train Acc : 0.602 ; Test Loss : 0.051503 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 246 ; Train Loss : 0.045944 ; Train Acc : 0.601 ; Test Loss : 0.051460 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 247 ; Train Loss : 0.045880 ; Train Acc : 0.600 ; Test Loss : 0.051493 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 248 ; Train Loss : 0.045951 ; Train Acc : 0.603 ; Test Loss : 0.051546 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 249 ; Train Loss : 0.045806 ; Train Acc : 0.602 ; Test Loss : 0.051506 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 250 ; Train Loss : 0.046058 ; Train Acc : 0.601 ; Test Loss : 0.051542 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 251 ; Train Loss : 0.045991 ; Train Acc : 0.601 ; Test Loss : 0.051388 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 252 ; Train Loss : 0.045727 ; Train Acc : 0.603 ; Test Loss : 0.051420 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 253 ; Train Loss : 0.045818 ; Train Acc : 0.602 ; Test Loss : 0.051426 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 254 ; Train Loss : 0.045707 ; Train Acc : 0.603 ; Test Loss : 0.051408 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 255 ; Train Loss : 0.045824 ; Train Acc : 0.602 ; Test Loss : 0.051568 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 256 ; Train Loss : 0.046034 ; Train Acc : 0.602 ; Test Loss : 0.051545 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 257 ; Train Loss : 0.045750 ; Train Acc : 0.602 ; Test Loss : 0.051552 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 258 ; Train Loss : 0.045913 ; Train Acc : 0.602 ; Test Loss : 0.051451 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 259 ; Train Loss : 0.045625 ; Train Acc : 0.602 ; Test Loss : 0.051570 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 260 ; Train Loss : 0.045425 ; Train Acc : 0.604 ; Test Loss : 0.051473 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 261 ; Train Loss : 0.045740 ; Train Acc : 0.604 ; Test Loss : 0.051398 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 262 ; Train Loss : 0.045695 ; Train Acc : 0.603 ; Test Loss : 0.051413 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 263 ; Train Loss : 0.045510 ; Train Acc : 0.602 ; Test Loss : 0.051454 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 264 ; Train Loss : 0.045722 ; Train Acc : 0.603 ; Test Loss : 0.051466 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 265 ; Train Loss : 0.045673 ; Train Acc : 0.602 ; Test Loss : 0.051710 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 266 ; Train Loss : 0.045630 ; Train Acc : 0.601 ; Test Loss : 0.051540 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 267 ; Train Loss : 0.045471 ; Train Acc : 0.601 ; Test Loss : 0.051886 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 268 ; Train Loss : 0.045495 ; Train Acc : 0.601 ; Test Loss : 0.051418 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 269 ; Train Loss : 0.045812 ; Train Acc : 0.603 ; Test Loss : 0.051595 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 270 ; Train Loss : 0.045659 ; Train Acc : 0.603 ; Test Loss : 0.051550 ; Test Acc : 0.562 ; LR : 0.027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 271 ; Train Loss : 0.045585 ; Train Acc : 0.603 ; Test Loss : 0.051379 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 272 ; Train Loss : 0.045676 ; Train Acc : 0.603 ; Test Loss : 0.051456 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 273 ; Train Loss : 0.045509 ; Train Acc : 0.603 ; Test Loss : 0.051467 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 274 ; Train Loss : 0.045523 ; Train Acc : 0.603 ; Test Loss : 0.051684 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 275 ; Train Loss : 0.045525 ; Train Acc : 0.602 ; Test Loss : 0.051440 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 276 ; Train Loss : 0.045519 ; Train Acc : 0.603 ; Test Loss : 0.051796 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 277 ; Train Loss : 0.045429 ; Train Acc : 0.605 ; Test Loss : 0.051582 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 278 ; Train Loss : 0.045843 ; Train Acc : 0.602 ; Test Loss : 0.051747 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 279 ; Train Loss : 0.045608 ; Train Acc : 0.602 ; Test Loss : 0.051488 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 280 ; Train Loss : 0.045602 ; Train Acc : 0.604 ; Test Loss : 0.051670 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 281 ; Train Loss : 0.045891 ; Train Acc : 0.603 ; Test Loss : 0.051773 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 282 ; Train Loss : 0.045619 ; Train Acc : 0.604 ; Test Loss : 0.051554 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 283 ; Train Loss : 0.045482 ; Train Acc : 0.602 ; Test Loss : 0.051641 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 284 ; Train Loss : 0.045383 ; Train Acc : 0.603 ; Test Loss : 0.051547 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 285 ; Train Loss : 0.045506 ; Train Acc : 0.604 ; Test Loss : 0.051548 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 286 ; Train Loss : 0.045447 ; Train Acc : 0.602 ; Test Loss : 0.051668 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 287 ; Train Loss : 0.045632 ; Train Acc : 0.601 ; Test Loss : 0.051872 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 288 ; Train Loss : 0.045397 ; Train Acc : 0.601 ; Test Loss : 0.051784 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 289 ; Train Loss : 0.045528 ; Train Acc : 0.604 ; Test Loss : 0.051646 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 290 ; Train Loss : 0.045321 ; Train Acc : 0.603 ; Test Loss : 0.051629 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 291 ; Train Loss : 0.045273 ; Train Acc : 0.603 ; Test Loss : 0.051712 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 292 ; Train Loss : 0.045415 ; Train Acc : 0.604 ; Test Loss : 0.051744 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 293 ; Train Loss : 0.045419 ; Train Acc : 0.603 ; Test Loss : 0.051807 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 294 ; Train Loss : 0.045539 ; Train Acc : 0.603 ; Test Loss : 0.051637 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 295 ; Train Loss : 0.045351 ; Train Acc : 0.604 ; Test Loss : 0.051732 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 296 ; Train Loss : 0.045301 ; Train Acc : 0.603 ; Test Loss : 0.051721 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 297 ; Train Loss : 0.045285 ; Train Acc : 0.603 ; Test Loss : 0.051863 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 298 ; Train Loss : 0.045480 ; Train Acc : 0.605 ; Test Loss : 0.051678 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 299 ; Train Loss : 0.045176 ; Train Acc : 0.604 ; Test Loss : 0.051805 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 300 ; Train Loss : 0.045423 ; Train Acc : 0.604 ; Test Loss : 0.051813 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 301 ; Train Loss : 0.045138 ; Train Acc : 0.604 ; Test Loss : 0.051691 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 302 ; Train Loss : 0.045212 ; Train Acc : 0.604 ; Test Loss : 0.051695 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 303 ; Train Loss : 0.045109 ; Train Acc : 0.603 ; Test Loss : 0.051606 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 304 ; Train Loss : 0.045218 ; Train Acc : 0.604 ; Test Loss : 0.051590 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 305 ; Train Loss : 0.045226 ; Train Acc : 0.604 ; Test Loss : 0.051684 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 306 ; Train Loss : 0.045325 ; Train Acc : 0.604 ; Test Loss : 0.051742 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 307 ; Train Loss : 0.045016 ; Train Acc : 0.603 ; Test Loss : 0.051847 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 308 ; Train Loss : 0.045529 ; Train Acc : 0.604 ; Test Loss : 0.051736 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 309 ; Train Loss : 0.045202 ; Train Acc : 0.604 ; Test Loss : 0.051958 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 310 ; Train Loss : 0.045426 ; Train Acc : 0.603 ; Test Loss : 0.051698 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 311 ; Train Loss : 0.045446 ; Train Acc : 0.603 ; Test Loss : 0.051676 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 312 ; Train Loss : 0.045176 ; Train Acc : 0.604 ; Test Loss : 0.051670 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 313 ; Train Loss : 0.045169 ; Train Acc : 0.604 ; Test Loss : 0.051691 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 314 ; Train Loss : 0.045241 ; Train Acc : 0.603 ; Test Loss : 0.051661 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 315 ; Train Loss : 0.045095 ; Train Acc : 0.603 ; Test Loss : 0.051723 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 316 ; Train Loss : 0.045207 ; Train Acc : 0.602 ; Test Loss : 0.051715 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 317 ; Train Loss : 0.045083 ; Train Acc : 0.604 ; Test Loss : 0.051913 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 318 ; Train Loss : 0.045236 ; Train Acc : 0.604 ; Test Loss : 0.051752 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 319 ; Train Loss : 0.045223 ; Train Acc : 0.603 ; Test Loss : 0.051839 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 320 ; Train Loss : 0.044735 ; Train Acc : 0.603 ; Test Loss : 0.051824 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 321 ; Train Loss : 0.045109 ; Train Acc : 0.604 ; Test Loss : 0.051742 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 322 ; Train Loss : 0.044880 ; Train Acc : 0.604 ; Test Loss : 0.051802 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 323 ; Train Loss : 0.045247 ; Train Acc : 0.604 ; Test Loss : 0.051752 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 324 ; Train Loss : 0.045177 ; Train Acc : 0.605 ; Test Loss : 0.051880 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 325 ; Train Loss : 0.044989 ; Train Acc : 0.604 ; Test Loss : 0.052007 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 326 ; Train Loss : 0.045294 ; Train Acc : 0.603 ; Test Loss : 0.051776 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 327 ; Train Loss : 0.045555 ; Train Acc : 0.604 ; Test Loss : 0.051993 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 328 ; Train Loss : 0.045199 ; Train Acc : 0.604 ; Test Loss : 0.051970 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 329 ; Train Loss : 0.045414 ; Train Acc : 0.603 ; Test Loss : 0.051764 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 330 ; Train Loss : 0.045161 ; Train Acc : 0.604 ; Test Loss : 0.051887 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 331 ; Train Loss : 0.045086 ; Train Acc : 0.604 ; Test Loss : 0.051810 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 332 ; Train Loss : 0.045388 ; Train Acc : 0.604 ; Test Loss : 0.051781 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 333 ; Train Loss : 0.045197 ; Train Acc : 0.603 ; Test Loss : 0.051764 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 334 ; Train Loss : 0.045194 ; Train Acc : 0.603 ; Test Loss : 0.052033 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 335 ; Train Loss : 0.045125 ; Train Acc : 0.604 ; Test Loss : 0.051804 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 336 ; Train Loss : 0.045113 ; Train Acc : 0.603 ; Test Loss : 0.051885 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 337 ; Train Loss : 0.045192 ; Train Acc : 0.603 ; Test Loss : 0.051898 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 338 ; Train Loss : 0.044880 ; Train Acc : 0.605 ; Test Loss : 0.051943 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 339 ; Train Loss : 0.045270 ; Train Acc : 0.606 ; Test Loss : 0.051861 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 340 ; Train Loss : 0.044987 ; Train Acc : 0.605 ; Test Loss : 0.052075 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 341 ; Train Loss : 0.044701 ; Train Acc : 0.604 ; Test Loss : 0.051935 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 342 ; Train Loss : 0.044748 ; Train Acc : 0.604 ; Test Loss : 0.051856 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 343 ; Train Loss : 0.045097 ; Train Acc : 0.604 ; Test Loss : 0.051900 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 344 ; Train Loss : 0.044840 ; Train Acc : 0.604 ; Test Loss : 0.051886 ; Test Acc : 0.562 ; LR : 0.024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 345 ; Train Loss : 0.044737 ; Train Acc : 0.602 ; Test Loss : 0.052004 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 346 ; Train Loss : 0.044966 ; Train Acc : 0.604 ; Test Loss : 0.052073 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 347 ; Train Loss : 0.044852 ; Train Acc : 0.604 ; Test Loss : 0.051826 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 348 ; Train Loss : 0.045027 ; Train Acc : 0.605 ; Test Loss : 0.051895 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 349 ; Train Loss : 0.044789 ; Train Acc : 0.604 ; Test Loss : 0.051871 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 350 ; Train Loss : 0.045007 ; Train Acc : 0.604 ; Test Loss : 0.051955 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 351 ; Train Loss : 0.045051 ; Train Acc : 0.605 ; Test Loss : 0.051986 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 352 ; Train Loss : 0.044993 ; Train Acc : 0.604 ; Test Loss : 0.052057 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 353 ; Train Loss : 0.044948 ; Train Acc : 0.605 ; Test Loss : 0.052186 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 354 ; Train Loss : 0.045085 ; Train Acc : 0.604 ; Test Loss : 0.051878 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 355 ; Train Loss : 0.045156 ; Train Acc : 0.606 ; Test Loss : 0.052022 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 356 ; Train Loss : 0.044927 ; Train Acc : 0.604 ; Test Loss : 0.052043 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 357 ; Train Loss : 0.044926 ; Train Acc : 0.604 ; Test Loss : 0.052040 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 358 ; Train Loss : 0.045107 ; Train Acc : 0.606 ; Test Loss : 0.052074 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 359 ; Train Loss : 0.045202 ; Train Acc : 0.604 ; Test Loss : 0.052108 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 360 ; Train Loss : 0.044722 ; Train Acc : 0.605 ; Test Loss : 0.052019 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 361 ; Train Loss : 0.044997 ; Train Acc : 0.604 ; Test Loss : 0.052111 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 362 ; Train Loss : 0.044990 ; Train Acc : 0.604 ; Test Loss : 0.052088 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 363 ; Train Loss : 0.044729 ; Train Acc : 0.603 ; Test Loss : 0.052005 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 364 ; Train Loss : 0.044829 ; Train Acc : 0.603 ; Test Loss : 0.052156 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 365 ; Train Loss : 0.045024 ; Train Acc : 0.604 ; Test Loss : 0.052097 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 366 ; Train Loss : 0.044922 ; Train Acc : 0.604 ; Test Loss : 0.052159 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 367 ; Train Loss : 0.045164 ; Train Acc : 0.606 ; Test Loss : 0.052390 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 368 ; Train Loss : 0.044863 ; Train Acc : 0.604 ; Test Loss : 0.051954 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 369 ; Train Loss : 0.044937 ; Train Acc : 0.605 ; Test Loss : 0.052093 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 370 ; Train Loss : 0.044873 ; Train Acc : 0.604 ; Test Loss : 0.051961 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 371 ; Train Loss : 0.044793 ; Train Acc : 0.604 ; Test Loss : 0.052154 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 372 ; Train Loss : 0.044837 ; Train Acc : 0.605 ; Test Loss : 0.052212 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 373 ; Train Loss : 0.044997 ; Train Acc : 0.604 ; Test Loss : 0.052027 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 374 ; Train Loss : 0.044716 ; Train Acc : 0.604 ; Test Loss : 0.052203 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 375 ; Train Loss : 0.044564 ; Train Acc : 0.604 ; Test Loss : 0.052222 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 376 ; Train Loss : 0.044714 ; Train Acc : 0.606 ; Test Loss : 0.052108 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 377 ; Train Loss : 0.044827 ; Train Acc : 0.604 ; Test Loss : 0.052042 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 378 ; Train Loss : 0.044632 ; Train Acc : 0.606 ; Test Loss : 0.052097 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 379 ; Train Loss : 0.045009 ; Train Acc : 0.605 ; Test Loss : 0.052283 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 380 ; Train Loss : 0.044865 ; Train Acc : 0.604 ; Test Loss : 0.052173 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 381 ; Train Loss : 0.044877 ; Train Acc : 0.604 ; Test Loss : 0.052037 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 382 ; Train Loss : 0.045225 ; Train Acc : 0.606 ; Test Loss : 0.052144 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 383 ; Train Loss : 0.044977 ; Train Acc : 0.604 ; Test Loss : 0.052133 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 384 ; Train Loss : 0.044805 ; Train Acc : 0.605 ; Test Loss : 0.052262 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 385 ; Train Loss : 0.044541 ; Train Acc : 0.605 ; Test Loss : 0.052055 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 386 ; Train Loss : 0.044851 ; Train Acc : 0.605 ; Test Loss : 0.052060 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 387 ; Train Loss : 0.044553 ; Train Acc : 0.604 ; Test Loss : 0.052039 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 388 ; Train Loss : 0.044601 ; Train Acc : 0.604 ; Test Loss : 0.052144 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 389 ; Train Loss : 0.044602 ; Train Acc : 0.605 ; Test Loss : 0.052079 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 390 ; Train Loss : 0.044668 ; Train Acc : 0.604 ; Test Loss : 0.052121 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 391 ; Train Loss : 0.044715 ; Train Acc : 0.605 ; Test Loss : 0.052329 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 392 ; Train Loss : 0.044964 ; Train Acc : 0.604 ; Test Loss : 0.052112 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 393 ; Train Loss : 0.044744 ; Train Acc : 0.604 ; Test Loss : 0.052119 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 394 ; Train Loss : 0.044697 ; Train Acc : 0.606 ; Test Loss : 0.052106 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 395 ; Train Loss : 0.044727 ; Train Acc : 0.604 ; Test Loss : 0.052057 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 396 ; Train Loss : 0.044913 ; Train Acc : 0.605 ; Test Loss : 0.052267 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 397 ; Train Loss : 0.045000 ; Train Acc : 0.605 ; Test Loss : 0.052347 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 398 ; Train Loss : 0.044513 ; Train Acc : 0.604 ; Test Loss : 0.052203 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 399 ; Train Loss : 0.044880 ; Train Acc : 0.605 ; Test Loss : 0.052374 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 400 ; Train Loss : 0.044833 ; Train Acc : 0.604 ; Test Loss : 0.052115 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 401 ; Train Loss : 0.044645 ; Train Acc : 0.605 ; Test Loss : 0.052298 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 402 ; Train Loss : 0.044912 ; Train Acc : 0.604 ; Test Loss : 0.052267 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 403 ; Train Loss : 0.044946 ; Train Acc : 0.605 ; Test Loss : 0.052202 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 404 ; Train Loss : 0.044621 ; Train Acc : 0.605 ; Test Loss : 0.052303 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 405 ; Train Loss : 0.044781 ; Train Acc : 0.604 ; Test Loss : 0.052398 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 406 ; Train Loss : 0.044766 ; Train Acc : 0.605 ; Test Loss : 0.052237 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 407 ; Train Loss : 0.044724 ; Train Acc : 0.605 ; Test Loss : 0.052311 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 408 ; Train Loss : 0.044890 ; Train Acc : 0.605 ; Test Loss : 0.052242 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 409 ; Train Loss : 0.044548 ; Train Acc : 0.606 ; Test Loss : 0.052389 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 410 ; Train Loss : 0.044547 ; Train Acc : 0.606 ; Test Loss : 0.052623 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 411 ; Train Loss : 0.044819 ; Train Acc : 0.605 ; Test Loss : 0.052391 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 412 ; Train Loss : 0.044740 ; Train Acc : 0.604 ; Test Loss : 0.052132 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 413 ; Train Loss : 0.044520 ; Train Acc : 0.605 ; Test Loss : 0.052136 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 414 ; Train Loss : 0.044718 ; Train Acc : 0.605 ; Test Loss : 0.052232 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 415 ; Train Loss : 0.044780 ; Train Acc : 0.605 ; Test Loss : 0.052214 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 416 ; Train Loss : 0.044518 ; Train Acc : 0.605 ; Test Loss : 0.052329 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 417 ; Train Loss : 0.044743 ; Train Acc : 0.606 ; Test Loss : 0.052379 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 418 ; Train Loss : 0.044755 ; Train Acc : 0.605 ; Test Loss : 0.052431 ; Test Acc : 0.500 ; LR : 0.019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 419 ; Train Loss : 0.044575 ; Train Acc : 0.605 ; Test Loss : 0.052170 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 420 ; Train Loss : 0.044352 ; Train Acc : 0.604 ; Test Loss : 0.052293 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 421 ; Train Loss : 0.044594 ; Train Acc : 0.605 ; Test Loss : 0.052267 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 422 ; Train Loss : 0.044770 ; Train Acc : 0.604 ; Test Loss : 0.052356 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 423 ; Train Loss : 0.044771 ; Train Acc : 0.605 ; Test Loss : 0.052276 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 424 ; Train Loss : 0.044730 ; Train Acc : 0.605 ; Test Loss : 0.052274 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 425 ; Train Loss : 0.044338 ; Train Acc : 0.604 ; Test Loss : 0.052294 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 426 ; Train Loss : 0.044751 ; Train Acc : 0.605 ; Test Loss : 0.052408 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 427 ; Train Loss : 0.044683 ; Train Acc : 0.604 ; Test Loss : 0.052493 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 428 ; Train Loss : 0.044939 ; Train Acc : 0.605 ; Test Loss : 0.052397 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 429 ; Train Loss : 0.044978 ; Train Acc : 0.604 ; Test Loss : 0.052305 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 430 ; Train Loss : 0.044586 ; Train Acc : 0.605 ; Test Loss : 0.052318 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 431 ; Train Loss : 0.044559 ; Train Acc : 0.605 ; Test Loss : 0.052256 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 432 ; Train Loss : 0.044604 ; Train Acc : 0.606 ; Test Loss : 0.052185 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 433 ; Train Loss : 0.044802 ; Train Acc : 0.605 ; Test Loss : 0.052516 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 434 ; Train Loss : 0.044289 ; Train Acc : 0.606 ; Test Loss : 0.052281 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 435 ; Train Loss : 0.044340 ; Train Acc : 0.605 ; Test Loss : 0.052347 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 436 ; Train Loss : 0.044539 ; Train Acc : 0.604 ; Test Loss : 0.052295 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 437 ; Train Loss : 0.045205 ; Train Acc : 0.603 ; Test Loss : 0.052977 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 438 ; Train Loss : 0.044615 ; Train Acc : 0.605 ; Test Loss : 0.052459 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 439 ; Train Loss : 0.044483 ; Train Acc : 0.605 ; Test Loss : 0.052215 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 440 ; Train Loss : 0.044349 ; Train Acc : 0.605 ; Test Loss : 0.052347 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 441 ; Train Loss : 0.044622 ; Train Acc : 0.605 ; Test Loss : 0.052456 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 442 ; Train Loss : 0.044558 ; Train Acc : 0.605 ; Test Loss : 0.052300 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 443 ; Train Loss : 0.044192 ; Train Acc : 0.605 ; Test Loss : 0.052299 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 444 ; Train Loss : 0.044694 ; Train Acc : 0.605 ; Test Loss : 0.052291 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 445 ; Train Loss : 0.044739 ; Train Acc : 0.604 ; Test Loss : 0.052323 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 446 ; Train Loss : 0.044713 ; Train Acc : 0.605 ; Test Loss : 0.052553 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 447 ; Train Loss : 0.044604 ; Train Acc : 0.606 ; Test Loss : 0.052434 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 448 ; Train Loss : 0.044488 ; Train Acc : 0.605 ; Test Loss : 0.052351 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 449 ; Train Loss : 0.044524 ; Train Acc : 0.604 ; Test Loss : 0.052460 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 450 ; Train Loss : 0.044593 ; Train Acc : 0.605 ; Test Loss : 0.052752 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 451 ; Train Loss : 0.044566 ; Train Acc : 0.604 ; Test Loss : 0.052473 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 452 ; Train Loss : 0.044397 ; Train Acc : 0.604 ; Test Loss : 0.052542 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 453 ; Train Loss : 0.045048 ; Train Acc : 0.605 ; Test Loss : 0.052474 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 454 ; Train Loss : 0.044673 ; Train Acc : 0.605 ; Test Loss : 0.052345 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 455 ; Train Loss : 0.044547 ; Train Acc : 0.604 ; Test Loss : 0.052333 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 456 ; Train Loss : 0.044333 ; Train Acc : 0.605 ; Test Loss : 0.052358 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 457 ; Train Loss : 0.044356 ; Train Acc : 0.606 ; Test Loss : 0.052373 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 458 ; Train Loss : 0.044600 ; Train Acc : 0.605 ; Test Loss : 0.052369 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 459 ; Train Loss : 0.044282 ; Train Acc : 0.604 ; Test Loss : 0.052312 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 460 ; Train Loss : 0.044418 ; Train Acc : 0.606 ; Test Loss : 0.052513 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 461 ; Train Loss : 0.044476 ; Train Acc : 0.605 ; Test Loss : 0.052468 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 462 ; Train Loss : 0.044424 ; Train Acc : 0.605 ; Test Loss : 0.052448 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 463 ; Train Loss : 0.044484 ; Train Acc : 0.605 ; Test Loss : 0.052443 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 464 ; Train Loss : 0.044719 ; Train Acc : 0.604 ; Test Loss : 0.052424 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 465 ; Train Loss : 0.044457 ; Train Acc : 0.606 ; Test Loss : 0.052672 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 466 ; Train Loss : 0.044563 ; Train Acc : 0.606 ; Test Loss : 0.052559 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 467 ; Train Loss : 0.044596 ; Train Acc : 0.606 ; Test Loss : 0.052379 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 468 ; Train Loss : 0.044444 ; Train Acc : 0.605 ; Test Loss : 0.052533 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 469 ; Train Loss : 0.044606 ; Train Acc : 0.605 ; Test Loss : 0.052495 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 470 ; Train Loss : 0.044402 ; Train Acc : 0.606 ; Test Loss : 0.052557 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 471 ; Train Loss : 0.044640 ; Train Acc : 0.605 ; Test Loss : 0.052708 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 472 ; Train Loss : 0.044770 ; Train Acc : 0.605 ; Test Loss : 0.052707 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 473 ; Train Loss : 0.044428 ; Train Acc : 0.606 ; Test Loss : 0.052508 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 474 ; Train Loss : 0.044606 ; Train Acc : 0.604 ; Test Loss : 0.052650 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 475 ; Train Loss : 0.044617 ; Train Acc : 0.606 ; Test Loss : 0.052607 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 476 ; Train Loss : 0.044569 ; Train Acc : 0.605 ; Test Loss : 0.052438 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 477 ; Train Loss : 0.044597 ; Train Acc : 0.606 ; Test Loss : 0.052651 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 478 ; Train Loss : 0.044393 ; Train Acc : 0.605 ; Test Loss : 0.052605 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 479 ; Train Loss : 0.044634 ; Train Acc : 0.604 ; Test Loss : 0.052505 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 480 ; Train Loss : 0.044586 ; Train Acc : 0.604 ; Test Loss : 0.052494 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 481 ; Train Loss : 0.044351 ; Train Acc : 0.607 ; Test Loss : 0.052558 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 482 ; Train Loss : 0.044401 ; Train Acc : 0.605 ; Test Loss : 0.052511 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 483 ; Train Loss : 0.044649 ; Train Acc : 0.604 ; Test Loss : 0.052900 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 484 ; Train Loss : 0.044575 ; Train Acc : 0.604 ; Test Loss : 0.052549 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 485 ; Train Loss : 0.044395 ; Train Acc : 0.605 ; Test Loss : 0.052643 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 486 ; Train Loss : 0.044457 ; Train Acc : 0.605 ; Test Loss : 0.052837 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 487 ; Train Loss : 0.044352 ; Train Acc : 0.605 ; Test Loss : 0.052488 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 488 ; Train Loss : 0.044144 ; Train Acc : 0.605 ; Test Loss : 0.052525 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 489 ; Train Loss : 0.044520 ; Train Acc : 0.605 ; Test Loss : 0.052571 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 490 ; Train Loss : 0.044559 ; Train Acc : 0.605 ; Test Loss : 0.052475 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 491 ; Train Loss : 0.044460 ; Train Acc : 0.605 ; Test Loss : 0.052527 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 492 ; Train Loss : 0.044236 ; Train Acc : 0.605 ; Test Loss : 0.052643 ; Test Acc : 0.500 ; LR : 0.017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 493 ; Train Loss : 0.044430 ; Train Acc : 0.605 ; Test Loss : 0.052615 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 494 ; Train Loss : 0.044417 ; Train Acc : 0.605 ; Test Loss : 0.052776 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 495 ; Train Loss : 0.044118 ; Train Acc : 0.606 ; Test Loss : 0.052666 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 496 ; Train Loss : 0.044283 ; Train Acc : 0.604 ; Test Loss : 0.052633 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 497 ; Train Loss : 0.044618 ; Train Acc : 0.605 ; Test Loss : 0.052727 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 498 ; Train Loss : 0.044267 ; Train Acc : 0.606 ; Test Loss : 0.052611 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 499 ; Train Loss : 0.044413 ; Train Acc : 0.605 ; Test Loss : 0.052559 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 500 ; Train Loss : 0.044284 ; Train Acc : 0.604 ; Test Loss : 0.052861 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 501 ; Train Loss : 0.044170 ; Train Acc : 0.605 ; Test Loss : 0.052859 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 502 ; Train Loss : 0.044379 ; Train Acc : 0.604 ; Test Loss : 0.052549 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 503 ; Train Loss : 0.044376 ; Train Acc : 0.605 ; Test Loss : 0.052695 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 504 ; Train Loss : 0.044421 ; Train Acc : 0.606 ; Test Loss : 0.052579 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 505 ; Train Loss : 0.044175 ; Train Acc : 0.606 ; Test Loss : 0.052830 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 506 ; Train Loss : 0.043860 ; Train Acc : 0.605 ; Test Loss : 0.052590 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 507 ; Train Loss : 0.044318 ; Train Acc : 0.606 ; Test Loss : 0.052740 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 508 ; Train Loss : 0.044244 ; Train Acc : 0.606 ; Test Loss : 0.052707 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 509 ; Train Loss : 0.044293 ; Train Acc : 0.605 ; Test Loss : 0.052686 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 510 ; Train Loss : 0.044095 ; Train Acc : 0.606 ; Test Loss : 0.052684 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 511 ; Train Loss : 0.044532 ; Train Acc : 0.606 ; Test Loss : 0.052574 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 512 ; Train Loss : 0.044336 ; Train Acc : 0.606 ; Test Loss : 0.052629 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 513 ; Train Loss : 0.044203 ; Train Acc : 0.605 ; Test Loss : 0.052885 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 514 ; Train Loss : 0.044412 ; Train Acc : 0.604 ; Test Loss : 0.052592 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 515 ; Train Loss : 0.044116 ; Train Acc : 0.605 ; Test Loss : 0.052725 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 516 ; Train Loss : 0.044314 ; Train Acc : 0.604 ; Test Loss : 0.052589 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 517 ; Train Loss : 0.044240 ; Train Acc : 0.605 ; Test Loss : 0.052680 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 518 ; Train Loss : 0.044086 ; Train Acc : 0.605 ; Test Loss : 0.052629 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 519 ; Train Loss : 0.044290 ; Train Acc : 0.606 ; Test Loss : 0.052745 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 520 ; Train Loss : 0.044217 ; Train Acc : 0.605 ; Test Loss : 0.052761 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 521 ; Train Loss : 0.044196 ; Train Acc : 0.606 ; Test Loss : 0.052766 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 522 ; Train Loss : 0.044435 ; Train Acc : 0.606 ; Test Loss : 0.052803 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 523 ; Train Loss : 0.044007 ; Train Acc : 0.606 ; Test Loss : 0.052736 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 524 ; Train Loss : 0.044351 ; Train Acc : 0.606 ; Test Loss : 0.052690 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 525 ; Train Loss : 0.044146 ; Train Acc : 0.606 ; Test Loss : 0.052751 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 526 ; Train Loss : 0.044311 ; Train Acc : 0.604 ; Test Loss : 0.052684 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 527 ; Train Loss : 0.044187 ; Train Acc : 0.605 ; Test Loss : 0.052742 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 528 ; Train Loss : 0.044499 ; Train Acc : 0.606 ; Test Loss : 0.052742 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 529 ; Train Loss : 0.044658 ; Train Acc : 0.605 ; Test Loss : 0.052895 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 530 ; Train Loss : 0.044337 ; Train Acc : 0.605 ; Test Loss : 0.052881 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 531 ; Train Loss : 0.043927 ; Train Acc : 0.606 ; Test Loss : 0.052670 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 532 ; Train Loss : 0.044307 ; Train Acc : 0.606 ; Test Loss : 0.052938 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 533 ; Train Loss : 0.044215 ; Train Acc : 0.606 ; Test Loss : 0.052961 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 534 ; Train Loss : 0.044085 ; Train Acc : 0.605 ; Test Loss : 0.052681 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 535 ; Train Loss : 0.044345 ; Train Acc : 0.606 ; Test Loss : 0.052829 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 536 ; Train Loss : 0.044258 ; Train Acc : 0.604 ; Test Loss : 0.052747 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 537 ; Train Loss : 0.044070 ; Train Acc : 0.605 ; Test Loss : 0.052820 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 538 ; Train Loss : 0.044217 ; Train Acc : 0.604 ; Test Loss : 0.052858 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 539 ; Train Loss : 0.044207 ; Train Acc : 0.605 ; Test Loss : 0.052984 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 540 ; Train Loss : 0.044281 ; Train Acc : 0.606 ; Test Loss : 0.052674 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 541 ; Train Loss : 0.044375 ; Train Acc : 0.605 ; Test Loss : 0.052771 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 542 ; Train Loss : 0.043883 ; Train Acc : 0.605 ; Test Loss : 0.052810 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 543 ; Train Loss : 0.044388 ; Train Acc : 0.605 ; Test Loss : 0.052960 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 544 ; Train Loss : 0.044172 ; Train Acc : 0.606 ; Test Loss : 0.052843 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 545 ; Train Loss : 0.044349 ; Train Acc : 0.606 ; Test Loss : 0.052746 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 546 ; Train Loss : 0.044059 ; Train Acc : 0.606 ; Test Loss : 0.052852 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 547 ; Train Loss : 0.044337 ; Train Acc : 0.605 ; Test Loss : 0.052939 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 548 ; Train Loss : 0.044241 ; Train Acc : 0.605 ; Test Loss : 0.053055 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 549 ; Train Loss : 0.044197 ; Train Acc : 0.606 ; Test Loss : 0.052817 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 550 ; Train Loss : 0.043931 ; Train Acc : 0.606 ; Test Loss : 0.052715 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 551 ; Train Loss : 0.044275 ; Train Acc : 0.605 ; Test Loss : 0.052813 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 552 ; Train Loss : 0.044255 ; Train Acc : 0.605 ; Test Loss : 0.052826 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 553 ; Train Loss : 0.044132 ; Train Acc : 0.606 ; Test Loss : 0.053076 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 554 ; Train Loss : 0.044368 ; Train Acc : 0.606 ; Test Loss : 0.052859 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 555 ; Train Loss : 0.043973 ; Train Acc : 0.605 ; Test Loss : 0.052761 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 556 ; Train Loss : 0.044065 ; Train Acc : 0.606 ; Test Loss : 0.052859 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 557 ; Train Loss : 0.043996 ; Train Acc : 0.605 ; Test Loss : 0.052849 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 558 ; Train Loss : 0.044150 ; Train Acc : 0.606 ; Test Loss : 0.052797 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 559 ; Train Loss : 0.044284 ; Train Acc : 0.605 ; Test Loss : 0.052961 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 560 ; Train Loss : 0.044141 ; Train Acc : 0.605 ; Test Loss : 0.052897 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 561 ; Train Loss : 0.044101 ; Train Acc : 0.605 ; Test Loss : 0.052841 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 562 ; Train Loss : 0.044176 ; Train Acc : 0.605 ; Test Loss : 0.052768 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 563 ; Train Loss : 0.044200 ; Train Acc : 0.606 ; Test Loss : 0.052849 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 564 ; Train Loss : 0.043961 ; Train Acc : 0.605 ; Test Loss : 0.052855 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 565 ; Train Loss : 0.044446 ; Train Acc : 0.606 ; Test Loss : 0.053007 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 566 ; Train Loss : 0.044313 ; Train Acc : 0.605 ; Test Loss : 0.052956 ; Test Acc : 0.500 ; LR : 0.014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 567 ; Train Loss : 0.044276 ; Train Acc : 0.605 ; Test Loss : 0.053072 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 568 ; Train Loss : 0.044331 ; Train Acc : 0.605 ; Test Loss : 0.052829 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 569 ; Train Loss : 0.044146 ; Train Acc : 0.607 ; Test Loss : 0.052853 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 570 ; Train Loss : 0.044288 ; Train Acc : 0.605 ; Test Loss : 0.052865 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 571 ; Train Loss : 0.044148 ; Train Acc : 0.604 ; Test Loss : 0.052984 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 572 ; Train Loss : 0.044278 ; Train Acc : 0.606 ; Test Loss : 0.053070 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 573 ; Train Loss : 0.044512 ; Train Acc : 0.605 ; Test Loss : 0.052985 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 574 ; Train Loss : 0.044295 ; Train Acc : 0.606 ; Test Loss : 0.052826 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 575 ; Train Loss : 0.043959 ; Train Acc : 0.605 ; Test Loss : 0.052955 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 576 ; Train Loss : 0.044031 ; Train Acc : 0.606 ; Test Loss : 0.052972 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 577 ; Train Loss : 0.044159 ; Train Acc : 0.606 ; Test Loss : 0.052819 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 578 ; Train Loss : 0.044049 ; Train Acc : 0.606 ; Test Loss : 0.052968 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 579 ; Train Loss : 0.044154 ; Train Acc : 0.605 ; Test Loss : 0.052899 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 580 ; Train Loss : 0.044088 ; Train Acc : 0.606 ; Test Loss : 0.052987 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 581 ; Train Loss : 0.044347 ; Train Acc : 0.606 ; Test Loss : 0.053166 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 582 ; Train Loss : 0.044145 ; Train Acc : 0.605 ; Test Loss : 0.052820 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 583 ; Train Loss : 0.044208 ; Train Acc : 0.606 ; Test Loss : 0.053100 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 584 ; Train Loss : 0.044345 ; Train Acc : 0.604 ; Test Loss : 0.053003 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 585 ; Train Loss : 0.044177 ; Train Acc : 0.606 ; Test Loss : 0.052917 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 586 ; Train Loss : 0.044278 ; Train Acc : 0.606 ; Test Loss : 0.052931 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 587 ; Train Loss : 0.044177 ; Train Acc : 0.606 ; Test Loss : 0.052938 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 588 ; Train Loss : 0.044153 ; Train Acc : 0.606 ; Test Loss : 0.052806 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 589 ; Train Loss : 0.044180 ; Train Acc : 0.606 ; Test Loss : 0.052981 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 590 ; Train Loss : 0.044228 ; Train Acc : 0.606 ; Test Loss : 0.052946 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 591 ; Train Loss : 0.044245 ; Train Acc : 0.605 ; Test Loss : 0.053090 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 592 ; Train Loss : 0.044324 ; Train Acc : 0.606 ; Test Loss : 0.052894 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 593 ; Train Loss : 0.044088 ; Train Acc : 0.605 ; Test Loss : 0.053098 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 594 ; Train Loss : 0.044030 ; Train Acc : 0.606 ; Test Loss : 0.052976 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 595 ; Train Loss : 0.044015 ; Train Acc : 0.606 ; Test Loss : 0.053110 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 596 ; Train Loss : 0.044292 ; Train Acc : 0.605 ; Test Loss : 0.053011 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 597 ; Train Loss : 0.044245 ; Train Acc : 0.605 ; Test Loss : 0.052992 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 598 ; Train Loss : 0.044176 ; Train Acc : 0.606 ; Test Loss : 0.053085 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 599 ; Train Loss : 0.044194 ; Train Acc : 0.605 ; Test Loss : 0.053006 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 600 ; Train Loss : 0.044262 ; Train Acc : 0.606 ; Test Loss : 0.052904 ; Test Acc : 0.500 ; LR : 0.014\n",
      "\n",
      "Hidden Neurons : 30 ; Train Loss : 0.044132 ; Train Acc : 0.605 ; Test Loss : 0.053124 ; Test Acc : 0.438\n",
      "\n",
      "\n",
      "Epoch : 1 ; Train Loss : 0.147318 ; Train Acc : 0.120 ; Test Loss : 0.091953 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 2 ; Train Loss : 0.089451 ; Train Acc : 0.162 ; Test Loss : 0.087344 ; Test Acc : 0.188 ; LR : 0.045\n",
      "Epoch : 3 ; Train Loss : 0.085347 ; Train Acc : 0.211 ; Test Loss : 0.084390 ; Test Acc : 0.188 ; LR : 0.045\n",
      "Epoch : 4 ; Train Loss : 0.083482 ; Train Acc : 0.212 ; Test Loss : 0.083455 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 5 ; Train Loss : 0.081673 ; Train Acc : 0.240 ; Test Loss : 0.081586 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 6 ; Train Loss : 0.079383 ; Train Acc : 0.261 ; Test Loss : 0.079984 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 7 ; Train Loss : 0.078165 ; Train Acc : 0.277 ; Test Loss : 0.079377 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 8 ; Train Loss : 0.077282 ; Train Acc : 0.296 ; Test Loss : 0.078062 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 9 ; Train Loss : 0.076269 ; Train Acc : 0.331 ; Test Loss : 0.076540 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 10 ; Train Loss : 0.074199 ; Train Acc : 0.361 ; Test Loss : 0.074222 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 11 ; Train Loss : 0.070316 ; Train Acc : 0.395 ; Test Loss : 0.069765 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 12 ; Train Loss : 0.066379 ; Train Acc : 0.436 ; Test Loss : 0.066120 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 13 ; Train Loss : 0.063536 ; Train Acc : 0.453 ; Test Loss : 0.064428 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 14 ; Train Loss : 0.062323 ; Train Acc : 0.457 ; Test Loss : 0.063845 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 15 ; Train Loss : 0.061984 ; Train Acc : 0.451 ; Test Loss : 0.063407 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 16 ; Train Loss : 0.061390 ; Train Acc : 0.466 ; Test Loss : 0.063183 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 17 ; Train Loss : 0.061379 ; Train Acc : 0.458 ; Test Loss : 0.062968 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 18 ; Train Loss : 0.061059 ; Train Acc : 0.460 ; Test Loss : 0.062567 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 19 ; Train Loss : 0.060673 ; Train Acc : 0.459 ; Test Loss : 0.062578 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 20 ; Train Loss : 0.060521 ; Train Acc : 0.457 ; Test Loss : 0.062229 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 21 ; Train Loss : 0.060146 ; Train Acc : 0.472 ; Test Loss : 0.062158 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 22 ; Train Loss : 0.060219 ; Train Acc : 0.459 ; Test Loss : 0.062021 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 23 ; Train Loss : 0.059974 ; Train Acc : 0.466 ; Test Loss : 0.061988 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 24 ; Train Loss : 0.059962 ; Train Acc : 0.470 ; Test Loss : 0.061966 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 25 ; Train Loss : 0.059604 ; Train Acc : 0.475 ; Test Loss : 0.062089 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 26 ; Train Loss : 0.059722 ; Train Acc : 0.464 ; Test Loss : 0.061527 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 27 ; Train Loss : 0.059112 ; Train Acc : 0.482 ; Test Loss : 0.061198 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 28 ; Train Loss : 0.058334 ; Train Acc : 0.492 ; Test Loss : 0.060100 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 29 ; Train Loss : 0.056978 ; Train Acc : 0.512 ; Test Loss : 0.058319 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 30 ; Train Loss : 0.054983 ; Train Acc : 0.532 ; Test Loss : 0.056498 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 31 ; Train Loss : 0.053092 ; Train Acc : 0.545 ; Test Loss : 0.055724 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 32 ; Train Loss : 0.052307 ; Train Acc : 0.556 ; Test Loss : 0.055242 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 33 ; Train Loss : 0.052066 ; Train Acc : 0.550 ; Test Loss : 0.054644 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 34 ; Train Loss : 0.051587 ; Train Acc : 0.555 ; Test Loss : 0.054484 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 35 ; Train Loss : 0.051393 ; Train Acc : 0.562 ; Test Loss : 0.054505 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 36 ; Train Loss : 0.051304 ; Train Acc : 0.571 ; Test Loss : 0.054248 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 37 ; Train Loss : 0.051040 ; Train Acc : 0.563 ; Test Loss : 0.054049 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 38 ; Train Loss : 0.050786 ; Train Acc : 0.560 ; Test Loss : 0.053953 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 39 ; Train Loss : 0.050594 ; Train Acc : 0.578 ; Test Loss : 0.052822 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 40 ; Train Loss : 0.048365 ; Train Acc : 0.606 ; Test Loss : 0.050207 ; Test Acc : 0.688 ; LR : 0.045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 41 ; Train Loss : 0.045896 ; Train Acc : 0.637 ; Test Loss : 0.048796 ; Test Acc : 0.688 ; LR : 0.045\n",
      "Epoch : 42 ; Train Loss : 0.045051 ; Train Acc : 0.650 ; Test Loss : 0.048062 ; Test Acc : 0.625 ; LR : 0.045\n",
      "Epoch : 43 ; Train Loss : 0.044404 ; Train Acc : 0.647 ; Test Loss : 0.047990 ; Test Acc : 0.625 ; LR : 0.045\n",
      "Epoch : 44 ; Train Loss : 0.044041 ; Train Acc : 0.651 ; Test Loss : 0.047327 ; Test Acc : 0.625 ; LR : 0.045\n",
      "Epoch : 45 ; Train Loss : 0.043691 ; Train Acc : 0.663 ; Test Loss : 0.046758 ; Test Acc : 0.625 ; LR : 0.045\n",
      "Epoch : 46 ; Train Loss : 0.042653 ; Train Acc : 0.678 ; Test Loss : 0.045215 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 47 ; Train Loss : 0.041198 ; Train Acc : 0.682 ; Test Loss : 0.043937 ; Test Acc : 0.625 ; LR : 0.045\n",
      "Epoch : 48 ; Train Loss : 0.039662 ; Train Acc : 0.720 ; Test Loss : 0.043082 ; Test Acc : 0.625 ; LR : 0.045\n",
      "Epoch : 49 ; Train Loss : 0.039183 ; Train Acc : 0.724 ; Test Loss : 0.042544 ; Test Acc : 0.625 ; LR : 0.045\n",
      "Epoch : 50 ; Train Loss : 0.038422 ; Train Acc : 0.729 ; Test Loss : 0.042217 ; Test Acc : 0.688 ; LR : 0.045\n",
      "Epoch : 51 ; Train Loss : 0.037990 ; Train Acc : 0.732 ; Test Loss : 0.041781 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 52 ; Train Loss : 0.037650 ; Train Acc : 0.731 ; Test Loss : 0.041674 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 53 ; Train Loss : 0.037348 ; Train Acc : 0.733 ; Test Loss : 0.041809 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 54 ; Train Loss : 0.037169 ; Train Acc : 0.737 ; Test Loss : 0.041611 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 55 ; Train Loss : 0.037035 ; Train Acc : 0.737 ; Test Loss : 0.041474 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 56 ; Train Loss : 0.037233 ; Train Acc : 0.732 ; Test Loss : 0.041959 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 57 ; Train Loss : 0.036932 ; Train Acc : 0.737 ; Test Loss : 0.041302 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 58 ; Train Loss : 0.036419 ; Train Acc : 0.737 ; Test Loss : 0.041030 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 59 ; Train Loss : 0.036293 ; Train Acc : 0.741 ; Test Loss : 0.041037 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 60 ; Train Loss : 0.035967 ; Train Acc : 0.740 ; Test Loss : 0.040866 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 61 ; Train Loss : 0.035963 ; Train Acc : 0.742 ; Test Loss : 0.040929 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 62 ; Train Loss : 0.035805 ; Train Acc : 0.741 ; Test Loss : 0.041142 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 63 ; Train Loss : 0.035880 ; Train Acc : 0.743 ; Test Loss : 0.040929 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 64 ; Train Loss : 0.035869 ; Train Acc : 0.741 ; Test Loss : 0.041060 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 65 ; Train Loss : 0.035421 ; Train Acc : 0.744 ; Test Loss : 0.040794 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 66 ; Train Loss : 0.035505 ; Train Acc : 0.742 ; Test Loss : 0.040810 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 67 ; Train Loss : 0.035471 ; Train Acc : 0.743 ; Test Loss : 0.041020 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 68 ; Train Loss : 0.035650 ; Train Acc : 0.744 ; Test Loss : 0.040957 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 69 ; Train Loss : 0.035271 ; Train Acc : 0.741 ; Test Loss : 0.040808 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 70 ; Train Loss : 0.035001 ; Train Acc : 0.742 ; Test Loss : 0.040910 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 71 ; Train Loss : 0.035021 ; Train Acc : 0.741 ; Test Loss : 0.041047 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 72 ; Train Loss : 0.035202 ; Train Acc : 0.742 ; Test Loss : 0.040659 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 73 ; Train Loss : 0.034939 ; Train Acc : 0.745 ; Test Loss : 0.040939 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 74 ; Train Loss : 0.035347 ; Train Acc : 0.746 ; Test Loss : 0.041069 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 75 ; Train Loss : 0.034819 ; Train Acc : 0.745 ; Test Loss : 0.040591 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 76 ; Train Loss : 0.034767 ; Train Acc : 0.745 ; Test Loss : 0.040740 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 77 ; Train Loss : 0.034763 ; Train Acc : 0.745 ; Test Loss : 0.040772 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 78 ; Train Loss : 0.034715 ; Train Acc : 0.746 ; Test Loss : 0.040736 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 79 ; Train Loss : 0.034415 ; Train Acc : 0.750 ; Test Loss : 0.040677 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 80 ; Train Loss : 0.034806 ; Train Acc : 0.749 ; Test Loss : 0.041240 ; Test Acc : 0.750 ; LR : 0.041\n",
      "Epoch : 81 ; Train Loss : 0.034426 ; Train Acc : 0.747 ; Test Loss : 0.040839 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 82 ; Train Loss : 0.034237 ; Train Acc : 0.748 ; Test Loss : 0.040650 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 83 ; Train Loss : 0.034582 ; Train Acc : 0.747 ; Test Loss : 0.040728 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 84 ; Train Loss : 0.034259 ; Train Acc : 0.745 ; Test Loss : 0.040805 ; Test Acc : 0.750 ; LR : 0.041\n",
      "Epoch : 85 ; Train Loss : 0.033926 ; Train Acc : 0.754 ; Test Loss : 0.040884 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 86 ; Train Loss : 0.034290 ; Train Acc : 0.744 ; Test Loss : 0.040790 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 87 ; Train Loss : 0.034094 ; Train Acc : 0.749 ; Test Loss : 0.040809 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 88 ; Train Loss : 0.034251 ; Train Acc : 0.745 ; Test Loss : 0.040635 ; Test Acc : 0.750 ; LR : 0.041\n",
      "Epoch : 89 ; Train Loss : 0.033842 ; Train Acc : 0.747 ; Test Loss : 0.040647 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 90 ; Train Loss : 0.033927 ; Train Acc : 0.746 ; Test Loss : 0.040867 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 91 ; Train Loss : 0.033762 ; Train Acc : 0.748 ; Test Loss : 0.040730 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 92 ; Train Loss : 0.033954 ; Train Acc : 0.747 ; Test Loss : 0.040998 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 93 ; Train Loss : 0.034003 ; Train Acc : 0.754 ; Test Loss : 0.041019 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 94 ; Train Loss : 0.033862 ; Train Acc : 0.750 ; Test Loss : 0.040784 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 95 ; Train Loss : 0.033683 ; Train Acc : 0.754 ; Test Loss : 0.040719 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 96 ; Train Loss : 0.033793 ; Train Acc : 0.753 ; Test Loss : 0.040589 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 97 ; Train Loss : 0.034137 ; Train Acc : 0.751 ; Test Loss : 0.040952 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 98 ; Train Loss : 0.033772 ; Train Acc : 0.751 ; Test Loss : 0.040981 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 99 ; Train Loss : 0.033523 ; Train Acc : 0.749 ; Test Loss : 0.040705 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 100 ; Train Loss : 0.033256 ; Train Acc : 0.751 ; Test Loss : 0.040714 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 101 ; Train Loss : 0.033316 ; Train Acc : 0.752 ; Test Loss : 0.040610 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 102 ; Train Loss : 0.033546 ; Train Acc : 0.755 ; Test Loss : 0.040676 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 103 ; Train Loss : 0.033525 ; Train Acc : 0.751 ; Test Loss : 0.040793 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 104 ; Train Loss : 0.033283 ; Train Acc : 0.750 ; Test Loss : 0.040684 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 105 ; Train Loss : 0.033416 ; Train Acc : 0.753 ; Test Loss : 0.040700 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 106 ; Train Loss : 0.033706 ; Train Acc : 0.758 ; Test Loss : 0.040939 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 107 ; Train Loss : 0.033063 ; Train Acc : 0.750 ; Test Loss : 0.040937 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 108 ; Train Loss : 0.033173 ; Train Acc : 0.755 ; Test Loss : 0.040813 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 109 ; Train Loss : 0.033316 ; Train Acc : 0.753 ; Test Loss : 0.040893 ; Test Acc : 0.750 ; LR : 0.036\n",
      "Epoch : 110 ; Train Loss : 0.033411 ; Train Acc : 0.757 ; Test Loss : 0.040947 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 111 ; Train Loss : 0.032987 ; Train Acc : 0.760 ; Test Loss : 0.040624 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 112 ; Train Loss : 0.033280 ; Train Acc : 0.754 ; Test Loss : 0.040748 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 113 ; Train Loss : 0.033172 ; Train Acc : 0.754 ; Test Loss : 0.040772 ; Test Acc : 0.750 ; LR : 0.036\n",
      "Epoch : 114 ; Train Loss : 0.033045 ; Train Acc : 0.759 ; Test Loss : 0.040861 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 115 ; Train Loss : 0.033231 ; Train Acc : 0.755 ; Test Loss : 0.040890 ; Test Acc : 0.625 ; LR : 0.036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 116 ; Train Loss : 0.033231 ; Train Acc : 0.757 ; Test Loss : 0.041073 ; Test Acc : 0.750 ; LR : 0.036\n",
      "Epoch : 117 ; Train Loss : 0.033093 ; Train Acc : 0.756 ; Test Loss : 0.040731 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 118 ; Train Loss : 0.032767 ; Train Acc : 0.755 ; Test Loss : 0.041271 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 119 ; Train Loss : 0.033462 ; Train Acc : 0.754 ; Test Loss : 0.041235 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 120 ; Train Loss : 0.033048 ; Train Acc : 0.749 ; Test Loss : 0.040711 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 121 ; Train Loss : 0.033428 ; Train Acc : 0.756 ; Test Loss : 0.040983 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 122 ; Train Loss : 0.032855 ; Train Acc : 0.758 ; Test Loss : 0.040818 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 123 ; Train Loss : 0.032931 ; Train Acc : 0.755 ; Test Loss : 0.041391 ; Test Acc : 0.750 ; LR : 0.036\n",
      "Epoch : 124 ; Train Loss : 0.032713 ; Train Acc : 0.755 ; Test Loss : 0.040877 ; Test Acc : 0.750 ; LR : 0.036\n",
      "Epoch : 125 ; Train Loss : 0.033084 ; Train Acc : 0.753 ; Test Loss : 0.040696 ; Test Acc : 0.750 ; LR : 0.036\n",
      "Epoch : 126 ; Train Loss : 0.032831 ; Train Acc : 0.757 ; Test Loss : 0.040730 ; Test Acc : 0.750 ; LR : 0.036\n",
      "Epoch : 127 ; Train Loss : 0.033066 ; Train Acc : 0.751 ; Test Loss : 0.041118 ; Test Acc : 0.750 ; LR : 0.036\n",
      "Epoch : 128 ; Train Loss : 0.032758 ; Train Acc : 0.756 ; Test Loss : 0.040868 ; Test Acc : 0.750 ; LR : 0.036\n",
      "Epoch : 129 ; Train Loss : 0.032461 ; Train Acc : 0.759 ; Test Loss : 0.041210 ; Test Acc : 0.750 ; LR : 0.036\n",
      "Epoch : 130 ; Train Loss : 0.032632 ; Train Acc : 0.753 ; Test Loss : 0.040808 ; Test Acc : 0.750 ; LR : 0.036\n",
      "Epoch : 131 ; Train Loss : 0.032427 ; Train Acc : 0.755 ; Test Loss : 0.040753 ; Test Acc : 0.750 ; LR : 0.036\n",
      "Epoch : 132 ; Train Loss : 0.032584 ; Train Acc : 0.755 ; Test Loss : 0.040908 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 133 ; Train Loss : 0.032653 ; Train Acc : 0.754 ; Test Loss : 0.041075 ; Test Acc : 0.750 ; LR : 0.036\n",
      "Epoch : 134 ; Train Loss : 0.032645 ; Train Acc : 0.756 ; Test Loss : 0.040807 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 135 ; Train Loss : 0.032490 ; Train Acc : 0.757 ; Test Loss : 0.041124 ; Test Acc : 0.750 ; LR : 0.036\n",
      "Epoch : 136 ; Train Loss : 0.032655 ; Train Acc : 0.757 ; Test Loss : 0.041012 ; Test Acc : 0.750 ; LR : 0.036\n",
      "Epoch : 137 ; Train Loss : 0.032207 ; Train Acc : 0.765 ; Test Loss : 0.040919 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 138 ; Train Loss : 0.032459 ; Train Acc : 0.759 ; Test Loss : 0.041070 ; Test Acc : 0.750 ; LR : 0.036\n",
      "Epoch : 139 ; Train Loss : 0.032419 ; Train Acc : 0.754 ; Test Loss : 0.040915 ; Test Acc : 0.812 ; LR : 0.036\n",
      "Epoch : 140 ; Train Loss : 0.032259 ; Train Acc : 0.762 ; Test Loss : 0.041496 ; Test Acc : 0.750 ; LR : 0.036\n",
      "Epoch : 141 ; Train Loss : 0.032686 ; Train Acc : 0.759 ; Test Loss : 0.041418 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 142 ; Train Loss : 0.032498 ; Train Acc : 0.765 ; Test Loss : 0.041062 ; Test Acc : 0.750 ; LR : 0.036\n",
      "Epoch : 143 ; Train Loss : 0.032243 ; Train Acc : 0.755 ; Test Loss : 0.041152 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 144 ; Train Loss : 0.032130 ; Train Acc : 0.759 ; Test Loss : 0.041373 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 145 ; Train Loss : 0.032201 ; Train Acc : 0.759 ; Test Loss : 0.041322 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 146 ; Train Loss : 0.032432 ; Train Acc : 0.762 ; Test Loss : 0.041273 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 147 ; Train Loss : 0.032289 ; Train Acc : 0.762 ; Test Loss : 0.041045 ; Test Acc : 0.750 ; LR : 0.036\n",
      "Epoch : 148 ; Train Loss : 0.032311 ; Train Acc : 0.749 ; Test Loss : 0.041226 ; Test Acc : 0.750 ; LR : 0.036\n",
      "Epoch : 149 ; Train Loss : 0.032174 ; Train Acc : 0.757 ; Test Loss : 0.041158 ; Test Acc : 0.812 ; LR : 0.036\n",
      "Epoch : 150 ; Train Loss : 0.032088 ; Train Acc : 0.758 ; Test Loss : 0.041266 ; Test Acc : 0.750 ; LR : 0.036\n",
      "Epoch : 151 ; Train Loss : 0.031888 ; Train Acc : 0.763 ; Test Loss : 0.041428 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 152 ; Train Loss : 0.032129 ; Train Acc : 0.759 ; Test Loss : 0.041034 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 153 ; Train Loss : 0.031822 ; Train Acc : 0.762 ; Test Loss : 0.041074 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 154 ; Train Loss : 0.031953 ; Train Acc : 0.757 ; Test Loss : 0.041322 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 155 ; Train Loss : 0.031993 ; Train Acc : 0.759 ; Test Loss : 0.041127 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 156 ; Train Loss : 0.031987 ; Train Acc : 0.759 ; Test Loss : 0.041460 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 157 ; Train Loss : 0.031997 ; Train Acc : 0.761 ; Test Loss : 0.041255 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 158 ; Train Loss : 0.032108 ; Train Acc : 0.758 ; Test Loss : 0.041216 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 159 ; Train Loss : 0.032052 ; Train Acc : 0.759 ; Test Loss : 0.041117 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 160 ; Train Loss : 0.031767 ; Train Acc : 0.761 ; Test Loss : 0.041209 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 161 ; Train Loss : 0.031518 ; Train Acc : 0.759 ; Test Loss : 0.041237 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 162 ; Train Loss : 0.032102 ; Train Acc : 0.756 ; Test Loss : 0.041231 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 163 ; Train Loss : 0.032012 ; Train Acc : 0.760 ; Test Loss : 0.041161 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 164 ; Train Loss : 0.031800 ; Train Acc : 0.757 ; Test Loss : 0.041319 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 165 ; Train Loss : 0.032218 ; Train Acc : 0.764 ; Test Loss : 0.041140 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 166 ; Train Loss : 0.031609 ; Train Acc : 0.759 ; Test Loss : 0.041170 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 167 ; Train Loss : 0.031732 ; Train Acc : 0.762 ; Test Loss : 0.041207 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 168 ; Train Loss : 0.031642 ; Train Acc : 0.757 ; Test Loss : 0.041209 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 169 ; Train Loss : 0.031599 ; Train Acc : 0.760 ; Test Loss : 0.041489 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 170 ; Train Loss : 0.032033 ; Train Acc : 0.763 ; Test Loss : 0.041394 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 171 ; Train Loss : 0.031570 ; Train Acc : 0.757 ; Test Loss : 0.041632 ; Test Acc : 0.812 ; LR : 0.033\n",
      "Epoch : 172 ; Train Loss : 0.031784 ; Train Acc : 0.759 ; Test Loss : 0.041352 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 173 ; Train Loss : 0.031309 ; Train Acc : 0.765 ; Test Loss : 0.041573 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 174 ; Train Loss : 0.031456 ; Train Acc : 0.763 ; Test Loss : 0.041623 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 175 ; Train Loss : 0.031742 ; Train Acc : 0.759 ; Test Loss : 0.041801 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 176 ; Train Loss : 0.031719 ; Train Acc : 0.765 ; Test Loss : 0.041348 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 177 ; Train Loss : 0.031603 ; Train Acc : 0.758 ; Test Loss : 0.041404 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 178 ; Train Loss : 0.031814 ; Train Acc : 0.763 ; Test Loss : 0.041540 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 179 ; Train Loss : 0.031777 ; Train Acc : 0.760 ; Test Loss : 0.041609 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 180 ; Train Loss : 0.031693 ; Train Acc : 0.762 ; Test Loss : 0.041247 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 181 ; Train Loss : 0.031460 ; Train Acc : 0.755 ; Test Loss : 0.041513 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 182 ; Train Loss : 0.031515 ; Train Acc : 0.759 ; Test Loss : 0.041970 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 183 ; Train Loss : 0.031691 ; Train Acc : 0.760 ; Test Loss : 0.041485 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 184 ; Train Loss : 0.031553 ; Train Acc : 0.768 ; Test Loss : 0.041351 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 185 ; Train Loss : 0.031494 ; Train Acc : 0.761 ; Test Loss : 0.041431 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 186 ; Train Loss : 0.031731 ; Train Acc : 0.766 ; Test Loss : 0.041978 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 187 ; Train Loss : 0.031735 ; Train Acc : 0.763 ; Test Loss : 0.041718 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 188 ; Train Loss : 0.031816 ; Train Acc : 0.763 ; Test Loss : 0.041731 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 189 ; Train Loss : 0.031464 ; Train Acc : 0.760 ; Test Loss : 0.041667 ; Test Acc : 0.750 ; LR : 0.033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 190 ; Train Loss : 0.031495 ; Train Acc : 0.761 ; Test Loss : 0.041582 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 191 ; Train Loss : 0.031451 ; Train Acc : 0.765 ; Test Loss : 0.041577 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 192 ; Train Loss : 0.031323 ; Train Acc : 0.760 ; Test Loss : 0.041557 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 193 ; Train Loss : 0.031404 ; Train Acc : 0.762 ; Test Loss : 0.041707 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 194 ; Train Loss : 0.031519 ; Train Acc : 0.764 ; Test Loss : 0.041789 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 195 ; Train Loss : 0.031093 ; Train Acc : 0.763 ; Test Loss : 0.041663 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 196 ; Train Loss : 0.031163 ; Train Acc : 0.770 ; Test Loss : 0.041791 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 197 ; Train Loss : 0.031381 ; Train Acc : 0.764 ; Test Loss : 0.041633 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 198 ; Train Loss : 0.031541 ; Train Acc : 0.763 ; Test Loss : 0.041688 ; Test Acc : 0.812 ; LR : 0.033\n",
      "Epoch : 199 ; Train Loss : 0.031450 ; Train Acc : 0.769 ; Test Loss : 0.041771 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 200 ; Train Loss : 0.031530 ; Train Acc : 0.764 ; Test Loss : 0.041818 ; Test Acc : 0.750 ; LR : 0.033\n",
      "Epoch : 201 ; Train Loss : 0.031508 ; Train Acc : 0.764 ; Test Loss : 0.042108 ; Test Acc : 0.750 ; LR : 0.030\n",
      "Epoch : 202 ; Train Loss : 0.031405 ; Train Acc : 0.768 ; Test Loss : 0.041842 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 203 ; Train Loss : 0.031490 ; Train Acc : 0.759 ; Test Loss : 0.042084 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 204 ; Train Loss : 0.030927 ; Train Acc : 0.763 ; Test Loss : 0.041624 ; Test Acc : 0.750 ; LR : 0.030\n",
      "Epoch : 205 ; Train Loss : 0.031010 ; Train Acc : 0.760 ; Test Loss : 0.042097 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 206 ; Train Loss : 0.031230 ; Train Acc : 0.762 ; Test Loss : 0.041766 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 207 ; Train Loss : 0.031189 ; Train Acc : 0.762 ; Test Loss : 0.041757 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 208 ; Train Loss : 0.031581 ; Train Acc : 0.766 ; Test Loss : 0.041865 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 209 ; Train Loss : 0.031202 ; Train Acc : 0.763 ; Test Loss : 0.041825 ; Test Acc : 0.750 ; LR : 0.030\n",
      "Epoch : 210 ; Train Loss : 0.031048 ; Train Acc : 0.762 ; Test Loss : 0.041864 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 211 ; Train Loss : 0.031114 ; Train Acc : 0.763 ; Test Loss : 0.041665 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 212 ; Train Loss : 0.030976 ; Train Acc : 0.758 ; Test Loss : 0.042095 ; Test Acc : 0.750 ; LR : 0.030\n",
      "Epoch : 213 ; Train Loss : 0.030968 ; Train Acc : 0.761 ; Test Loss : 0.041880 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 214 ; Train Loss : 0.030958 ; Train Acc : 0.763 ; Test Loss : 0.041881 ; Test Acc : 0.750 ; LR : 0.030\n",
      "Epoch : 215 ; Train Loss : 0.031239 ; Train Acc : 0.763 ; Test Loss : 0.042172 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 216 ; Train Loss : 0.031201 ; Train Acc : 0.759 ; Test Loss : 0.042372 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 217 ; Train Loss : 0.031174 ; Train Acc : 0.762 ; Test Loss : 0.042249 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 218 ; Train Loss : 0.030797 ; Train Acc : 0.759 ; Test Loss : 0.042076 ; Test Acc : 0.812 ; LR : 0.030\n",
      "Epoch : 219 ; Train Loss : 0.030930 ; Train Acc : 0.764 ; Test Loss : 0.041867 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 220 ; Train Loss : 0.031151 ; Train Acc : 0.766 ; Test Loss : 0.042073 ; Test Acc : 0.750 ; LR : 0.030\n",
      "Epoch : 221 ; Train Loss : 0.031010 ; Train Acc : 0.767 ; Test Loss : 0.042685 ; Test Acc : 0.750 ; LR : 0.030\n",
      "Epoch : 222 ; Train Loss : 0.031265 ; Train Acc : 0.764 ; Test Loss : 0.042483 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 223 ; Train Loss : 0.030756 ; Train Acc : 0.765 ; Test Loss : 0.042201 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 224 ; Train Loss : 0.030897 ; Train Acc : 0.767 ; Test Loss : 0.041954 ; Test Acc : 0.750 ; LR : 0.030\n",
      "Epoch : 225 ; Train Loss : 0.030826 ; Train Acc : 0.765 ; Test Loss : 0.041945 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 226 ; Train Loss : 0.030903 ; Train Acc : 0.767 ; Test Loss : 0.042720 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 227 ; Train Loss : 0.030786 ; Train Acc : 0.764 ; Test Loss : 0.042052 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 228 ; Train Loss : 0.031049 ; Train Acc : 0.762 ; Test Loss : 0.042208 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 229 ; Train Loss : 0.031109 ; Train Acc : 0.762 ; Test Loss : 0.042163 ; Test Acc : 0.750 ; LR : 0.030\n",
      "Epoch : 230 ; Train Loss : 0.031163 ; Train Acc : 0.766 ; Test Loss : 0.042316 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 231 ; Train Loss : 0.030921 ; Train Acc : 0.765 ; Test Loss : 0.042240 ; Test Acc : 0.750 ; LR : 0.030\n",
      "Epoch : 232 ; Train Loss : 0.030841 ; Train Acc : 0.766 ; Test Loss : 0.042334 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 233 ; Train Loss : 0.030600 ; Train Acc : 0.761 ; Test Loss : 0.042255 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 234 ; Train Loss : 0.030757 ; Train Acc : 0.767 ; Test Loss : 0.042184 ; Test Acc : 0.750 ; LR : 0.030\n",
      "Epoch : 235 ; Train Loss : 0.030769 ; Train Acc : 0.762 ; Test Loss : 0.042094 ; Test Acc : 0.750 ; LR : 0.030\n",
      "Epoch : 236 ; Train Loss : 0.030766 ; Train Acc : 0.762 ; Test Loss : 0.042179 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 237 ; Train Loss : 0.030745 ; Train Acc : 0.769 ; Test Loss : 0.042207 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 238 ; Train Loss : 0.030862 ; Train Acc : 0.765 ; Test Loss : 0.042454 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 239 ; Train Loss : 0.030976 ; Train Acc : 0.768 ; Test Loss : 0.042598 ; Test Acc : 0.812 ; LR : 0.030\n",
      "Epoch : 240 ; Train Loss : 0.030839 ; Train Acc : 0.763 ; Test Loss : 0.042585 ; Test Acc : 0.750 ; LR : 0.030\n",
      "Epoch : 241 ; Train Loss : 0.030665 ; Train Acc : 0.762 ; Test Loss : 0.042309 ; Test Acc : 0.750 ; LR : 0.030\n",
      "Epoch : 242 ; Train Loss : 0.030987 ; Train Acc : 0.762 ; Test Loss : 0.042796 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 243 ; Train Loss : 0.030948 ; Train Acc : 0.759 ; Test Loss : 0.042521 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 244 ; Train Loss : 0.030782 ; Train Acc : 0.765 ; Test Loss : 0.042737 ; Test Acc : 0.750 ; LR : 0.030\n",
      "Epoch : 245 ; Train Loss : 0.030939 ; Train Acc : 0.762 ; Test Loss : 0.042299 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 246 ; Train Loss : 0.031290 ; Train Acc : 0.763 ; Test Loss : 0.042575 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 247 ; Train Loss : 0.030911 ; Train Acc : 0.762 ; Test Loss : 0.042215 ; Test Acc : 0.750 ; LR : 0.030\n",
      "Epoch : 248 ; Train Loss : 0.030463 ; Train Acc : 0.766 ; Test Loss : 0.042970 ; Test Acc : 0.750 ; LR : 0.030\n",
      "Epoch : 249 ; Train Loss : 0.030809 ; Train Acc : 0.765 ; Test Loss : 0.042765 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 250 ; Train Loss : 0.030948 ; Train Acc : 0.766 ; Test Loss : 0.042170 ; Test Acc : 0.750 ; LR : 0.030\n",
      "Epoch : 251 ; Train Loss : 0.030645 ; Train Acc : 0.763 ; Test Loss : 0.042528 ; Test Acc : 0.688 ; LR : 0.027\n",
      "Epoch : 252 ; Train Loss : 0.030535 ; Train Acc : 0.765 ; Test Loss : 0.042597 ; Test Acc : 0.750 ; LR : 0.027\n",
      "Epoch : 253 ; Train Loss : 0.030452 ; Train Acc : 0.767 ; Test Loss : 0.042445 ; Test Acc : 0.688 ; LR : 0.027\n",
      "Epoch : 254 ; Train Loss : 0.030439 ; Train Acc : 0.765 ; Test Loss : 0.042529 ; Test Acc : 0.750 ; LR : 0.027\n",
      "Epoch : 255 ; Train Loss : 0.030490 ; Train Acc : 0.772 ; Test Loss : 0.042400 ; Test Acc : 0.688 ; LR : 0.027\n",
      "Epoch : 256 ; Train Loss : 0.030779 ; Train Acc : 0.763 ; Test Loss : 0.042422 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 257 ; Train Loss : 0.030552 ; Train Acc : 0.770 ; Test Loss : 0.042473 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 258 ; Train Loss : 0.030445 ; Train Acc : 0.764 ; Test Loss : 0.042911 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 259 ; Train Loss : 0.030662 ; Train Acc : 0.767 ; Test Loss : 0.042629 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 260 ; Train Loss : 0.030834 ; Train Acc : 0.768 ; Test Loss : 0.042706 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 261 ; Train Loss : 0.030374 ; Train Acc : 0.768 ; Test Loss : 0.042479 ; Test Acc : 0.688 ; LR : 0.027\n",
      "Epoch : 262 ; Train Loss : 0.030375 ; Train Acc : 0.766 ; Test Loss : 0.042967 ; Test Acc : 0.688 ; LR : 0.027\n",
      "Epoch : 263 ; Train Loss : 0.030442 ; Train Acc : 0.766 ; Test Loss : 0.042770 ; Test Acc : 0.688 ; LR : 0.027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 264 ; Train Loss : 0.030696 ; Train Acc : 0.769 ; Test Loss : 0.043331 ; Test Acc : 0.688 ; LR : 0.027\n",
      "Epoch : 265 ; Train Loss : 0.030629 ; Train Acc : 0.764 ; Test Loss : 0.042630 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 266 ; Train Loss : 0.030360 ; Train Acc : 0.764 ; Test Loss : 0.042486 ; Test Acc : 0.688 ; LR : 0.027\n",
      "Epoch : 267 ; Train Loss : 0.030429 ; Train Acc : 0.771 ; Test Loss : 0.043107 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 268 ; Train Loss : 0.030501 ; Train Acc : 0.764 ; Test Loss : 0.042696 ; Test Acc : 0.750 ; LR : 0.027\n",
      "Epoch : 269 ; Train Loss : 0.030302 ; Train Acc : 0.763 ; Test Loss : 0.042763 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 270 ; Train Loss : 0.030269 ; Train Acc : 0.763 ; Test Loss : 0.042897 ; Test Acc : 0.688 ; LR : 0.027\n",
      "Epoch : 271 ; Train Loss : 0.030274 ; Train Acc : 0.766 ; Test Loss : 0.042836 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 272 ; Train Loss : 0.030658 ; Train Acc : 0.768 ; Test Loss : 0.042834 ; Test Acc : 0.688 ; LR : 0.027\n",
      "Epoch : 273 ; Train Loss : 0.030273 ; Train Acc : 0.766 ; Test Loss : 0.042609 ; Test Acc : 0.688 ; LR : 0.027\n",
      "Epoch : 274 ; Train Loss : 0.030341 ; Train Acc : 0.770 ; Test Loss : 0.042774 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 275 ; Train Loss : 0.030346 ; Train Acc : 0.767 ; Test Loss : 0.042633 ; Test Acc : 0.688 ; LR : 0.027\n",
      "Epoch : 276 ; Train Loss : 0.030404 ; Train Acc : 0.772 ; Test Loss : 0.043005 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 277 ; Train Loss : 0.030358 ; Train Acc : 0.767 ; Test Loss : 0.042756 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 278 ; Train Loss : 0.030656 ; Train Acc : 0.763 ; Test Loss : 0.042907 ; Test Acc : 0.688 ; LR : 0.027\n",
      "Epoch : 279 ; Train Loss : 0.030276 ; Train Acc : 0.765 ; Test Loss : 0.042789 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 280 ; Train Loss : 0.030389 ; Train Acc : 0.766 ; Test Loss : 0.043187 ; Test Acc : 0.812 ; LR : 0.027\n",
      "Epoch : 281 ; Train Loss : 0.030539 ; Train Acc : 0.770 ; Test Loss : 0.043175 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 282 ; Train Loss : 0.030126 ; Train Acc : 0.765 ; Test Loss : 0.042810 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 283 ; Train Loss : 0.030385 ; Train Acc : 0.768 ; Test Loss : 0.043003 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 284 ; Train Loss : 0.030034 ; Train Acc : 0.764 ; Test Loss : 0.042824 ; Test Acc : 0.750 ; LR : 0.027\n",
      "Epoch : 285 ; Train Loss : 0.030283 ; Train Acc : 0.770 ; Test Loss : 0.043542 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 286 ; Train Loss : 0.030431 ; Train Acc : 0.765 ; Test Loss : 0.043024 ; Test Acc : 0.688 ; LR : 0.027\n",
      "Epoch : 287 ; Train Loss : 0.030567 ; Train Acc : 0.767 ; Test Loss : 0.042743 ; Test Acc : 0.688 ; LR : 0.027\n",
      "Epoch : 288 ; Train Loss : 0.030055 ; Train Acc : 0.765 ; Test Loss : 0.042712 ; Test Acc : 0.688 ; LR : 0.027\n",
      "Epoch : 289 ; Train Loss : 0.030115 ; Train Acc : 0.770 ; Test Loss : 0.042634 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 290 ; Train Loss : 0.030308 ; Train Acc : 0.766 ; Test Loss : 0.043190 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 291 ; Train Loss : 0.030343 ; Train Acc : 0.768 ; Test Loss : 0.042928 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 292 ; Train Loss : 0.030330 ; Train Acc : 0.763 ; Test Loss : 0.042796 ; Test Acc : 0.688 ; LR : 0.027\n",
      "Epoch : 293 ; Train Loss : 0.030014 ; Train Acc : 0.764 ; Test Loss : 0.043006 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 294 ; Train Loss : 0.030242 ; Train Acc : 0.771 ; Test Loss : 0.043006 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 295 ; Train Loss : 0.029921 ; Train Acc : 0.762 ; Test Loss : 0.043243 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 296 ; Train Loss : 0.030304 ; Train Acc : 0.767 ; Test Loss : 0.043085 ; Test Acc : 0.688 ; LR : 0.027\n",
      "Epoch : 297 ; Train Loss : 0.030266 ; Train Acc : 0.769 ; Test Loss : 0.043713 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 298 ; Train Loss : 0.030512 ; Train Acc : 0.767 ; Test Loss : 0.042873 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 299 ; Train Loss : 0.030580 ; Train Acc : 0.769 ; Test Loss : 0.043388 ; Test Acc : 0.688 ; LR : 0.027\n",
      "Epoch : 300 ; Train Loss : 0.030319 ; Train Acc : 0.762 ; Test Loss : 0.043386 ; Test Acc : 0.688 ; LR : 0.027\n",
      "Epoch : 301 ; Train Loss : 0.030147 ; Train Acc : 0.770 ; Test Loss : 0.042762 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 302 ; Train Loss : 0.030283 ; Train Acc : 0.769 ; Test Loss : 0.042846 ; Test Acc : 0.688 ; LR : 0.024\n",
      "Epoch : 303 ; Train Loss : 0.030284 ; Train Acc : 0.766 ; Test Loss : 0.043085 ; Test Acc : 0.688 ; LR : 0.024\n",
      "Epoch : 304 ; Train Loss : 0.030001 ; Train Acc : 0.768 ; Test Loss : 0.043141 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 305 ; Train Loss : 0.030401 ; Train Acc : 0.768 ; Test Loss : 0.042936 ; Test Acc : 0.688 ; LR : 0.024\n",
      "Epoch : 306 ; Train Loss : 0.029916 ; Train Acc : 0.768 ; Test Loss : 0.043050 ; Test Acc : 0.688 ; LR : 0.024\n",
      "Epoch : 307 ; Train Loss : 0.030025 ; Train Acc : 0.767 ; Test Loss : 0.043053 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 308 ; Train Loss : 0.030230 ; Train Acc : 0.758 ; Test Loss : 0.043113 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 309 ; Train Loss : 0.030045 ; Train Acc : 0.769 ; Test Loss : 0.043065 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 310 ; Train Loss : 0.029809 ; Train Acc : 0.767 ; Test Loss : 0.042927 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 311 ; Train Loss : 0.029984 ; Train Acc : 0.769 ; Test Loss : 0.043018 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 312 ; Train Loss : 0.030009 ; Train Acc : 0.767 ; Test Loss : 0.043206 ; Test Acc : 0.688 ; LR : 0.024\n",
      "Epoch : 313 ; Train Loss : 0.029986 ; Train Acc : 0.769 ; Test Loss : 0.043533 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 314 ; Train Loss : 0.030039 ; Train Acc : 0.767 ; Test Loss : 0.043538 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 315 ; Train Loss : 0.030175 ; Train Acc : 0.768 ; Test Loss : 0.043125 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 316 ; Train Loss : 0.030102 ; Train Acc : 0.768 ; Test Loss : 0.043040 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 317 ; Train Loss : 0.030241 ; Train Acc : 0.768 ; Test Loss : 0.043079 ; Test Acc : 0.688 ; LR : 0.024\n",
      "Epoch : 318 ; Train Loss : 0.030065 ; Train Acc : 0.767 ; Test Loss : 0.043373 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 319 ; Train Loss : 0.029797 ; Train Acc : 0.767 ; Test Loss : 0.043509 ; Test Acc : 0.750 ; LR : 0.024\n",
      "Epoch : 320 ; Train Loss : 0.030016 ; Train Acc : 0.773 ; Test Loss : 0.043704 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 321 ; Train Loss : 0.030242 ; Train Acc : 0.762 ; Test Loss : 0.043885 ; Test Acc : 0.688 ; LR : 0.024\n",
      "Epoch : 322 ; Train Loss : 0.030203 ; Train Acc : 0.773 ; Test Loss : 0.043827 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 323 ; Train Loss : 0.029781 ; Train Acc : 0.767 ; Test Loss : 0.043510 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 324 ; Train Loss : 0.030421 ; Train Acc : 0.766 ; Test Loss : 0.043310 ; Test Acc : 0.688 ; LR : 0.024\n",
      "Epoch : 325 ; Train Loss : 0.030279 ; Train Acc : 0.765 ; Test Loss : 0.043676 ; Test Acc : 0.688 ; LR : 0.024\n",
      "Epoch : 326 ; Train Loss : 0.029906 ; Train Acc : 0.766 ; Test Loss : 0.043401 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 327 ; Train Loss : 0.030254 ; Train Acc : 0.769 ; Test Loss : 0.043481 ; Test Acc : 0.688 ; LR : 0.024\n",
      "Epoch : 328 ; Train Loss : 0.029922 ; Train Acc : 0.767 ; Test Loss : 0.043376 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 329 ; Train Loss : 0.029553 ; Train Acc : 0.768 ; Test Loss : 0.043313 ; Test Acc : 0.688 ; LR : 0.024\n",
      "Epoch : 330 ; Train Loss : 0.029861 ; Train Acc : 0.766 ; Test Loss : 0.043193 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 331 ; Train Loss : 0.029890 ; Train Acc : 0.768 ; Test Loss : 0.043446 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 332 ; Train Loss : 0.029957 ; Train Acc : 0.765 ; Test Loss : 0.043686 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 333 ; Train Loss : 0.029709 ; Train Acc : 0.766 ; Test Loss : 0.043252 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 334 ; Train Loss : 0.029842 ; Train Acc : 0.772 ; Test Loss : 0.043513 ; Test Acc : 0.688 ; LR : 0.024\n",
      "Epoch : 335 ; Train Loss : 0.029746 ; Train Acc : 0.768 ; Test Loss : 0.043293 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 336 ; Train Loss : 0.030013 ; Train Acc : 0.769 ; Test Loss : 0.043382 ; Test Acc : 0.688 ; LR : 0.024\n",
      "Epoch : 337 ; Train Loss : 0.029612 ; Train Acc : 0.770 ; Test Loss : 0.043782 ; Test Acc : 0.562 ; LR : 0.024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 338 ; Train Loss : 0.030010 ; Train Acc : 0.769 ; Test Loss : 0.043758 ; Test Acc : 0.688 ; LR : 0.024\n",
      "Epoch : 339 ; Train Loss : 0.029892 ; Train Acc : 0.766 ; Test Loss : 0.043478 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 340 ; Train Loss : 0.029799 ; Train Acc : 0.767 ; Test Loss : 0.043564 ; Test Acc : 0.812 ; LR : 0.024\n",
      "Epoch : 341 ; Train Loss : 0.029805 ; Train Acc : 0.774 ; Test Loss : 0.043478 ; Test Acc : 0.688 ; LR : 0.024\n",
      "Epoch : 342 ; Train Loss : 0.029680 ; Train Acc : 0.777 ; Test Loss : 0.043495 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 343 ; Train Loss : 0.030091 ; Train Acc : 0.770 ; Test Loss : 0.043769 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 344 ; Train Loss : 0.030113 ; Train Acc : 0.763 ; Test Loss : 0.043484 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 345 ; Train Loss : 0.030002 ; Train Acc : 0.768 ; Test Loss : 0.043473 ; Test Acc : 0.688 ; LR : 0.024\n",
      "Epoch : 346 ; Train Loss : 0.029565 ; Train Acc : 0.769 ; Test Loss : 0.044244 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 347 ; Train Loss : 0.029836 ; Train Acc : 0.771 ; Test Loss : 0.043702 ; Test Acc : 0.688 ; LR : 0.024\n",
      "Epoch : 348 ; Train Loss : 0.029723 ; Train Acc : 0.774 ; Test Loss : 0.044129 ; Test Acc : 0.688 ; LR : 0.024\n",
      "Epoch : 349 ; Train Loss : 0.029958 ; Train Acc : 0.768 ; Test Loss : 0.043500 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 350 ; Train Loss : 0.029805 ; Train Acc : 0.770 ; Test Loss : 0.043711 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 351 ; Train Loss : 0.030015 ; Train Acc : 0.770 ; Test Loss : 0.043474 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 352 ; Train Loss : 0.029892 ; Train Acc : 0.766 ; Test Loss : 0.043932 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 353 ; Train Loss : 0.030056 ; Train Acc : 0.766 ; Test Loss : 0.043697 ; Test Acc : 0.688 ; LR : 0.022\n",
      "Epoch : 354 ; Train Loss : 0.029738 ; Train Acc : 0.767 ; Test Loss : 0.043865 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 355 ; Train Loss : 0.029756 ; Train Acc : 0.767 ; Test Loss : 0.043473 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 356 ; Train Loss : 0.029790 ; Train Acc : 0.767 ; Test Loss : 0.043444 ; Test Acc : 0.688 ; LR : 0.022\n",
      "Epoch : 357 ; Train Loss : 0.029542 ; Train Acc : 0.767 ; Test Loss : 0.043892 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 358 ; Train Loss : 0.029690 ; Train Acc : 0.767 ; Test Loss : 0.043752 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 359 ; Train Loss : 0.029484 ; Train Acc : 0.764 ; Test Loss : 0.043648 ; Test Acc : 0.688 ; LR : 0.022\n",
      "Epoch : 360 ; Train Loss : 0.029702 ; Train Acc : 0.769 ; Test Loss : 0.043639 ; Test Acc : 0.688 ; LR : 0.022\n",
      "Epoch : 361 ; Train Loss : 0.030296 ; Train Acc : 0.766 ; Test Loss : 0.044064 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 362 ; Train Loss : 0.029464 ; Train Acc : 0.771 ; Test Loss : 0.044010 ; Test Acc : 0.688 ; LR : 0.022\n",
      "Epoch : 363 ; Train Loss : 0.029848 ; Train Acc : 0.774 ; Test Loss : 0.043686 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 364 ; Train Loss : 0.029583 ; Train Acc : 0.770 ; Test Loss : 0.043741 ; Test Acc : 0.688 ; LR : 0.022\n",
      "Epoch : 365 ; Train Loss : 0.029825 ; Train Acc : 0.774 ; Test Loss : 0.044202 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 366 ; Train Loss : 0.029698 ; Train Acc : 0.768 ; Test Loss : 0.043958 ; Test Acc : 0.688 ; LR : 0.022\n",
      "Epoch : 367 ; Train Loss : 0.029579 ; Train Acc : 0.769 ; Test Loss : 0.043928 ; Test Acc : 0.688 ; LR : 0.022\n",
      "Epoch : 368 ; Train Loss : 0.029896 ; Train Acc : 0.770 ; Test Loss : 0.043943 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 369 ; Train Loss : 0.029854 ; Train Acc : 0.773 ; Test Loss : 0.044202 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 370 ; Train Loss : 0.030097 ; Train Acc : 0.768 ; Test Loss : 0.044455 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 371 ; Train Loss : 0.029548 ; Train Acc : 0.768 ; Test Loss : 0.043810 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 372 ; Train Loss : 0.029729 ; Train Acc : 0.768 ; Test Loss : 0.043957 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 373 ; Train Loss : 0.029800 ; Train Acc : 0.766 ; Test Loss : 0.043802 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 374 ; Train Loss : 0.029493 ; Train Acc : 0.771 ; Test Loss : 0.043975 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 375 ; Train Loss : 0.029447 ; Train Acc : 0.768 ; Test Loss : 0.044001 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 376 ; Train Loss : 0.029895 ; Train Acc : 0.774 ; Test Loss : 0.044283 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 377 ; Train Loss : 0.029795 ; Train Acc : 0.771 ; Test Loss : 0.044057 ; Test Acc : 0.688 ; LR : 0.022\n",
      "Epoch : 378 ; Train Loss : 0.029766 ; Train Acc : 0.771 ; Test Loss : 0.044430 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 379 ; Train Loss : 0.029935 ; Train Acc : 0.767 ; Test Loss : 0.043765 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 380 ; Train Loss : 0.029403 ; Train Acc : 0.770 ; Test Loss : 0.043655 ; Test Acc : 0.688 ; LR : 0.022\n",
      "Epoch : 381 ; Train Loss : 0.029533 ; Train Acc : 0.768 ; Test Loss : 0.043862 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 382 ; Train Loss : 0.029303 ; Train Acc : 0.769 ; Test Loss : 0.044009 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 383 ; Train Loss : 0.029694 ; Train Acc : 0.773 ; Test Loss : 0.043976 ; Test Acc : 0.688 ; LR : 0.022\n",
      "Epoch : 384 ; Train Loss : 0.029394 ; Train Acc : 0.768 ; Test Loss : 0.044103 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 385 ; Train Loss : 0.029548 ; Train Acc : 0.766 ; Test Loss : 0.043904 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 386 ; Train Loss : 0.029492 ; Train Acc : 0.774 ; Test Loss : 0.043995 ; Test Acc : 0.688 ; LR : 0.022\n",
      "Epoch : 387 ; Train Loss : 0.029272 ; Train Acc : 0.770 ; Test Loss : 0.044389 ; Test Acc : 0.688 ; LR : 0.022\n",
      "Epoch : 388 ; Train Loss : 0.029334 ; Train Acc : 0.770 ; Test Loss : 0.044003 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 389 ; Train Loss : 0.029553 ; Train Acc : 0.770 ; Test Loss : 0.044035 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 390 ; Train Loss : 0.029447 ; Train Acc : 0.771 ; Test Loss : 0.043967 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 391 ; Train Loss : 0.029296 ; Train Acc : 0.778 ; Test Loss : 0.044000 ; Test Acc : 0.688 ; LR : 0.022\n",
      "Epoch : 392 ; Train Loss : 0.029535 ; Train Acc : 0.772 ; Test Loss : 0.044146 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 393 ; Train Loss : 0.029678 ; Train Acc : 0.771 ; Test Loss : 0.044441 ; Test Acc : 0.688 ; LR : 0.022\n",
      "Epoch : 394 ; Train Loss : 0.029493 ; Train Acc : 0.770 ; Test Loss : 0.044043 ; Test Acc : 0.688 ; LR : 0.022\n",
      "Epoch : 395 ; Train Loss : 0.029816 ; Train Acc : 0.767 ; Test Loss : 0.044560 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 396 ; Train Loss : 0.029924 ; Train Acc : 0.760 ; Test Loss : 0.044254 ; Test Acc : 0.750 ; LR : 0.022\n",
      "Epoch : 397 ; Train Loss : 0.029556 ; Train Acc : 0.767 ; Test Loss : 0.044316 ; Test Acc : 0.688 ; LR : 0.022\n",
      "Epoch : 398 ; Train Loss : 0.029557 ; Train Acc : 0.768 ; Test Loss : 0.044241 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 399 ; Train Loss : 0.029662 ; Train Acc : 0.770 ; Test Loss : 0.044038 ; Test Acc : 0.625 ; LR : 0.022\n",
      "Epoch : 400 ; Train Loss : 0.029684 ; Train Acc : 0.771 ; Test Loss : 0.044869 ; Test Acc : 0.688 ; LR : 0.022\n",
      "Epoch : 401 ; Train Loss : 0.029798 ; Train Acc : 0.767 ; Test Loss : 0.044189 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 402 ; Train Loss : 0.029800 ; Train Acc : 0.768 ; Test Loss : 0.044148 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 403 ; Train Loss : 0.029516 ; Train Acc : 0.769 ; Test Loss : 0.044198 ; Test Acc : 0.688 ; LR : 0.019\n",
      "Epoch : 404 ; Train Loss : 0.029491 ; Train Acc : 0.770 ; Test Loss : 0.044147 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 405 ; Train Loss : 0.029452 ; Train Acc : 0.768 ; Test Loss : 0.044394 ; Test Acc : 0.688 ; LR : 0.019\n",
      "Epoch : 406 ; Train Loss : 0.029345 ; Train Acc : 0.772 ; Test Loss : 0.044384 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 407 ; Train Loss : 0.029442 ; Train Acc : 0.775 ; Test Loss : 0.044234 ; Test Acc : 0.688 ; LR : 0.019\n",
      "Epoch : 408 ; Train Loss : 0.029269 ; Train Acc : 0.769 ; Test Loss : 0.044090 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 409 ; Train Loss : 0.029083 ; Train Acc : 0.768 ; Test Loss : 0.044361 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 410 ; Train Loss : 0.029372 ; Train Acc : 0.773 ; Test Loss : 0.044293 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 411 ; Train Loss : 0.029486 ; Train Acc : 0.769 ; Test Loss : 0.044369 ; Test Acc : 0.625 ; LR : 0.019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 412 ; Train Loss : 0.029383 ; Train Acc : 0.770 ; Test Loss : 0.044325 ; Test Acc : 0.688 ; LR : 0.019\n",
      "Epoch : 413 ; Train Loss : 0.029302 ; Train Acc : 0.771 ; Test Loss : 0.044414 ; Test Acc : 0.688 ; LR : 0.019\n",
      "Epoch : 414 ; Train Loss : 0.029283 ; Train Acc : 0.770 ; Test Loss : 0.044696 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 415 ; Train Loss : 0.029498 ; Train Acc : 0.770 ; Test Loss : 0.044248 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 416 ; Train Loss : 0.029393 ; Train Acc : 0.772 ; Test Loss : 0.044224 ; Test Acc : 0.688 ; LR : 0.019\n",
      "Epoch : 417 ; Train Loss : 0.029429 ; Train Acc : 0.774 ; Test Loss : 0.044267 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 418 ; Train Loss : 0.029385 ; Train Acc : 0.769 ; Test Loss : 0.044344 ; Test Acc : 0.688 ; LR : 0.019\n",
      "Epoch : 419 ; Train Loss : 0.029103 ; Train Acc : 0.773 ; Test Loss : 0.044256 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 420 ; Train Loss : 0.029253 ; Train Acc : 0.777 ; Test Loss : 0.044482 ; Test Acc : 0.688 ; LR : 0.019\n",
      "Epoch : 421 ; Train Loss : 0.029386 ; Train Acc : 0.773 ; Test Loss : 0.044503 ; Test Acc : 0.688 ; LR : 0.019\n",
      "Epoch : 422 ; Train Loss : 0.029655 ; Train Acc : 0.766 ; Test Loss : 0.044291 ; Test Acc : 0.688 ; LR : 0.019\n",
      "Epoch : 423 ; Train Loss : 0.029476 ; Train Acc : 0.767 ; Test Loss : 0.044907 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 424 ; Train Loss : 0.029545 ; Train Acc : 0.770 ; Test Loss : 0.045446 ; Test Acc : 0.688 ; LR : 0.019\n",
      "Epoch : 425 ; Train Loss : 0.029305 ; Train Acc : 0.767 ; Test Loss : 0.044276 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 426 ; Train Loss : 0.029213 ; Train Acc : 0.772 ; Test Loss : 0.044362 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 427 ; Train Loss : 0.029393 ; Train Acc : 0.771 ; Test Loss : 0.044498 ; Test Acc : 0.688 ; LR : 0.019\n",
      "Epoch : 428 ; Train Loss : 0.029330 ; Train Acc : 0.770 ; Test Loss : 0.044365 ; Test Acc : 0.688 ; LR : 0.019\n",
      "Epoch : 429 ; Train Loss : 0.029190 ; Train Acc : 0.773 ; Test Loss : 0.044426 ; Test Acc : 0.688 ; LR : 0.019\n",
      "Epoch : 430 ; Train Loss : 0.029210 ; Train Acc : 0.771 ; Test Loss : 0.044480 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 431 ; Train Loss : 0.029328 ; Train Acc : 0.770 ; Test Loss : 0.044836 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 432 ; Train Loss : 0.029349 ; Train Acc : 0.770 ; Test Loss : 0.044433 ; Test Acc : 0.688 ; LR : 0.019\n",
      "Epoch : 433 ; Train Loss : 0.029136 ; Train Acc : 0.769 ; Test Loss : 0.044559 ; Test Acc : 0.688 ; LR : 0.019\n",
      "Epoch : 434 ; Train Loss : 0.029203 ; Train Acc : 0.772 ; Test Loss : 0.044411 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 435 ; Train Loss : 0.029189 ; Train Acc : 0.770 ; Test Loss : 0.044610 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 436 ; Train Loss : 0.029046 ; Train Acc : 0.769 ; Test Loss : 0.044698 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 437 ; Train Loss : 0.029029 ; Train Acc : 0.771 ; Test Loss : 0.044548 ; Test Acc : 0.688 ; LR : 0.019\n",
      "Epoch : 438 ; Train Loss : 0.029341 ; Train Acc : 0.769 ; Test Loss : 0.044672 ; Test Acc : 0.688 ; LR : 0.019\n",
      "Epoch : 439 ; Train Loss : 0.029171 ; Train Acc : 0.766 ; Test Loss : 0.044446 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 440 ; Train Loss : 0.029620 ; Train Acc : 0.765 ; Test Loss : 0.044440 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 441 ; Train Loss : 0.029856 ; Train Acc : 0.769 ; Test Loss : 0.045398 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 442 ; Train Loss : 0.029486 ; Train Acc : 0.769 ; Test Loss : 0.044790 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 443 ; Train Loss : 0.029480 ; Train Acc : 0.767 ; Test Loss : 0.045099 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 444 ; Train Loss : 0.029341 ; Train Acc : 0.771 ; Test Loss : 0.044581 ; Test Acc : 0.688 ; LR : 0.019\n",
      "Epoch : 445 ; Train Loss : 0.029473 ; Train Acc : 0.768 ; Test Loss : 0.044498 ; Test Acc : 0.688 ; LR : 0.019\n",
      "Epoch : 446 ; Train Loss : 0.029235 ; Train Acc : 0.769 ; Test Loss : 0.044582 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 447 ; Train Loss : 0.029217 ; Train Acc : 0.775 ; Test Loss : 0.044637 ; Test Acc : 0.688 ; LR : 0.019\n",
      "Epoch : 448 ; Train Loss : 0.028984 ; Train Acc : 0.769 ; Test Loss : 0.044694 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 449 ; Train Loss : 0.029461 ; Train Acc : 0.776 ; Test Loss : 0.044782 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 450 ; Train Loss : 0.029511 ; Train Acc : 0.771 ; Test Loss : 0.044872 ; Test Acc : 0.625 ; LR : 0.019\n",
      "Epoch : 451 ; Train Loss : 0.029193 ; Train Acc : 0.773 ; Test Loss : 0.044946 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 452 ; Train Loss : 0.029121 ; Train Acc : 0.774 ; Test Loss : 0.044682 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 453 ; Train Loss : 0.029044 ; Train Acc : 0.770 ; Test Loss : 0.044646 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 454 ; Train Loss : 0.029112 ; Train Acc : 0.771 ; Test Loss : 0.044450 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 455 ; Train Loss : 0.029111 ; Train Acc : 0.770 ; Test Loss : 0.045047 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 456 ; Train Loss : 0.029297 ; Train Acc : 0.774 ; Test Loss : 0.044643 ; Test Acc : 0.688 ; LR : 0.017\n",
      "Epoch : 457 ; Train Loss : 0.029308 ; Train Acc : 0.768 ; Test Loss : 0.044802 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 458 ; Train Loss : 0.029019 ; Train Acc : 0.768 ; Test Loss : 0.044571 ; Test Acc : 0.688 ; LR : 0.017\n",
      "Epoch : 459 ; Train Loss : 0.029246 ; Train Acc : 0.771 ; Test Loss : 0.045257 ; Test Acc : 0.688 ; LR : 0.017\n",
      "Epoch : 460 ; Train Loss : 0.029338 ; Train Acc : 0.769 ; Test Loss : 0.044855 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 461 ; Train Loss : 0.028795 ; Train Acc : 0.771 ; Test Loss : 0.044778 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 462 ; Train Loss : 0.029145 ; Train Acc : 0.763 ; Test Loss : 0.044917 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 463 ; Train Loss : 0.029675 ; Train Acc : 0.770 ; Test Loss : 0.044924 ; Test Acc : 0.688 ; LR : 0.017\n",
      "Epoch : 464 ; Train Loss : 0.029233 ; Train Acc : 0.769 ; Test Loss : 0.045091 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 465 ; Train Loss : 0.029563 ; Train Acc : 0.771 ; Test Loss : 0.044919 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 466 ; Train Loss : 0.029245 ; Train Acc : 0.773 ; Test Loss : 0.044830 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 467 ; Train Loss : 0.029009 ; Train Acc : 0.771 ; Test Loss : 0.044821 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 468 ; Train Loss : 0.029281 ; Train Acc : 0.768 ; Test Loss : 0.044799 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 469 ; Train Loss : 0.029062 ; Train Acc : 0.771 ; Test Loss : 0.045304 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 470 ; Train Loss : 0.028956 ; Train Acc : 0.769 ; Test Loss : 0.044585 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 471 ; Train Loss : 0.029161 ; Train Acc : 0.771 ; Test Loss : 0.044944 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 472 ; Train Loss : 0.029220 ; Train Acc : 0.769 ; Test Loss : 0.045089 ; Test Acc : 0.688 ; LR : 0.017\n",
      "Epoch : 473 ; Train Loss : 0.029183 ; Train Acc : 0.770 ; Test Loss : 0.044811 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 474 ; Train Loss : 0.029268 ; Train Acc : 0.769 ; Test Loss : 0.044987 ; Test Acc : 0.688 ; LR : 0.017\n",
      "Epoch : 475 ; Train Loss : 0.029544 ; Train Acc : 0.769 ; Test Loss : 0.045037 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 476 ; Train Loss : 0.029058 ; Train Acc : 0.772 ; Test Loss : 0.044677 ; Test Acc : 0.688 ; LR : 0.017\n",
      "Epoch : 477 ; Train Loss : 0.028815 ; Train Acc : 0.771 ; Test Loss : 0.044876 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 478 ; Train Loss : 0.029020 ; Train Acc : 0.771 ; Test Loss : 0.044825 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 479 ; Train Loss : 0.028984 ; Train Acc : 0.769 ; Test Loss : 0.044787 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 480 ; Train Loss : 0.029119 ; Train Acc : 0.772 ; Test Loss : 0.045133 ; Test Acc : 0.688 ; LR : 0.017\n",
      "Epoch : 481 ; Train Loss : 0.029245 ; Train Acc : 0.769 ; Test Loss : 0.045031 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 482 ; Train Loss : 0.029159 ; Train Acc : 0.769 ; Test Loss : 0.045084 ; Test Acc : 0.688 ; LR : 0.017\n",
      "Epoch : 483 ; Train Loss : 0.029153 ; Train Acc : 0.773 ; Test Loss : 0.045004 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 484 ; Train Loss : 0.029286 ; Train Acc : 0.770 ; Test Loss : 0.045107 ; Test Acc : 0.688 ; LR : 0.017\n",
      "Epoch : 485 ; Train Loss : 0.029153 ; Train Acc : 0.769 ; Test Loss : 0.045377 ; Test Acc : 0.625 ; LR : 0.017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 486 ; Train Loss : 0.029300 ; Train Acc : 0.774 ; Test Loss : 0.045451 ; Test Acc : 0.688 ; LR : 0.017\n",
      "Epoch : 487 ; Train Loss : 0.029190 ; Train Acc : 0.771 ; Test Loss : 0.044748 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 488 ; Train Loss : 0.029189 ; Train Acc : 0.771 ; Test Loss : 0.044870 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 489 ; Train Loss : 0.029135 ; Train Acc : 0.771 ; Test Loss : 0.045440 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 490 ; Train Loss : 0.029150 ; Train Acc : 0.774 ; Test Loss : 0.045332 ; Test Acc : 0.688 ; LR : 0.017\n",
      "Epoch : 491 ; Train Loss : 0.029069 ; Train Acc : 0.773 ; Test Loss : 0.045182 ; Test Acc : 0.688 ; LR : 0.017\n",
      "Epoch : 492 ; Train Loss : 0.029170 ; Train Acc : 0.773 ; Test Loss : 0.045175 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 493 ; Train Loss : 0.028980 ; Train Acc : 0.769 ; Test Loss : 0.044977 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 494 ; Train Loss : 0.029125 ; Train Acc : 0.768 ; Test Loss : 0.045042 ; Test Acc : 0.688 ; LR : 0.017\n",
      "Epoch : 495 ; Train Loss : 0.029362 ; Train Acc : 0.770 ; Test Loss : 0.044993 ; Test Acc : 0.688 ; LR : 0.017\n",
      "Epoch : 496 ; Train Loss : 0.029084 ; Train Acc : 0.775 ; Test Loss : 0.045107 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 497 ; Train Loss : 0.028882 ; Train Acc : 0.769 ; Test Loss : 0.045113 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 498 ; Train Loss : 0.029156 ; Train Acc : 0.768 ; Test Loss : 0.045292 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 499 ; Train Loss : 0.029015 ; Train Acc : 0.773 ; Test Loss : 0.045388 ; Test Acc : 0.625 ; LR : 0.017\n",
      "Epoch : 500 ; Train Loss : 0.028990 ; Train Acc : 0.771 ; Test Loss : 0.044960 ; Test Acc : 0.688 ; LR : 0.017\n",
      "Epoch : 501 ; Train Loss : 0.029257 ; Train Acc : 0.769 ; Test Loss : 0.045238 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 502 ; Train Loss : 0.029095 ; Train Acc : 0.771 ; Test Loss : 0.044995 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 503 ; Train Loss : 0.028896 ; Train Acc : 0.774 ; Test Loss : 0.045030 ; Test Acc : 0.750 ; LR : 0.016\n",
      "Epoch : 504 ; Train Loss : 0.029015 ; Train Acc : 0.777 ; Test Loss : 0.045228 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 505 ; Train Loss : 0.029164 ; Train Acc : 0.776 ; Test Loss : 0.045335 ; Test Acc : 0.688 ; LR : 0.016\n",
      "Epoch : 506 ; Train Loss : 0.029178 ; Train Acc : 0.778 ; Test Loss : 0.045687 ; Test Acc : 0.688 ; LR : 0.016\n",
      "Epoch : 507 ; Train Loss : 0.029133 ; Train Acc : 0.777 ; Test Loss : 0.044993 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 508 ; Train Loss : 0.028598 ; Train Acc : 0.776 ; Test Loss : 0.045422 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 509 ; Train Loss : 0.028683 ; Train Acc : 0.773 ; Test Loss : 0.045466 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 510 ; Train Loss : 0.028969 ; Train Acc : 0.772 ; Test Loss : 0.045057 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 511 ; Train Loss : 0.028942 ; Train Acc : 0.773 ; Test Loss : 0.046123 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 512 ; Train Loss : 0.029254 ; Train Acc : 0.773 ; Test Loss : 0.045458 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 513 ; Train Loss : 0.029065 ; Train Acc : 0.772 ; Test Loss : 0.045361 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 514 ; Train Loss : 0.029067 ; Train Acc : 0.767 ; Test Loss : 0.045365 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 515 ; Train Loss : 0.029290 ; Train Acc : 0.768 ; Test Loss : 0.045069 ; Test Acc : 0.688 ; LR : 0.016\n",
      "Epoch : 516 ; Train Loss : 0.028984 ; Train Acc : 0.777 ; Test Loss : 0.045601 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 517 ; Train Loss : 0.029169 ; Train Acc : 0.772 ; Test Loss : 0.045030 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 518 ; Train Loss : 0.029053 ; Train Acc : 0.772 ; Test Loss : 0.045220 ; Test Acc : 0.688 ; LR : 0.016\n",
      "Epoch : 519 ; Train Loss : 0.029039 ; Train Acc : 0.775 ; Test Loss : 0.045395 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 520 ; Train Loss : 0.029375 ; Train Acc : 0.781 ; Test Loss : 0.045472 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 521 ; Train Loss : 0.029065 ; Train Acc : 0.771 ; Test Loss : 0.045294 ; Test Acc : 0.688 ; LR : 0.016\n",
      "Epoch : 522 ; Train Loss : 0.028985 ; Train Acc : 0.769 ; Test Loss : 0.045597 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 523 ; Train Loss : 0.029043 ; Train Acc : 0.776 ; Test Loss : 0.045816 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 524 ; Train Loss : 0.029051 ; Train Acc : 0.770 ; Test Loss : 0.045213 ; Test Acc : 0.688 ; LR : 0.016\n",
      "Epoch : 525 ; Train Loss : 0.028649 ; Train Acc : 0.772 ; Test Loss : 0.045450 ; Test Acc : 0.688 ; LR : 0.016\n",
      "Epoch : 526 ; Train Loss : 0.029014 ; Train Acc : 0.764 ; Test Loss : 0.045002 ; Test Acc : 0.688 ; LR : 0.016\n",
      "Epoch : 527 ; Train Loss : 0.028918 ; Train Acc : 0.767 ; Test Loss : 0.045321 ; Test Acc : 0.688 ; LR : 0.016\n",
      "Epoch : 528 ; Train Loss : 0.029150 ; Train Acc : 0.770 ; Test Loss : 0.045552 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 529 ; Train Loss : 0.029076 ; Train Acc : 0.775 ; Test Loss : 0.045307 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 530 ; Train Loss : 0.028816 ; Train Acc : 0.775 ; Test Loss : 0.045202 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 531 ; Train Loss : 0.028878 ; Train Acc : 0.770 ; Test Loss : 0.045337 ; Test Acc : 0.688 ; LR : 0.016\n",
      "Epoch : 532 ; Train Loss : 0.029083 ; Train Acc : 0.774 ; Test Loss : 0.045196 ; Test Acc : 0.688 ; LR : 0.016\n",
      "Epoch : 533 ; Train Loss : 0.029038 ; Train Acc : 0.774 ; Test Loss : 0.045406 ; Test Acc : 0.688 ; LR : 0.016\n",
      "Epoch : 534 ; Train Loss : 0.028816 ; Train Acc : 0.778 ; Test Loss : 0.045641 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 535 ; Train Loss : 0.029100 ; Train Acc : 0.770 ; Test Loss : 0.045471 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 536 ; Train Loss : 0.029021 ; Train Acc : 0.771 ; Test Loss : 0.045291 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 537 ; Train Loss : 0.029319 ; Train Acc : 0.770 ; Test Loss : 0.045692 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 538 ; Train Loss : 0.028760 ; Train Acc : 0.773 ; Test Loss : 0.045999 ; Test Acc : 0.750 ; LR : 0.016\n",
      "Epoch : 539 ; Train Loss : 0.029310 ; Train Acc : 0.772 ; Test Loss : 0.045269 ; Test Acc : 0.688 ; LR : 0.016\n",
      "Epoch : 540 ; Train Loss : 0.029223 ; Train Acc : 0.771 ; Test Loss : 0.045411 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 541 ; Train Loss : 0.029074 ; Train Acc : 0.773 ; Test Loss : 0.045677 ; Test Acc : 0.688 ; LR : 0.016\n",
      "Epoch : 542 ; Train Loss : 0.029478 ; Train Acc : 0.765 ; Test Loss : 0.045242 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 543 ; Train Loss : 0.029019 ; Train Acc : 0.772 ; Test Loss : 0.045415 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 544 ; Train Loss : 0.029059 ; Train Acc : 0.773 ; Test Loss : 0.045781 ; Test Acc : 0.688 ; LR : 0.016\n",
      "Epoch : 545 ; Train Loss : 0.028946 ; Train Acc : 0.772 ; Test Loss : 0.045493 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 546 ; Train Loss : 0.028794 ; Train Acc : 0.774 ; Test Loss : 0.045279 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 547 ; Train Loss : 0.028693 ; Train Acc : 0.773 ; Test Loss : 0.045304 ; Test Acc : 0.688 ; LR : 0.016\n",
      "Epoch : 548 ; Train Loss : 0.028749 ; Train Acc : 0.772 ; Test Loss : 0.045619 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 549 ; Train Loss : 0.029159 ; Train Acc : 0.770 ; Test Loss : 0.045557 ; Test Acc : 0.750 ; LR : 0.016\n",
      "Epoch : 550 ; Train Loss : 0.029027 ; Train Acc : 0.771 ; Test Loss : 0.045864 ; Test Acc : 0.688 ; LR : 0.016\n",
      "Epoch : 551 ; Train Loss : 0.028585 ; Train Acc : 0.776 ; Test Loss : 0.045244 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 552 ; Train Loss : 0.028826 ; Train Acc : 0.771 ; Test Loss : 0.045288 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 553 ; Train Loss : 0.028928 ; Train Acc : 0.772 ; Test Loss : 0.045373 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 554 ; Train Loss : 0.028878 ; Train Acc : 0.778 ; Test Loss : 0.045828 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 555 ; Train Loss : 0.029235 ; Train Acc : 0.770 ; Test Loss : 0.045848 ; Test Acc : 0.688 ; LR : 0.014\n",
      "Epoch : 556 ; Train Loss : 0.029050 ; Train Acc : 0.772 ; Test Loss : 0.045591 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 557 ; Train Loss : 0.028545 ; Train Acc : 0.770 ; Test Loss : 0.045425 ; Test Acc : 0.688 ; LR : 0.014\n",
      "Epoch : 558 ; Train Loss : 0.028787 ; Train Acc : 0.770 ; Test Loss : 0.045795 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 559 ; Train Loss : 0.028753 ; Train Acc : 0.772 ; Test Loss : 0.045291 ; Test Acc : 0.688 ; LR : 0.014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 560 ; Train Loss : 0.028713 ; Train Acc : 0.772 ; Test Loss : 0.045370 ; Test Acc : 0.688 ; LR : 0.014\n",
      "Epoch : 561 ; Train Loss : 0.029131 ; Train Acc : 0.767 ; Test Loss : 0.045641 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 562 ; Train Loss : 0.028822 ; Train Acc : 0.773 ; Test Loss : 0.045815 ; Test Acc : 0.688 ; LR : 0.014\n",
      "Epoch : 563 ; Train Loss : 0.028835 ; Train Acc : 0.773 ; Test Loss : 0.045694 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 564 ; Train Loss : 0.029127 ; Train Acc : 0.773 ; Test Loss : 0.045791 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 565 ; Train Loss : 0.028865 ; Train Acc : 0.773 ; Test Loss : 0.045483 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 566 ; Train Loss : 0.028746 ; Train Acc : 0.780 ; Test Loss : 0.045439 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 567 ; Train Loss : 0.028660 ; Train Acc : 0.773 ; Test Loss : 0.045977 ; Test Acc : 0.688 ; LR : 0.014\n",
      "Epoch : 568 ; Train Loss : 0.028679 ; Train Acc : 0.772 ; Test Loss : 0.045828 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 569 ; Train Loss : 0.028843 ; Train Acc : 0.770 ; Test Loss : 0.045476 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 570 ; Train Loss : 0.028684 ; Train Acc : 0.773 ; Test Loss : 0.045674 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 571 ; Train Loss : 0.028928 ; Train Acc : 0.771 ; Test Loss : 0.046420 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 572 ; Train Loss : 0.029069 ; Train Acc : 0.770 ; Test Loss : 0.045875 ; Test Acc : 0.688 ; LR : 0.014\n",
      "Epoch : 573 ; Train Loss : 0.028878 ; Train Acc : 0.771 ; Test Loss : 0.046363 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 574 ; Train Loss : 0.028920 ; Train Acc : 0.773 ; Test Loss : 0.045590 ; Test Acc : 0.688 ; LR : 0.014\n",
      "Epoch : 575 ; Train Loss : 0.029113 ; Train Acc : 0.771 ; Test Loss : 0.045779 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 576 ; Train Loss : 0.029145 ; Train Acc : 0.771 ; Test Loss : 0.045496 ; Test Acc : 0.688 ; LR : 0.014\n",
      "Epoch : 577 ; Train Loss : 0.029209 ; Train Acc : 0.774 ; Test Loss : 0.045881 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 578 ; Train Loss : 0.028969 ; Train Acc : 0.774 ; Test Loss : 0.045550 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 579 ; Train Loss : 0.028845 ; Train Acc : 0.770 ; Test Loss : 0.045622 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 580 ; Train Loss : 0.028821 ; Train Acc : 0.771 ; Test Loss : 0.045752 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 581 ; Train Loss : 0.028887 ; Train Acc : 0.773 ; Test Loss : 0.045513 ; Test Acc : 0.688 ; LR : 0.014\n",
      "Epoch : 582 ; Train Loss : 0.028926 ; Train Acc : 0.774 ; Test Loss : 0.046598 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 583 ; Train Loss : 0.029149 ; Train Acc : 0.771 ; Test Loss : 0.045483 ; Test Acc : 0.688 ; LR : 0.014\n",
      "Epoch : 584 ; Train Loss : 0.028835 ; Train Acc : 0.769 ; Test Loss : 0.046269 ; Test Acc : 0.688 ; LR : 0.014\n",
      "Epoch : 585 ; Train Loss : 0.028800 ; Train Acc : 0.773 ; Test Loss : 0.045935 ; Test Acc : 0.688 ; LR : 0.014\n",
      "Epoch : 586 ; Train Loss : 0.029263 ; Train Acc : 0.773 ; Test Loss : 0.045730 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 587 ; Train Loss : 0.028558 ; Train Acc : 0.772 ; Test Loss : 0.046065 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 588 ; Train Loss : 0.029016 ; Train Acc : 0.771 ; Test Loss : 0.045925 ; Test Acc : 0.688 ; LR : 0.014\n",
      "Epoch : 589 ; Train Loss : 0.028790 ; Train Acc : 0.775 ; Test Loss : 0.045821 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 590 ; Train Loss : 0.028618 ; Train Acc : 0.774 ; Test Loss : 0.045748 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 591 ; Train Loss : 0.028794 ; Train Acc : 0.776 ; Test Loss : 0.046387 ; Test Acc : 0.688 ; LR : 0.014\n",
      "Epoch : 592 ; Train Loss : 0.028822 ; Train Acc : 0.772 ; Test Loss : 0.045722 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 593 ; Train Loss : 0.028763 ; Train Acc : 0.771 ; Test Loss : 0.046207 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 594 ; Train Loss : 0.028805 ; Train Acc : 0.778 ; Test Loss : 0.045759 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 595 ; Train Loss : 0.028889 ; Train Acc : 0.776 ; Test Loss : 0.046152 ; Test Acc : 0.688 ; LR : 0.014\n",
      "Epoch : 596 ; Train Loss : 0.028887 ; Train Acc : 0.774 ; Test Loss : 0.045866 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 597 ; Train Loss : 0.028752 ; Train Acc : 0.769 ; Test Loss : 0.045707 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 598 ; Train Loss : 0.028579 ; Train Acc : 0.774 ; Test Loss : 0.045871 ; Test Acc : 0.688 ; LR : 0.014\n",
      "Epoch : 599 ; Train Loss : 0.028665 ; Train Acc : 0.767 ; Test Loss : 0.046085 ; Test Acc : 0.750 ; LR : 0.014\n",
      "Epoch : 600 ; Train Loss : 0.028982 ; Train Acc : 0.769 ; Test Loss : 0.046042 ; Test Acc : 0.625 ; LR : 0.014\n",
      "\n",
      "Hidden Neurons : 40 ; Train Loss : 0.028654 ; Train Acc : 0.770 ; Test Loss : 0.045659 ; Test Acc : 0.625\n",
      "\n",
      "\n",
      "Epoch : 1 ; Train Loss : 0.238930 ; Train Acc : 0.135 ; Test Loss : 0.090353 ; Test Acc : 0.188 ; LR : 0.045\n",
      "Epoch : 2 ; Train Loss : 0.086796 ; Train Acc : 0.185 ; Test Loss : 0.085447 ; Test Acc : 0.188 ; LR : 0.045\n",
      "Epoch : 3 ; Train Loss : 0.083003 ; Train Acc : 0.218 ; Test Loss : 0.082489 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 4 ; Train Loss : 0.080461 ; Train Acc : 0.234 ; Test Loss : 0.079662 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 5 ; Train Loss : 0.076832 ; Train Acc : 0.284 ; Test Loss : 0.076771 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 6 ; Train Loss : 0.075001 ; Train Acc : 0.288 ; Test Loss : 0.075705 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 7 ; Train Loss : 0.074231 ; Train Acc : 0.296 ; Test Loss : 0.075281 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 8 ; Train Loss : 0.074153 ; Train Acc : 0.299 ; Test Loss : 0.075035 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 9 ; Train Loss : 0.073695 ; Train Acc : 0.290 ; Test Loss : 0.074845 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 10 ; Train Loss : 0.073432 ; Train Acc : 0.300 ; Test Loss : 0.074682 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 11 ; Train Loss : 0.073109 ; Train Acc : 0.307 ; Test Loss : 0.074325 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 12 ; Train Loss : 0.072845 ; Train Acc : 0.306 ; Test Loss : 0.073481 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 13 ; Train Loss : 0.071700 ; Train Acc : 0.335 ; Test Loss : 0.072165 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 14 ; Train Loss : 0.069812 ; Train Acc : 0.348 ; Test Loss : 0.070314 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 15 ; Train Loss : 0.068603 ; Train Acc : 0.359 ; Test Loss : 0.069325 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 16 ; Train Loss : 0.067806 ; Train Acc : 0.374 ; Test Loss : 0.068627 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 17 ; Train Loss : 0.067058 ; Train Acc : 0.378 ; Test Loss : 0.068438 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 18 ; Train Loss : 0.066935 ; Train Acc : 0.377 ; Test Loss : 0.068003 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 19 ; Train Loss : 0.066661 ; Train Acc : 0.379 ; Test Loss : 0.067831 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 20 ; Train Loss : 0.066332 ; Train Acc : 0.390 ; Test Loss : 0.067559 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 21 ; Train Loss : 0.066130 ; Train Acc : 0.406 ; Test Loss : 0.066700 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 22 ; Train Loss : 0.064003 ; Train Acc : 0.452 ; Test Loss : 0.063306 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 23 ; Train Loss : 0.060728 ; Train Acc : 0.478 ; Test Loss : 0.061678 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 24 ; Train Loss : 0.059581 ; Train Acc : 0.492 ; Test Loss : 0.061043 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 25 ; Train Loss : 0.059260 ; Train Acc : 0.488 ; Test Loss : 0.060659 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 26 ; Train Loss : 0.058602 ; Train Acc : 0.488 ; Test Loss : 0.060060 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 27 ; Train Loss : 0.058437 ; Train Acc : 0.491 ; Test Loss : 0.060039 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 28 ; Train Loss : 0.058327 ; Train Acc : 0.495 ; Test Loss : 0.059924 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 29 ; Train Loss : 0.058074 ; Train Acc : 0.492 ; Test Loss : 0.060112 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 30 ; Train Loss : 0.057717 ; Train Acc : 0.490 ; Test Loss : 0.059596 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 31 ; Train Loss : 0.057705 ; Train Acc : 0.494 ; Test Loss : 0.059624 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 32 ; Train Loss : 0.057671 ; Train Acc : 0.495 ; Test Loss : 0.059419 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 33 ; Train Loss : 0.057702 ; Train Acc : 0.492 ; Test Loss : 0.059445 ; Test Acc : 0.562 ; LR : 0.045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 34 ; Train Loss : 0.057380 ; Train Acc : 0.495 ; Test Loss : 0.059456 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 35 ; Train Loss : 0.057361 ; Train Acc : 0.496 ; Test Loss : 0.059293 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 36 ; Train Loss : 0.057363 ; Train Acc : 0.490 ; Test Loss : 0.059509 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 37 ; Train Loss : 0.057361 ; Train Acc : 0.491 ; Test Loss : 0.059344 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 38 ; Train Loss : 0.057044 ; Train Acc : 0.498 ; Test Loss : 0.059283 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 39 ; Train Loss : 0.056850 ; Train Acc : 0.495 ; Test Loss : 0.059210 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 40 ; Train Loss : 0.056982 ; Train Acc : 0.494 ; Test Loss : 0.059097 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 41 ; Train Loss : 0.057122 ; Train Acc : 0.494 ; Test Loss : 0.059295 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 42 ; Train Loss : 0.056919 ; Train Acc : 0.493 ; Test Loss : 0.059093 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 43 ; Train Loss : 0.057052 ; Train Acc : 0.485 ; Test Loss : 0.059127 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 44 ; Train Loss : 0.056772 ; Train Acc : 0.495 ; Test Loss : 0.059005 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 45 ; Train Loss : 0.056589 ; Train Acc : 0.495 ; Test Loss : 0.058985 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 46 ; Train Loss : 0.056665 ; Train Acc : 0.494 ; Test Loss : 0.059080 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 47 ; Train Loss : 0.056757 ; Train Acc : 0.492 ; Test Loss : 0.058946 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 48 ; Train Loss : 0.056633 ; Train Acc : 0.499 ; Test Loss : 0.059318 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 49 ; Train Loss : 0.057004 ; Train Acc : 0.495 ; Test Loss : 0.059438 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 50 ; Train Loss : 0.056846 ; Train Acc : 0.494 ; Test Loss : 0.059018 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 51 ; Train Loss : 0.056328 ; Train Acc : 0.492 ; Test Loss : 0.058869 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 52 ; Train Loss : 0.056261 ; Train Acc : 0.493 ; Test Loss : 0.058902 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 53 ; Train Loss : 0.056396 ; Train Acc : 0.494 ; Test Loss : 0.059190 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 54 ; Train Loss : 0.056384 ; Train Acc : 0.497 ; Test Loss : 0.058922 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 55 ; Train Loss : 0.056121 ; Train Acc : 0.492 ; Test Loss : 0.059025 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 56 ; Train Loss : 0.056167 ; Train Acc : 0.500 ; Test Loss : 0.059017 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 57 ; Train Loss : 0.056064 ; Train Acc : 0.494 ; Test Loss : 0.058961 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 58 ; Train Loss : 0.056202 ; Train Acc : 0.494 ; Test Loss : 0.058963 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 59 ; Train Loss : 0.056180 ; Train Acc : 0.495 ; Test Loss : 0.058955 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 60 ; Train Loss : 0.055757 ; Train Acc : 0.495 ; Test Loss : 0.058899 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 61 ; Train Loss : 0.056012 ; Train Acc : 0.495 ; Test Loss : 0.059015 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 62 ; Train Loss : 0.056131 ; Train Acc : 0.495 ; Test Loss : 0.058827 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 63 ; Train Loss : 0.056000 ; Train Acc : 0.496 ; Test Loss : 0.058897 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 64 ; Train Loss : 0.056051 ; Train Acc : 0.498 ; Test Loss : 0.058930 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 65 ; Train Loss : 0.055835 ; Train Acc : 0.494 ; Test Loss : 0.058888 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 66 ; Train Loss : 0.056028 ; Train Acc : 0.498 ; Test Loss : 0.058944 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 67 ; Train Loss : 0.056099 ; Train Acc : 0.497 ; Test Loss : 0.059210 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 68 ; Train Loss : 0.055963 ; Train Acc : 0.495 ; Test Loss : 0.058815 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 69 ; Train Loss : 0.055970 ; Train Acc : 0.486 ; Test Loss : 0.058897 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 70 ; Train Loss : 0.055723 ; Train Acc : 0.498 ; Test Loss : 0.059058 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 71 ; Train Loss : 0.056045 ; Train Acc : 0.496 ; Test Loss : 0.059037 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 72 ; Train Loss : 0.055844 ; Train Acc : 0.496 ; Test Loss : 0.058930 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 73 ; Train Loss : 0.055950 ; Train Acc : 0.499 ; Test Loss : 0.058950 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 74 ; Train Loss : 0.055643 ; Train Acc : 0.497 ; Test Loss : 0.059101 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 75 ; Train Loss : 0.055645 ; Train Acc : 0.496 ; Test Loss : 0.058925 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 76 ; Train Loss : 0.055768 ; Train Acc : 0.498 ; Test Loss : 0.058848 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 77 ; Train Loss : 0.055641 ; Train Acc : 0.498 ; Test Loss : 0.058839 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 78 ; Train Loss : 0.055820 ; Train Acc : 0.500 ; Test Loss : 0.058909 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 79 ; Train Loss : 0.055432 ; Train Acc : 0.487 ; Test Loss : 0.058861 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 80 ; Train Loss : 0.055545 ; Train Acc : 0.497 ; Test Loss : 0.059030 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 81 ; Train Loss : 0.055502 ; Train Acc : 0.498 ; Test Loss : 0.059085 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 82 ; Train Loss : 0.055608 ; Train Acc : 0.505 ; Test Loss : 0.058991 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 83 ; Train Loss : 0.055464 ; Train Acc : 0.499 ; Test Loss : 0.058870 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 84 ; Train Loss : 0.055480 ; Train Acc : 0.499 ; Test Loss : 0.058946 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 85 ; Train Loss : 0.055401 ; Train Acc : 0.493 ; Test Loss : 0.059004 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 86 ; Train Loss : 0.055260 ; Train Acc : 0.498 ; Test Loss : 0.059349 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 87 ; Train Loss : 0.055403 ; Train Acc : 0.501 ; Test Loss : 0.058848 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 88 ; Train Loss : 0.055548 ; Train Acc : 0.498 ; Test Loss : 0.059026 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 89 ; Train Loss : 0.055605 ; Train Acc : 0.499 ; Test Loss : 0.058705 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 90 ; Train Loss : 0.055195 ; Train Acc : 0.506 ; Test Loss : 0.058794 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 91 ; Train Loss : 0.055153 ; Train Acc : 0.503 ; Test Loss : 0.058427 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 92 ; Train Loss : 0.054577 ; Train Acc : 0.513 ; Test Loss : 0.057560 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 93 ; Train Loss : 0.053299 ; Train Acc : 0.534 ; Test Loss : 0.055842 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 94 ; Train Loss : 0.051503 ; Train Acc : 0.553 ; Test Loss : 0.054248 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 95 ; Train Loss : 0.050118 ; Train Acc : 0.580 ; Test Loss : 0.053440 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 96 ; Train Loss : 0.049522 ; Train Acc : 0.581 ; Test Loss : 0.053105 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 97 ; Train Loss : 0.048994 ; Train Acc : 0.584 ; Test Loss : 0.052840 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 98 ; Train Loss : 0.049069 ; Train Acc : 0.584 ; Test Loss : 0.052894 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 99 ; Train Loss : 0.048431 ; Train Acc : 0.586 ; Test Loss : 0.052655 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 100 ; Train Loss : 0.048993 ; Train Acc : 0.583 ; Test Loss : 0.052572 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 101 ; Train Loss : 0.048662 ; Train Acc : 0.583 ; Test Loss : 0.052727 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 102 ; Train Loss : 0.048424 ; Train Acc : 0.582 ; Test Loss : 0.052677 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 103 ; Train Loss : 0.048141 ; Train Acc : 0.583 ; Test Loss : 0.052568 ; Test Acc : 0.562 ; LR : 0.036\n",
      "Epoch : 104 ; Train Loss : 0.048289 ; Train Acc : 0.583 ; Test Loss : 0.052451 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 105 ; Train Loss : 0.048428 ; Train Acc : 0.584 ; Test Loss : 0.052374 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 106 ; Train Loss : 0.048028 ; Train Acc : 0.585 ; Test Loss : 0.052479 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 107 ; Train Loss : 0.048306 ; Train Acc : 0.583 ; Test Loss : 0.052573 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 108 ; Train Loss : 0.047952 ; Train Acc : 0.584 ; Test Loss : 0.052568 ; Test Acc : 0.688 ; LR : 0.036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 109 ; Train Loss : 0.047884 ; Train Acc : 0.582 ; Test Loss : 0.052492 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 110 ; Train Loss : 0.048149 ; Train Acc : 0.578 ; Test Loss : 0.052478 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 111 ; Train Loss : 0.047880 ; Train Acc : 0.584 ; Test Loss : 0.052303 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 112 ; Train Loss : 0.047633 ; Train Acc : 0.585 ; Test Loss : 0.052439 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 113 ; Train Loss : 0.047714 ; Train Acc : 0.583 ; Test Loss : 0.052595 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 114 ; Train Loss : 0.047902 ; Train Acc : 0.583 ; Test Loss : 0.052494 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 115 ; Train Loss : 0.048048 ; Train Acc : 0.582 ; Test Loss : 0.052331 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 116 ; Train Loss : 0.047909 ; Train Acc : 0.585 ; Test Loss : 0.052329 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 117 ; Train Loss : 0.048057 ; Train Acc : 0.584 ; Test Loss : 0.052351 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 118 ; Train Loss : 0.047685 ; Train Acc : 0.585 ; Test Loss : 0.052354 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 119 ; Train Loss : 0.047799 ; Train Acc : 0.583 ; Test Loss : 0.052417 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 120 ; Train Loss : 0.047786 ; Train Acc : 0.585 ; Test Loss : 0.052464 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 121 ; Train Loss : 0.047453 ; Train Acc : 0.586 ; Test Loss : 0.052347 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 122 ; Train Loss : 0.047970 ; Train Acc : 0.583 ; Test Loss : 0.052374 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 123 ; Train Loss : 0.047902 ; Train Acc : 0.585 ; Test Loss : 0.052549 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 124 ; Train Loss : 0.047807 ; Train Acc : 0.586 ; Test Loss : 0.052286 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 125 ; Train Loss : 0.047594 ; Train Acc : 0.586 ; Test Loss : 0.052419 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 126 ; Train Loss : 0.047552 ; Train Acc : 0.587 ; Test Loss : 0.052322 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 127 ; Train Loss : 0.047305 ; Train Acc : 0.586 ; Test Loss : 0.052387 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 128 ; Train Loss : 0.047468 ; Train Acc : 0.587 ; Test Loss : 0.052329 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 129 ; Train Loss : 0.047807 ; Train Acc : 0.585 ; Test Loss : 0.052442 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 130 ; Train Loss : 0.047413 ; Train Acc : 0.586 ; Test Loss : 0.052377 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 131 ; Train Loss : 0.047384 ; Train Acc : 0.585 ; Test Loss : 0.052410 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 132 ; Train Loss : 0.047362 ; Train Acc : 0.586 ; Test Loss : 0.052409 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 133 ; Train Loss : 0.047448 ; Train Acc : 0.579 ; Test Loss : 0.052502 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 134 ; Train Loss : 0.047492 ; Train Acc : 0.586 ; Test Loss : 0.052510 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 135 ; Train Loss : 0.047210 ; Train Acc : 0.586 ; Test Loss : 0.052509 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 136 ; Train Loss : 0.047421 ; Train Acc : 0.587 ; Test Loss : 0.052605 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 137 ; Train Loss : 0.047230 ; Train Acc : 0.585 ; Test Loss : 0.052500 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 138 ; Train Loss : 0.047119 ; Train Acc : 0.580 ; Test Loss : 0.052377 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 139 ; Train Loss : 0.047000 ; Train Acc : 0.586 ; Test Loss : 0.052466 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 140 ; Train Loss : 0.047292 ; Train Acc : 0.584 ; Test Loss : 0.052840 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 141 ; Train Loss : 0.047458 ; Train Acc : 0.587 ; Test Loss : 0.052657 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 142 ; Train Loss : 0.047270 ; Train Acc : 0.585 ; Test Loss : 0.052434 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 143 ; Train Loss : 0.047226 ; Train Acc : 0.588 ; Test Loss : 0.052467 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 144 ; Train Loss : 0.047209 ; Train Acc : 0.585 ; Test Loss : 0.052505 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 145 ; Train Loss : 0.046877 ; Train Acc : 0.587 ; Test Loss : 0.052487 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 146 ; Train Loss : 0.047137 ; Train Acc : 0.585 ; Test Loss : 0.052424 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 147 ; Train Loss : 0.047060 ; Train Acc : 0.586 ; Test Loss : 0.052380 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 148 ; Train Loss : 0.047099 ; Train Acc : 0.580 ; Test Loss : 0.052641 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 149 ; Train Loss : 0.046894 ; Train Acc : 0.586 ; Test Loss : 0.052729 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 150 ; Train Loss : 0.046938 ; Train Acc : 0.588 ; Test Loss : 0.052469 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 151 ; Train Loss : 0.046631 ; Train Acc : 0.588 ; Test Loss : 0.052510 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 152 ; Train Loss : 0.047248 ; Train Acc : 0.578 ; Test Loss : 0.052489 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 153 ; Train Loss : 0.046934 ; Train Acc : 0.587 ; Test Loss : 0.052447 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 154 ; Train Loss : 0.046998 ; Train Acc : 0.588 ; Test Loss : 0.052742 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 155 ; Train Loss : 0.046650 ; Train Acc : 0.586 ; Test Loss : 0.052451 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 156 ; Train Loss : 0.046770 ; Train Acc : 0.585 ; Test Loss : 0.052505 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 157 ; Train Loss : 0.047005 ; Train Acc : 0.588 ; Test Loss : 0.052729 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 158 ; Train Loss : 0.046640 ; Train Acc : 0.587 ; Test Loss : 0.052524 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 159 ; Train Loss : 0.046886 ; Train Acc : 0.587 ; Test Loss : 0.052610 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 160 ; Train Loss : 0.046984 ; Train Acc : 0.588 ; Test Loss : 0.052479 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 161 ; Train Loss : 0.046738 ; Train Acc : 0.588 ; Test Loss : 0.052640 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 162 ; Train Loss : 0.047038 ; Train Acc : 0.586 ; Test Loss : 0.052642 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 163 ; Train Loss : 0.046683 ; Train Acc : 0.589 ; Test Loss : 0.052491 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 164 ; Train Loss : 0.046489 ; Train Acc : 0.588 ; Test Loss : 0.052554 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 165 ; Train Loss : 0.046638 ; Train Acc : 0.587 ; Test Loss : 0.052672 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 166 ; Train Loss : 0.046717 ; Train Acc : 0.587 ; Test Loss : 0.052635 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 167 ; Train Loss : 0.046782 ; Train Acc : 0.580 ; Test Loss : 0.052645 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 168 ; Train Loss : 0.046814 ; Train Acc : 0.587 ; Test Loss : 0.052677 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 169 ; Train Loss : 0.046845 ; Train Acc : 0.588 ; Test Loss : 0.052515 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 170 ; Train Loss : 0.046524 ; Train Acc : 0.582 ; Test Loss : 0.052642 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 171 ; Train Loss : 0.046694 ; Train Acc : 0.590 ; Test Loss : 0.052745 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 172 ; Train Loss : 0.046909 ; Train Acc : 0.588 ; Test Loss : 0.052602 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 173 ; Train Loss : 0.046633 ; Train Acc : 0.581 ; Test Loss : 0.053081 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 174 ; Train Loss : 0.047103 ; Train Acc : 0.588 ; Test Loss : 0.052581 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 175 ; Train Loss : 0.046979 ; Train Acc : 0.588 ; Test Loss : 0.052619 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 176 ; Train Loss : 0.046719 ; Train Acc : 0.590 ; Test Loss : 0.052766 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 177 ; Train Loss : 0.046832 ; Train Acc : 0.590 ; Test Loss : 0.052743 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 178 ; Train Loss : 0.047068 ; Train Acc : 0.584 ; Test Loss : 0.053180 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 179 ; Train Loss : 0.046479 ; Train Acc : 0.587 ; Test Loss : 0.052638 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 180 ; Train Loss : 0.046344 ; Train Acc : 0.589 ; Test Loss : 0.052797 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 181 ; Train Loss : 0.046448 ; Train Acc : 0.588 ; Test Loss : 0.052692 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 182 ; Train Loss : 0.046456 ; Train Acc : 0.579 ; Test Loss : 0.052806 ; Test Acc : 0.562 ; LR : 0.033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 183 ; Train Loss : 0.046295 ; Train Acc : 0.590 ; Test Loss : 0.052668 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 184 ; Train Loss : 0.046513 ; Train Acc : 0.590 ; Test Loss : 0.052869 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 185 ; Train Loss : 0.046312 ; Train Acc : 0.590 ; Test Loss : 0.052914 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 186 ; Train Loss : 0.046402 ; Train Acc : 0.590 ; Test Loss : 0.052842 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 187 ; Train Loss : 0.046430 ; Train Acc : 0.588 ; Test Loss : 0.053201 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 188 ; Train Loss : 0.046518 ; Train Acc : 0.588 ; Test Loss : 0.052774 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 189 ; Train Loss : 0.046611 ; Train Acc : 0.581 ; Test Loss : 0.052757 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 190 ; Train Loss : 0.046342 ; Train Acc : 0.589 ; Test Loss : 0.052782 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 191 ; Train Loss : 0.046279 ; Train Acc : 0.589 ; Test Loss : 0.052777 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 192 ; Train Loss : 0.046138 ; Train Acc : 0.591 ; Test Loss : 0.052971 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 193 ; Train Loss : 0.046325 ; Train Acc : 0.589 ; Test Loss : 0.052801 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 194 ; Train Loss : 0.046329 ; Train Acc : 0.588 ; Test Loss : 0.053143 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 195 ; Train Loss : 0.046387 ; Train Acc : 0.579 ; Test Loss : 0.052916 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 196 ; Train Loss : 0.046398 ; Train Acc : 0.590 ; Test Loss : 0.052899 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 197 ; Train Loss : 0.046051 ; Train Acc : 0.591 ; Test Loss : 0.052832 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 198 ; Train Loss : 0.046311 ; Train Acc : 0.589 ; Test Loss : 0.052795 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 199 ; Train Loss : 0.046150 ; Train Acc : 0.590 ; Test Loss : 0.052789 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 200 ; Train Loss : 0.046171 ; Train Acc : 0.588 ; Test Loss : 0.052880 ; Test Acc : 0.688 ; LR : 0.033\n",
      "Epoch : 201 ; Train Loss : 0.046135 ; Train Acc : 0.590 ; Test Loss : 0.052973 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 202 ; Train Loss : 0.046146 ; Train Acc : 0.587 ; Test Loss : 0.052943 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 203 ; Train Loss : 0.046223 ; Train Acc : 0.586 ; Test Loss : 0.052969 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 204 ; Train Loss : 0.046046 ; Train Acc : 0.588 ; Test Loss : 0.052928 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 205 ; Train Loss : 0.046133 ; Train Acc : 0.581 ; Test Loss : 0.052877 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 206 ; Train Loss : 0.046151 ; Train Acc : 0.586 ; Test Loss : 0.052839 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 207 ; Train Loss : 0.045977 ; Train Acc : 0.593 ; Test Loss : 0.053005 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 208 ; Train Loss : 0.046258 ; Train Acc : 0.591 ; Test Loss : 0.053045 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 209 ; Train Loss : 0.045904 ; Train Acc : 0.590 ; Test Loss : 0.052981 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 210 ; Train Loss : 0.046293 ; Train Acc : 0.591 ; Test Loss : 0.052985 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 211 ; Train Loss : 0.046265 ; Train Acc : 0.590 ; Test Loss : 0.053041 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 212 ; Train Loss : 0.046188 ; Train Acc : 0.584 ; Test Loss : 0.052891 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 213 ; Train Loss : 0.046033 ; Train Acc : 0.590 ; Test Loss : 0.053221 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 214 ; Train Loss : 0.046269 ; Train Acc : 0.582 ; Test Loss : 0.053125 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 215 ; Train Loss : 0.046230 ; Train Acc : 0.590 ; Test Loss : 0.052891 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 216 ; Train Loss : 0.045947 ; Train Acc : 0.590 ; Test Loss : 0.053132 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 217 ; Train Loss : 0.046102 ; Train Acc : 0.590 ; Test Loss : 0.052957 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 218 ; Train Loss : 0.045748 ; Train Acc : 0.591 ; Test Loss : 0.053086 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 219 ; Train Loss : 0.046375 ; Train Acc : 0.588 ; Test Loss : 0.053041 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 220 ; Train Loss : 0.046312 ; Train Acc : 0.591 ; Test Loss : 0.053131 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 221 ; Train Loss : 0.045906 ; Train Acc : 0.590 ; Test Loss : 0.052957 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 222 ; Train Loss : 0.046049 ; Train Acc : 0.591 ; Test Loss : 0.053041 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 223 ; Train Loss : 0.046053 ; Train Acc : 0.591 ; Test Loss : 0.053020 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 224 ; Train Loss : 0.045858 ; Train Acc : 0.590 ; Test Loss : 0.053336 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 225 ; Train Loss : 0.046085 ; Train Acc : 0.585 ; Test Loss : 0.053043 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 226 ; Train Loss : 0.045843 ; Train Acc : 0.582 ; Test Loss : 0.053061 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 227 ; Train Loss : 0.045959 ; Train Acc : 0.593 ; Test Loss : 0.053247 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 228 ; Train Loss : 0.046155 ; Train Acc : 0.589 ; Test Loss : 0.053092 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 229 ; Train Loss : 0.045706 ; Train Acc : 0.593 ; Test Loss : 0.053033 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 230 ; Train Loss : 0.046042 ; Train Acc : 0.591 ; Test Loss : 0.053042 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 231 ; Train Loss : 0.045851 ; Train Acc : 0.592 ; Test Loss : 0.053294 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 232 ; Train Loss : 0.045706 ; Train Acc : 0.591 ; Test Loss : 0.053194 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 233 ; Train Loss : 0.046062 ; Train Acc : 0.589 ; Test Loss : 0.053333 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 234 ; Train Loss : 0.046160 ; Train Acc : 0.591 ; Test Loss : 0.053316 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 235 ; Train Loss : 0.045885 ; Train Acc : 0.585 ; Test Loss : 0.053284 ; Test Acc : 0.688 ; LR : 0.030\n",
      "Epoch : 236 ; Train Loss : 0.045806 ; Train Acc : 0.591 ; Test Loss : 0.053303 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 237 ; Train Loss : 0.045955 ; Train Acc : 0.592 ; Test Loss : 0.053180 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 238 ; Train Loss : 0.045922 ; Train Acc : 0.592 ; Test Loss : 0.053168 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 239 ; Train Loss : 0.045968 ; Train Acc : 0.592 ; Test Loss : 0.053094 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 240 ; Train Loss : 0.045959 ; Train Acc : 0.591 ; Test Loss : 0.053147 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 241 ; Train Loss : 0.045657 ; Train Acc : 0.591 ; Test Loss : 0.053176 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 242 ; Train Loss : 0.045665 ; Train Acc : 0.591 ; Test Loss : 0.053125 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 243 ; Train Loss : 0.045978 ; Train Acc : 0.591 ; Test Loss : 0.053191 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 244 ; Train Loss : 0.045716 ; Train Acc : 0.593 ; Test Loss : 0.053206 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 245 ; Train Loss : 0.045763 ; Train Acc : 0.593 ; Test Loss : 0.053169 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 246 ; Train Loss : 0.045687 ; Train Acc : 0.591 ; Test Loss : 0.053298 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 247 ; Train Loss : 0.045820 ; Train Acc : 0.592 ; Test Loss : 0.053277 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 248 ; Train Loss : 0.045730 ; Train Acc : 0.589 ; Test Loss : 0.053462 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 249 ; Train Loss : 0.045493 ; Train Acc : 0.592 ; Test Loss : 0.053577 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 250 ; Train Loss : 0.045557 ; Train Acc : 0.591 ; Test Loss : 0.053365 ; Test Acc : 0.625 ; LR : 0.030\n",
      "Epoch : 251 ; Train Loss : 0.045933 ; Train Acc : 0.592 ; Test Loss : 0.053211 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 252 ; Train Loss : 0.045359 ; Train Acc : 0.585 ; Test Loss : 0.053427 ; Test Acc : 0.688 ; LR : 0.027\n",
      "Epoch : 253 ; Train Loss : 0.045657 ; Train Acc : 0.593 ; Test Loss : 0.053420 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 254 ; Train Loss : 0.045613 ; Train Acc : 0.592 ; Test Loss : 0.053347 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 255 ; Train Loss : 0.045517 ; Train Acc : 0.592 ; Test Loss : 0.053355 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 256 ; Train Loss : 0.045468 ; Train Acc : 0.586 ; Test Loss : 0.053377 ; Test Acc : 0.562 ; LR : 0.027\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 257 ; Train Loss : 0.045533 ; Train Acc : 0.590 ; Test Loss : 0.053399 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 258 ; Train Loss : 0.045815 ; Train Acc : 0.591 ; Test Loss : 0.053733 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 259 ; Train Loss : 0.045736 ; Train Acc : 0.585 ; Test Loss : 0.053350 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 260 ; Train Loss : 0.045807 ; Train Acc : 0.591 ; Test Loss : 0.053339 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 261 ; Train Loss : 0.045520 ; Train Acc : 0.583 ; Test Loss : 0.053324 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 262 ; Train Loss : 0.045891 ; Train Acc : 0.590 ; Test Loss : 0.053429 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 263 ; Train Loss : 0.045574 ; Train Acc : 0.590 ; Test Loss : 0.053417 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 264 ; Train Loss : 0.045614 ; Train Acc : 0.593 ; Test Loss : 0.053392 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 265 ; Train Loss : 0.045455 ; Train Acc : 0.582 ; Test Loss : 0.053623 ; Test Acc : 0.688 ; LR : 0.027\n",
      "Epoch : 266 ; Train Loss : 0.045657 ; Train Acc : 0.589 ; Test Loss : 0.053434 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 267 ; Train Loss : 0.045344 ; Train Acc : 0.593 ; Test Loss : 0.053499 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 268 ; Train Loss : 0.045641 ; Train Acc : 0.591 ; Test Loss : 0.053622 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 269 ; Train Loss : 0.045707 ; Train Acc : 0.590 ; Test Loss : 0.053577 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 270 ; Train Loss : 0.045783 ; Train Acc : 0.593 ; Test Loss : 0.053454 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 271 ; Train Loss : 0.045614 ; Train Acc : 0.591 ; Test Loss : 0.053517 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 272 ; Train Loss : 0.045687 ; Train Acc : 0.590 ; Test Loss : 0.053535 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 273 ; Train Loss : 0.045507 ; Train Acc : 0.588 ; Test Loss : 0.053634 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 274 ; Train Loss : 0.045440 ; Train Acc : 0.591 ; Test Loss : 0.053442 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 275 ; Train Loss : 0.045713 ; Train Acc : 0.591 ; Test Loss : 0.053479 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 276 ; Train Loss : 0.045573 ; Train Acc : 0.592 ; Test Loss : 0.053651 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 277 ; Train Loss : 0.045690 ; Train Acc : 0.589 ; Test Loss : 0.053616 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 278 ; Train Loss : 0.045659 ; Train Acc : 0.591 ; Test Loss : 0.053488 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 279 ; Train Loss : 0.045415 ; Train Acc : 0.592 ; Test Loss : 0.053770 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 280 ; Train Loss : 0.045567 ; Train Acc : 0.590 ; Test Loss : 0.053467 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 281 ; Train Loss : 0.045643 ; Train Acc : 0.592 ; Test Loss : 0.053998 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 282 ; Train Loss : 0.045869 ; Train Acc : 0.592 ; Test Loss : 0.053594 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 283 ; Train Loss : 0.045231 ; Train Acc : 0.591 ; Test Loss : 0.053616 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 284 ; Train Loss : 0.045311 ; Train Acc : 0.591 ; Test Loss : 0.053725 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 285 ; Train Loss : 0.045367 ; Train Acc : 0.585 ; Test Loss : 0.053693 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 286 ; Train Loss : 0.045342 ; Train Acc : 0.592 ; Test Loss : 0.053591 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 287 ; Train Loss : 0.045470 ; Train Acc : 0.591 ; Test Loss : 0.053545 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 288 ; Train Loss : 0.045438 ; Train Acc : 0.593 ; Test Loss : 0.053639 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 289 ; Train Loss : 0.045542 ; Train Acc : 0.593 ; Test Loss : 0.053678 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 290 ; Train Loss : 0.045791 ; Train Acc : 0.591 ; Test Loss : 0.053544 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 291 ; Train Loss : 0.045248 ; Train Acc : 0.592 ; Test Loss : 0.053633 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 292 ; Train Loss : 0.045379 ; Train Acc : 0.593 ; Test Loss : 0.053692 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 293 ; Train Loss : 0.045667 ; Train Acc : 0.592 ; Test Loss : 0.053591 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 294 ; Train Loss : 0.045343 ; Train Acc : 0.593 ; Test Loss : 0.053765 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 295 ; Train Loss : 0.045468 ; Train Acc : 0.593 ; Test Loss : 0.053601 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 296 ; Train Loss : 0.045736 ; Train Acc : 0.580 ; Test Loss : 0.053848 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 297 ; Train Loss : 0.045357 ; Train Acc : 0.593 ; Test Loss : 0.053597 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 298 ; Train Loss : 0.045572 ; Train Acc : 0.583 ; Test Loss : 0.053858 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 299 ; Train Loss : 0.045418 ; Train Acc : 0.590 ; Test Loss : 0.053650 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 300 ; Train Loss : 0.045251 ; Train Acc : 0.589 ; Test Loss : 0.053845 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 301 ; Train Loss : 0.045098 ; Train Acc : 0.594 ; Test Loss : 0.053797 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 302 ; Train Loss : 0.045515 ; Train Acc : 0.593 ; Test Loss : 0.053745 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 303 ; Train Loss : 0.045430 ; Train Acc : 0.582 ; Test Loss : 0.053656 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 304 ; Train Loss : 0.045446 ; Train Acc : 0.594 ; Test Loss : 0.053814 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 305 ; Train Loss : 0.045766 ; Train Acc : 0.587 ; Test Loss : 0.053764 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 306 ; Train Loss : 0.045322 ; Train Acc : 0.591 ; Test Loss : 0.053825 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 307 ; Train Loss : 0.045419 ; Train Acc : 0.589 ; Test Loss : 0.053799 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 308 ; Train Loss : 0.045487 ; Train Acc : 0.592 ; Test Loss : 0.053796 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 309 ; Train Loss : 0.045479 ; Train Acc : 0.589 ; Test Loss : 0.054078 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 310 ; Train Loss : 0.045334 ; Train Acc : 0.584 ; Test Loss : 0.053686 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 311 ; Train Loss : 0.045567 ; Train Acc : 0.592 ; Test Loss : 0.053814 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 312 ; Train Loss : 0.045512 ; Train Acc : 0.594 ; Test Loss : 0.053697 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 313 ; Train Loss : 0.045066 ; Train Acc : 0.593 ; Test Loss : 0.053848 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 314 ; Train Loss : 0.045392 ; Train Acc : 0.593 ; Test Loss : 0.053785 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 315 ; Train Loss : 0.045121 ; Train Acc : 0.592 ; Test Loss : 0.053802 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 316 ; Train Loss : 0.045091 ; Train Acc : 0.593 ; Test Loss : 0.053930 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 317 ; Train Loss : 0.045481 ; Train Acc : 0.593 ; Test Loss : 0.053989 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 318 ; Train Loss : 0.045463 ; Train Acc : 0.590 ; Test Loss : 0.053952 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 319 ; Train Loss : 0.045508 ; Train Acc : 0.590 ; Test Loss : 0.053879 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 320 ; Train Loss : 0.045185 ; Train Acc : 0.596 ; Test Loss : 0.053952 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 321 ; Train Loss : 0.045187 ; Train Acc : 0.587 ; Test Loss : 0.053865 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 322 ; Train Loss : 0.045421 ; Train Acc : 0.591 ; Test Loss : 0.054014 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 323 ; Train Loss : 0.045147 ; Train Acc : 0.594 ; Test Loss : 0.053986 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 324 ; Train Loss : 0.045395 ; Train Acc : 0.587 ; Test Loss : 0.053923 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 325 ; Train Loss : 0.045383 ; Train Acc : 0.593 ; Test Loss : 0.053804 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 326 ; Train Loss : 0.045290 ; Train Acc : 0.592 ; Test Loss : 0.053879 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 327 ; Train Loss : 0.045230 ; Train Acc : 0.593 ; Test Loss : 0.053970 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 328 ; Train Loss : 0.045245 ; Train Acc : 0.594 ; Test Loss : 0.053947 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 329 ; Train Loss : 0.045187 ; Train Acc : 0.582 ; Test Loss : 0.053939 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 330 ; Train Loss : 0.045544 ; Train Acc : 0.594 ; Test Loss : 0.053882 ; Test Acc : 0.500 ; LR : 0.024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 331 ; Train Loss : 0.045333 ; Train Acc : 0.588 ; Test Loss : 0.054276 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 332 ; Train Loss : 0.045776 ; Train Acc : 0.590 ; Test Loss : 0.054007 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 333 ; Train Loss : 0.045212 ; Train Acc : 0.593 ; Test Loss : 0.054097 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 334 ; Train Loss : 0.045095 ; Train Acc : 0.592 ; Test Loss : 0.053901 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 335 ; Train Loss : 0.045193 ; Train Acc : 0.593 ; Test Loss : 0.053974 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 336 ; Train Loss : 0.044912 ; Train Acc : 0.592 ; Test Loss : 0.054010 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 337 ; Train Loss : 0.045138 ; Train Acc : 0.594 ; Test Loss : 0.053995 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 338 ; Train Loss : 0.045327 ; Train Acc : 0.589 ; Test Loss : 0.054002 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 339 ; Train Loss : 0.045311 ; Train Acc : 0.593 ; Test Loss : 0.054153 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 340 ; Train Loss : 0.045339 ; Train Acc : 0.592 ; Test Loss : 0.054222 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 341 ; Train Loss : 0.045080 ; Train Acc : 0.593 ; Test Loss : 0.053954 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 342 ; Train Loss : 0.045323 ; Train Acc : 0.592 ; Test Loss : 0.054049 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 343 ; Train Loss : 0.045359 ; Train Acc : 0.592 ; Test Loss : 0.054305 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 344 ; Train Loss : 0.045443 ; Train Acc : 0.592 ; Test Loss : 0.054132 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 345 ; Train Loss : 0.045311 ; Train Acc : 0.585 ; Test Loss : 0.054073 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 346 ; Train Loss : 0.045154 ; Train Acc : 0.593 ; Test Loss : 0.054154 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 347 ; Train Loss : 0.045259 ; Train Acc : 0.592 ; Test Loss : 0.054100 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 348 ; Train Loss : 0.045201 ; Train Acc : 0.582 ; Test Loss : 0.054226 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 349 ; Train Loss : 0.045148 ; Train Acc : 0.592 ; Test Loss : 0.054011 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 350 ; Train Loss : 0.045085 ; Train Acc : 0.593 ; Test Loss : 0.054110 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 351 ; Train Loss : 0.045148 ; Train Acc : 0.589 ; Test Loss : 0.054103 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 352 ; Train Loss : 0.045152 ; Train Acc : 0.594 ; Test Loss : 0.054145 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 353 ; Train Loss : 0.044982 ; Train Acc : 0.593 ; Test Loss : 0.054006 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 354 ; Train Loss : 0.045041 ; Train Acc : 0.591 ; Test Loss : 0.053973 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 355 ; Train Loss : 0.045184 ; Train Acc : 0.593 ; Test Loss : 0.054112 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 356 ; Train Loss : 0.045029 ; Train Acc : 0.590 ; Test Loss : 0.054079 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 357 ; Train Loss : 0.045084 ; Train Acc : 0.594 ; Test Loss : 0.053983 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 358 ; Train Loss : 0.045274 ; Train Acc : 0.595 ; Test Loss : 0.054020 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 359 ; Train Loss : 0.045049 ; Train Acc : 0.592 ; Test Loss : 0.054132 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 360 ; Train Loss : 0.044711 ; Train Acc : 0.591 ; Test Loss : 0.054066 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 361 ; Train Loss : 0.044939 ; Train Acc : 0.591 ; Test Loss : 0.054138 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 362 ; Train Loss : 0.045137 ; Train Acc : 0.593 ; Test Loss : 0.054043 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 363 ; Train Loss : 0.045309 ; Train Acc : 0.592 ; Test Loss : 0.054195 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 364 ; Train Loss : 0.044794 ; Train Acc : 0.596 ; Test Loss : 0.054397 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 365 ; Train Loss : 0.045171 ; Train Acc : 0.592 ; Test Loss : 0.054054 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 366 ; Train Loss : 0.045145 ; Train Acc : 0.593 ; Test Loss : 0.054013 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 367 ; Train Loss : 0.044820 ; Train Acc : 0.594 ; Test Loss : 0.054155 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 368 ; Train Loss : 0.045187 ; Train Acc : 0.588 ; Test Loss : 0.054148 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 369 ; Train Loss : 0.045129 ; Train Acc : 0.594 ; Test Loss : 0.054075 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 370 ; Train Loss : 0.045102 ; Train Acc : 0.594 ; Test Loss : 0.054198 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 371 ; Train Loss : 0.044996 ; Train Acc : 0.594 ; Test Loss : 0.054164 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 372 ; Train Loss : 0.045169 ; Train Acc : 0.592 ; Test Loss : 0.054185 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 373 ; Train Loss : 0.045017 ; Train Acc : 0.592 ; Test Loss : 0.054259 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 374 ; Train Loss : 0.045047 ; Train Acc : 0.591 ; Test Loss : 0.054146 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 375 ; Train Loss : 0.045028 ; Train Acc : 0.589 ; Test Loss : 0.054331 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 376 ; Train Loss : 0.045148 ; Train Acc : 0.593 ; Test Loss : 0.054200 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 377 ; Train Loss : 0.045474 ; Train Acc : 0.593 ; Test Loss : 0.054348 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 378 ; Train Loss : 0.045010 ; Train Acc : 0.590 ; Test Loss : 0.054215 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 379 ; Train Loss : 0.044930 ; Train Acc : 0.594 ; Test Loss : 0.054211 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 380 ; Train Loss : 0.045133 ; Train Acc : 0.594 ; Test Loss : 0.054328 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 381 ; Train Loss : 0.045024 ; Train Acc : 0.596 ; Test Loss : 0.054173 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 382 ; Train Loss : 0.045053 ; Train Acc : 0.593 ; Test Loss : 0.054234 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 383 ; Train Loss : 0.044898 ; Train Acc : 0.595 ; Test Loss : 0.054157 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 384 ; Train Loss : 0.044809 ; Train Acc : 0.594 ; Test Loss : 0.054293 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 385 ; Train Loss : 0.044820 ; Train Acc : 0.595 ; Test Loss : 0.054243 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 386 ; Train Loss : 0.045111 ; Train Acc : 0.593 ; Test Loss : 0.054551 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 387 ; Train Loss : 0.045157 ; Train Acc : 0.588 ; Test Loss : 0.054353 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 388 ; Train Loss : 0.044894 ; Train Acc : 0.594 ; Test Loss : 0.054434 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 389 ; Train Loss : 0.044989 ; Train Acc : 0.592 ; Test Loss : 0.054283 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 390 ; Train Loss : 0.044890 ; Train Acc : 0.589 ; Test Loss : 0.054413 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 391 ; Train Loss : 0.045229 ; Train Acc : 0.592 ; Test Loss : 0.054349 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 392 ; Train Loss : 0.044915 ; Train Acc : 0.589 ; Test Loss : 0.054373 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 393 ; Train Loss : 0.044834 ; Train Acc : 0.592 ; Test Loss : 0.054257 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 394 ; Train Loss : 0.044758 ; Train Acc : 0.594 ; Test Loss : 0.054441 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 395 ; Train Loss : 0.044846 ; Train Acc : 0.591 ; Test Loss : 0.054339 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 396 ; Train Loss : 0.044997 ; Train Acc : 0.592 ; Test Loss : 0.054784 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 397 ; Train Loss : 0.045262 ; Train Acc : 0.587 ; Test Loss : 0.054396 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 398 ; Train Loss : 0.044926 ; Train Acc : 0.594 ; Test Loss : 0.054647 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 399 ; Train Loss : 0.045002 ; Train Acc : 0.591 ; Test Loss : 0.054417 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 400 ; Train Loss : 0.044988 ; Train Acc : 0.593 ; Test Loss : 0.054227 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 401 ; Train Loss : 0.044840 ; Train Acc : 0.594 ; Test Loss : 0.054362 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 402 ; Train Loss : 0.044796 ; Train Acc : 0.593 ; Test Loss : 0.054383 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 403 ; Train Loss : 0.044960 ; Train Acc : 0.593 ; Test Loss : 0.054619 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 404 ; Train Loss : 0.044851 ; Train Acc : 0.594 ; Test Loss : 0.054341 ; Test Acc : 0.500 ; LR : 0.019\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 405 ; Train Loss : 0.044878 ; Train Acc : 0.587 ; Test Loss : 0.054380 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 406 ; Train Loss : 0.044713 ; Train Acc : 0.593 ; Test Loss : 0.054385 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 407 ; Train Loss : 0.044958 ; Train Acc : 0.593 ; Test Loss : 0.054418 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 408 ; Train Loss : 0.045023 ; Train Acc : 0.590 ; Test Loss : 0.054576 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 409 ; Train Loss : 0.044972 ; Train Acc : 0.595 ; Test Loss : 0.054396 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 410 ; Train Loss : 0.044752 ; Train Acc : 0.590 ; Test Loss : 0.054420 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 411 ; Train Loss : 0.044987 ; Train Acc : 0.592 ; Test Loss : 0.054293 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 412 ; Train Loss : 0.044539 ; Train Acc : 0.594 ; Test Loss : 0.054548 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 413 ; Train Loss : 0.044669 ; Train Acc : 0.592 ; Test Loss : 0.054369 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 414 ; Train Loss : 0.044787 ; Train Acc : 0.594 ; Test Loss : 0.054422 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 415 ; Train Loss : 0.044856 ; Train Acc : 0.596 ; Test Loss : 0.054585 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 416 ; Train Loss : 0.045009 ; Train Acc : 0.585 ; Test Loss : 0.054408 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 417 ; Train Loss : 0.044503 ; Train Acc : 0.594 ; Test Loss : 0.054506 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 418 ; Train Loss : 0.044817 ; Train Acc : 0.592 ; Test Loss : 0.054399 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 419 ; Train Loss : 0.045037 ; Train Acc : 0.593 ; Test Loss : 0.054422 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 420 ; Train Loss : 0.044812 ; Train Acc : 0.595 ; Test Loss : 0.054510 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 421 ; Train Loss : 0.044855 ; Train Acc : 0.589 ; Test Loss : 0.054488 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 422 ; Train Loss : 0.044739 ; Train Acc : 0.595 ; Test Loss : 0.054492 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 423 ; Train Loss : 0.044822 ; Train Acc : 0.593 ; Test Loss : 0.054452 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 424 ; Train Loss : 0.044597 ; Train Acc : 0.593 ; Test Loss : 0.054776 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 425 ; Train Loss : 0.044716 ; Train Acc : 0.594 ; Test Loss : 0.054505 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 426 ; Train Loss : 0.044727 ; Train Acc : 0.594 ; Test Loss : 0.054448 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 427 ; Train Loss : 0.044797 ; Train Acc : 0.585 ; Test Loss : 0.054973 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 428 ; Train Loss : 0.044884 ; Train Acc : 0.595 ; Test Loss : 0.054613 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 429 ; Train Loss : 0.044626 ; Train Acc : 0.594 ; Test Loss : 0.054689 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 430 ; Train Loss : 0.044787 ; Train Acc : 0.591 ; Test Loss : 0.054583 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 431 ; Train Loss : 0.044886 ; Train Acc : 0.593 ; Test Loss : 0.054629 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 432 ; Train Loss : 0.044990 ; Train Acc : 0.590 ; Test Loss : 0.054669 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 433 ; Train Loss : 0.044862 ; Train Acc : 0.591 ; Test Loss : 0.054742 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 434 ; Train Loss : 0.044809 ; Train Acc : 0.593 ; Test Loss : 0.054545 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 435 ; Train Loss : 0.044564 ; Train Acc : 0.592 ; Test Loss : 0.054785 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 436 ; Train Loss : 0.044666 ; Train Acc : 0.595 ; Test Loss : 0.054569 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 437 ; Train Loss : 0.044892 ; Train Acc : 0.595 ; Test Loss : 0.054520 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 438 ; Train Loss : 0.044666 ; Train Acc : 0.595 ; Test Loss : 0.054865 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 439 ; Train Loss : 0.044782 ; Train Acc : 0.588 ; Test Loss : 0.054597 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 440 ; Train Loss : 0.044805 ; Train Acc : 0.595 ; Test Loss : 0.054731 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 441 ; Train Loss : 0.044672 ; Train Acc : 0.595 ; Test Loss : 0.054645 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 442 ; Train Loss : 0.044983 ; Train Acc : 0.592 ; Test Loss : 0.054811 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 443 ; Train Loss : 0.044706 ; Train Acc : 0.593 ; Test Loss : 0.054688 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 444 ; Train Loss : 0.044469 ; Train Acc : 0.587 ; Test Loss : 0.055029 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 445 ; Train Loss : 0.044645 ; Train Acc : 0.590 ; Test Loss : 0.054672 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 446 ; Train Loss : 0.044783 ; Train Acc : 0.594 ; Test Loss : 0.054607 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 447 ; Train Loss : 0.044898 ; Train Acc : 0.591 ; Test Loss : 0.055026 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 448 ; Train Loss : 0.044758 ; Train Acc : 0.594 ; Test Loss : 0.054632 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 449 ; Train Loss : 0.045113 ; Train Acc : 0.590 ; Test Loss : 0.054699 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 450 ; Train Loss : 0.044938 ; Train Acc : 0.593 ; Test Loss : 0.054862 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 451 ; Train Loss : 0.044681 ; Train Acc : 0.593 ; Test Loss : 0.054686 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 452 ; Train Loss : 0.044756 ; Train Acc : 0.593 ; Test Loss : 0.054867 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 453 ; Train Loss : 0.044657 ; Train Acc : 0.593 ; Test Loss : 0.054704 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 454 ; Train Loss : 0.044989 ; Train Acc : 0.595 ; Test Loss : 0.054794 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 455 ; Train Loss : 0.044608 ; Train Acc : 0.584 ; Test Loss : 0.054826 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 456 ; Train Loss : 0.044887 ; Train Acc : 0.595 ; Test Loss : 0.054647 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 457 ; Train Loss : 0.045180 ; Train Acc : 0.588 ; Test Loss : 0.054804 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 458 ; Train Loss : 0.044773 ; Train Acc : 0.593 ; Test Loss : 0.054887 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 459 ; Train Loss : 0.045007 ; Train Acc : 0.593 ; Test Loss : 0.054595 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 460 ; Train Loss : 0.044594 ; Train Acc : 0.594 ; Test Loss : 0.054695 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 461 ; Train Loss : 0.044877 ; Train Acc : 0.588 ; Test Loss : 0.054930 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 462 ; Train Loss : 0.044697 ; Train Acc : 0.593 ; Test Loss : 0.054711 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 463 ; Train Loss : 0.044782 ; Train Acc : 0.594 ; Test Loss : 0.054771 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 464 ; Train Loss : 0.044640 ; Train Acc : 0.595 ; Test Loss : 0.054704 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 465 ; Train Loss : 0.044721 ; Train Acc : 0.592 ; Test Loss : 0.054768 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 466 ; Train Loss : 0.044695 ; Train Acc : 0.594 ; Test Loss : 0.054870 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 467 ; Train Loss : 0.044964 ; Train Acc : 0.590 ; Test Loss : 0.054726 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 468 ; Train Loss : 0.044496 ; Train Acc : 0.594 ; Test Loss : 0.054809 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 469 ; Train Loss : 0.044401 ; Train Acc : 0.593 ; Test Loss : 0.054828 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 470 ; Train Loss : 0.044781 ; Train Acc : 0.593 ; Test Loss : 0.054737 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 471 ; Train Loss : 0.044526 ; Train Acc : 0.594 ; Test Loss : 0.054766 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 472 ; Train Loss : 0.044444 ; Train Acc : 0.591 ; Test Loss : 0.054883 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 473 ; Train Loss : 0.044729 ; Train Acc : 0.594 ; Test Loss : 0.054784 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 474 ; Train Loss : 0.044654 ; Train Acc : 0.595 ; Test Loss : 0.054872 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 475 ; Train Loss : 0.044625 ; Train Acc : 0.595 ; Test Loss : 0.054810 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 476 ; Train Loss : 0.044679 ; Train Acc : 0.592 ; Test Loss : 0.054859 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 477 ; Train Loss : 0.044843 ; Train Acc : 0.595 ; Test Loss : 0.054798 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 478 ; Train Loss : 0.044766 ; Train Acc : 0.594 ; Test Loss : 0.055071 ; Test Acc : 0.562 ; LR : 0.017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 479 ; Train Loss : 0.044783 ; Train Acc : 0.593 ; Test Loss : 0.054920 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 480 ; Train Loss : 0.044778 ; Train Acc : 0.593 ; Test Loss : 0.055068 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 481 ; Train Loss : 0.044795 ; Train Acc : 0.593 ; Test Loss : 0.054926 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 482 ; Train Loss : 0.044576 ; Train Acc : 0.594 ; Test Loss : 0.054902 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 483 ; Train Loss : 0.044384 ; Train Acc : 0.594 ; Test Loss : 0.054774 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 484 ; Train Loss : 0.044456 ; Train Acc : 0.587 ; Test Loss : 0.054892 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 485 ; Train Loss : 0.044622 ; Train Acc : 0.595 ; Test Loss : 0.055005 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 486 ; Train Loss : 0.044603 ; Train Acc : 0.595 ; Test Loss : 0.054884 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 487 ; Train Loss : 0.044758 ; Train Acc : 0.587 ; Test Loss : 0.054880 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 488 ; Train Loss : 0.044736 ; Train Acc : 0.597 ; Test Loss : 0.054912 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 489 ; Train Loss : 0.044727 ; Train Acc : 0.593 ; Test Loss : 0.054993 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 490 ; Train Loss : 0.044589 ; Train Acc : 0.595 ; Test Loss : 0.054974 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 491 ; Train Loss : 0.044670 ; Train Acc : 0.591 ; Test Loss : 0.054946 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 492 ; Train Loss : 0.044913 ; Train Acc : 0.594 ; Test Loss : 0.054947 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 493 ; Train Loss : 0.044652 ; Train Acc : 0.593 ; Test Loss : 0.054909 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 494 ; Train Loss : 0.044618 ; Train Acc : 0.593 ; Test Loss : 0.055227 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 495 ; Train Loss : 0.044668 ; Train Acc : 0.592 ; Test Loss : 0.055077 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 496 ; Train Loss : 0.044571 ; Train Acc : 0.592 ; Test Loss : 0.054921 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 497 ; Train Loss : 0.044467 ; Train Acc : 0.593 ; Test Loss : 0.054961 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 498 ; Train Loss : 0.044516 ; Train Acc : 0.595 ; Test Loss : 0.054903 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 499 ; Train Loss : 0.044633 ; Train Acc : 0.595 ; Test Loss : 0.054879 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 500 ; Train Loss : 0.044809 ; Train Acc : 0.594 ; Test Loss : 0.054961 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 501 ; Train Loss : 0.044810 ; Train Acc : 0.592 ; Test Loss : 0.054970 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 502 ; Train Loss : 0.044544 ; Train Acc : 0.595 ; Test Loss : 0.055074 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 503 ; Train Loss : 0.044726 ; Train Acc : 0.592 ; Test Loss : 0.055067 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 504 ; Train Loss : 0.044864 ; Train Acc : 0.593 ; Test Loss : 0.054946 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 505 ; Train Loss : 0.044643 ; Train Acc : 0.594 ; Test Loss : 0.055135 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 506 ; Train Loss : 0.044808 ; Train Acc : 0.593 ; Test Loss : 0.054919 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 507 ; Train Loss : 0.044687 ; Train Acc : 0.595 ; Test Loss : 0.054997 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 508 ; Train Loss : 0.044467 ; Train Acc : 0.594 ; Test Loss : 0.054936 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 509 ; Train Loss : 0.044567 ; Train Acc : 0.595 ; Test Loss : 0.055179 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 510 ; Train Loss : 0.044528 ; Train Acc : 0.595 ; Test Loss : 0.054951 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 511 ; Train Loss : 0.044436 ; Train Acc : 0.593 ; Test Loss : 0.054977 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 512 ; Train Loss : 0.044453 ; Train Acc : 0.595 ; Test Loss : 0.054992 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 513 ; Train Loss : 0.044823 ; Train Acc : 0.594 ; Test Loss : 0.054931 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 514 ; Train Loss : 0.044733 ; Train Acc : 0.594 ; Test Loss : 0.055132 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 515 ; Train Loss : 0.044822 ; Train Acc : 0.594 ; Test Loss : 0.054960 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 516 ; Train Loss : 0.044498 ; Train Acc : 0.597 ; Test Loss : 0.055020 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 517 ; Train Loss : 0.044696 ; Train Acc : 0.592 ; Test Loss : 0.055013 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 518 ; Train Loss : 0.044348 ; Train Acc : 0.594 ; Test Loss : 0.055054 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 519 ; Train Loss : 0.044903 ; Train Acc : 0.594 ; Test Loss : 0.055111 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 520 ; Train Loss : 0.044509 ; Train Acc : 0.592 ; Test Loss : 0.055168 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 521 ; Train Loss : 0.044815 ; Train Acc : 0.593 ; Test Loss : 0.055127 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 522 ; Train Loss : 0.044349 ; Train Acc : 0.594 ; Test Loss : 0.055122 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 523 ; Train Loss : 0.044730 ; Train Acc : 0.595 ; Test Loss : 0.055189 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 524 ; Train Loss : 0.044558 ; Train Acc : 0.588 ; Test Loss : 0.055092 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 525 ; Train Loss : 0.044788 ; Train Acc : 0.596 ; Test Loss : 0.055055 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 526 ; Train Loss : 0.044431 ; Train Acc : 0.594 ; Test Loss : 0.055110 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 527 ; Train Loss : 0.044530 ; Train Acc : 0.593 ; Test Loss : 0.055015 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 528 ; Train Loss : 0.044922 ; Train Acc : 0.595 ; Test Loss : 0.055251 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 529 ; Train Loss : 0.044480 ; Train Acc : 0.594 ; Test Loss : 0.055057 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 530 ; Train Loss : 0.044686 ; Train Acc : 0.595 ; Test Loss : 0.055158 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 531 ; Train Loss : 0.044655 ; Train Acc : 0.594 ; Test Loss : 0.055185 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 532 ; Train Loss : 0.044707 ; Train Acc : 0.595 ; Test Loss : 0.055084 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 533 ; Train Loss : 0.044596 ; Train Acc : 0.587 ; Test Loss : 0.055170 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 534 ; Train Loss : 0.044679 ; Train Acc : 0.595 ; Test Loss : 0.055124 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 535 ; Train Loss : 0.044558 ; Train Acc : 0.593 ; Test Loss : 0.055485 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 536 ; Train Loss : 0.044546 ; Train Acc : 0.596 ; Test Loss : 0.055095 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 537 ; Train Loss : 0.044538 ; Train Acc : 0.593 ; Test Loss : 0.055140 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 538 ; Train Loss : 0.044367 ; Train Acc : 0.594 ; Test Loss : 0.055112 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 539 ; Train Loss : 0.044433 ; Train Acc : 0.594 ; Test Loss : 0.055257 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 540 ; Train Loss : 0.044215 ; Train Acc : 0.593 ; Test Loss : 0.055192 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 541 ; Train Loss : 0.044507 ; Train Acc : 0.594 ; Test Loss : 0.055156 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 542 ; Train Loss : 0.044436 ; Train Acc : 0.594 ; Test Loss : 0.055199 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 543 ; Train Loss : 0.044651 ; Train Acc : 0.593 ; Test Loss : 0.055176 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 544 ; Train Loss : 0.044586 ; Train Acc : 0.594 ; Test Loss : 0.055115 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 545 ; Train Loss : 0.044405 ; Train Acc : 0.594 ; Test Loss : 0.055160 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 546 ; Train Loss : 0.044399 ; Train Acc : 0.595 ; Test Loss : 0.055475 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 547 ; Train Loss : 0.044210 ; Train Acc : 0.594 ; Test Loss : 0.055168 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 548 ; Train Loss : 0.044476 ; Train Acc : 0.595 ; Test Loss : 0.055221 ; Test Acc : 0.562 ; LR : 0.016\n",
      "Epoch : 549 ; Train Loss : 0.044564 ; Train Acc : 0.595 ; Test Loss : 0.055225 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 550 ; Train Loss : 0.044540 ; Train Acc : 0.591 ; Test Loss : 0.055131 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 551 ; Train Loss : 0.044459 ; Train Acc : 0.595 ; Test Loss : 0.055263 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 552 ; Train Loss : 0.044741 ; Train Acc : 0.595 ; Test Loss : 0.055178 ; Test Acc : 0.562 ; LR : 0.014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 553 ; Train Loss : 0.044439 ; Train Acc : 0.592 ; Test Loss : 0.055124 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 554 ; Train Loss : 0.044261 ; Train Acc : 0.596 ; Test Loss : 0.055248 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 555 ; Train Loss : 0.044250 ; Train Acc : 0.594 ; Test Loss : 0.055275 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 556 ; Train Loss : 0.044304 ; Train Acc : 0.593 ; Test Loss : 0.055456 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 557 ; Train Loss : 0.044273 ; Train Acc : 0.595 ; Test Loss : 0.055114 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 558 ; Train Loss : 0.044652 ; Train Acc : 0.595 ; Test Loss : 0.055287 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 559 ; Train Loss : 0.044839 ; Train Acc : 0.594 ; Test Loss : 0.055154 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 560 ; Train Loss : 0.044434 ; Train Acc : 0.594 ; Test Loss : 0.055468 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 561 ; Train Loss : 0.044273 ; Train Acc : 0.594 ; Test Loss : 0.055450 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 562 ; Train Loss : 0.044823 ; Train Acc : 0.588 ; Test Loss : 0.055337 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 563 ; Train Loss : 0.044583 ; Train Acc : 0.594 ; Test Loss : 0.055262 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 564 ; Train Loss : 0.044445 ; Train Acc : 0.593 ; Test Loss : 0.055418 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 565 ; Train Loss : 0.044544 ; Train Acc : 0.595 ; Test Loss : 0.055259 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 566 ; Train Loss : 0.044443 ; Train Acc : 0.594 ; Test Loss : 0.055296 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 567 ; Train Loss : 0.044417 ; Train Acc : 0.594 ; Test Loss : 0.055187 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 568 ; Train Loss : 0.044563 ; Train Acc : 0.595 ; Test Loss : 0.055250 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 569 ; Train Loss : 0.044182 ; Train Acc : 0.594 ; Test Loss : 0.055251 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 570 ; Train Loss : 0.044557 ; Train Acc : 0.593 ; Test Loss : 0.055281 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 571 ; Train Loss : 0.044295 ; Train Acc : 0.595 ; Test Loss : 0.055251 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 572 ; Train Loss : 0.044513 ; Train Acc : 0.594 ; Test Loss : 0.055381 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 573 ; Train Loss : 0.044708 ; Train Acc : 0.594 ; Test Loss : 0.055298 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 574 ; Train Loss : 0.044321 ; Train Acc : 0.594 ; Test Loss : 0.055333 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 575 ; Train Loss : 0.044779 ; Train Acc : 0.595 ; Test Loss : 0.055301 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 576 ; Train Loss : 0.044478 ; Train Acc : 0.594 ; Test Loss : 0.055613 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 577 ; Train Loss : 0.044666 ; Train Acc : 0.595 ; Test Loss : 0.055252 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 578 ; Train Loss : 0.044523 ; Train Acc : 0.594 ; Test Loss : 0.055275 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 579 ; Train Loss : 0.044312 ; Train Acc : 0.593 ; Test Loss : 0.055335 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 580 ; Train Loss : 0.044481 ; Train Acc : 0.594 ; Test Loss : 0.055391 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 581 ; Train Loss : 0.044848 ; Train Acc : 0.593 ; Test Loss : 0.055469 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 582 ; Train Loss : 0.044200 ; Train Acc : 0.595 ; Test Loss : 0.055312 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 583 ; Train Loss : 0.044328 ; Train Acc : 0.594 ; Test Loss : 0.055278 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 584 ; Train Loss : 0.044596 ; Train Acc : 0.598 ; Test Loss : 0.055295 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 585 ; Train Loss : 0.044325 ; Train Acc : 0.593 ; Test Loss : 0.055408 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 586 ; Train Loss : 0.044366 ; Train Acc : 0.586 ; Test Loss : 0.055529 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 587 ; Train Loss : 0.044389 ; Train Acc : 0.594 ; Test Loss : 0.055288 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 588 ; Train Loss : 0.044166 ; Train Acc : 0.595 ; Test Loss : 0.055265 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 589 ; Train Loss : 0.044732 ; Train Acc : 0.595 ; Test Loss : 0.055411 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 590 ; Train Loss : 0.044177 ; Train Acc : 0.594 ; Test Loss : 0.055399 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 591 ; Train Loss : 0.044250 ; Train Acc : 0.594 ; Test Loss : 0.055551 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 592 ; Train Loss : 0.044367 ; Train Acc : 0.595 ; Test Loss : 0.055345 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 593 ; Train Loss : 0.044475 ; Train Acc : 0.594 ; Test Loss : 0.055338 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 594 ; Train Loss : 0.044352 ; Train Acc : 0.594 ; Test Loss : 0.055553 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 595 ; Train Loss : 0.044226 ; Train Acc : 0.595 ; Test Loss : 0.055327 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 596 ; Train Loss : 0.044613 ; Train Acc : 0.594 ; Test Loss : 0.055510 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 597 ; Train Loss : 0.044369 ; Train Acc : 0.594 ; Test Loss : 0.055525 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 598 ; Train Loss : 0.044051 ; Train Acc : 0.595 ; Test Loss : 0.055458 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 599 ; Train Loss : 0.044803 ; Train Acc : 0.595 ; Test Loss : 0.055521 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 600 ; Train Loss : 0.044437 ; Train Acc : 0.592 ; Test Loss : 0.055376 ; Test Acc : 0.562 ; LR : 0.014\n",
      "\n",
      "Hidden Neurons : 45 ; Train Loss : 0.044282 ; Train Acc : 0.596 ; Test Loss : 0.055454 ; Test Acc : 0.562\n",
      "\n",
      "\n",
      "Epoch : 1 ; Train Loss : 0.191399 ; Train Acc : 0.114 ; Test Loss : 0.092170 ; Test Acc : 0.125 ; LR : 0.045\n",
      "Epoch : 2 ; Train Loss : 0.091082 ; Train Acc : 0.114 ; Test Loss : 0.090029 ; Test Acc : 0.188 ; LR : 0.045\n",
      "Epoch : 3 ; Train Loss : 0.089169 ; Train Acc : 0.166 ; Test Loss : 0.087302 ; Test Acc : 0.188 ; LR : 0.045\n",
      "Epoch : 4 ; Train Loss : 0.085075 ; Train Acc : 0.222 ; Test Loss : 0.082818 ; Test Acc : 0.188 ; LR : 0.045\n",
      "Epoch : 5 ; Train Loss : 0.080946 ; Train Acc : 0.276 ; Test Loss : 0.079262 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 6 ; Train Loss : 0.077851 ; Train Acc : 0.297 ; Test Loss : 0.077013 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 7 ; Train Loss : 0.075050 ; Train Acc : 0.319 ; Test Loss : 0.073214 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 8 ; Train Loss : 0.071451 ; Train Acc : 0.349 ; Test Loss : 0.070509 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 9 ; Train Loss : 0.069279 ; Train Acc : 0.366 ; Test Loss : 0.069372 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 10 ; Train Loss : 0.068269 ; Train Acc : 0.376 ; Test Loss : 0.068767 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 11 ; Train Loss : 0.067781 ; Train Acc : 0.374 ; Test Loss : 0.068353 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 12 ; Train Loss : 0.067169 ; Train Acc : 0.382 ; Test Loss : 0.068178 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 13 ; Train Loss : 0.067073 ; Train Acc : 0.381 ; Test Loss : 0.068111 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 14 ; Train Loss : 0.066808 ; Train Acc : 0.375 ; Test Loss : 0.067850 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 15 ; Train Loss : 0.066484 ; Train Acc : 0.376 ; Test Loss : 0.067655 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 16 ; Train Loss : 0.066588 ; Train Acc : 0.380 ; Test Loss : 0.067626 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 17 ; Train Loss : 0.066454 ; Train Acc : 0.386 ; Test Loss : 0.067463 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 18 ; Train Loss : 0.066222 ; Train Acc : 0.384 ; Test Loss : 0.067396 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 19 ; Train Loss : 0.066400 ; Train Acc : 0.376 ; Test Loss : 0.067280 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 20 ; Train Loss : 0.065927 ; Train Acc : 0.386 ; Test Loss : 0.067200 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 21 ; Train Loss : 0.066069 ; Train Acc : 0.387 ; Test Loss : 0.067275 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 22 ; Train Loss : 0.066055 ; Train Acc : 0.378 ; Test Loss : 0.067205 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 23 ; Train Loss : 0.065917 ; Train Acc : 0.383 ; Test Loss : 0.067232 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 24 ; Train Loss : 0.066105 ; Train Acc : 0.388 ; Test Loss : 0.067105 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 25 ; Train Loss : 0.065550 ; Train Acc : 0.386 ; Test Loss : 0.067069 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 26 ; Train Loss : 0.065809 ; Train Acc : 0.387 ; Test Loss : 0.067017 ; Test Acc : 0.438 ; LR : 0.045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 27 ; Train Loss : 0.065737 ; Train Acc : 0.390 ; Test Loss : 0.067063 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 28 ; Train Loss : 0.065498 ; Train Acc : 0.386 ; Test Loss : 0.067030 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 29 ; Train Loss : 0.065549 ; Train Acc : 0.381 ; Test Loss : 0.066996 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 30 ; Train Loss : 0.065439 ; Train Acc : 0.380 ; Test Loss : 0.066851 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 31 ; Train Loss : 0.065107 ; Train Acc : 0.394 ; Test Loss : 0.066706 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 32 ; Train Loss : 0.065036 ; Train Acc : 0.400 ; Test Loss : 0.065948 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 33 ; Train Loss : 0.063435 ; Train Acc : 0.422 ; Test Loss : 0.063952 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 34 ; Train Loss : 0.061334 ; Train Acc : 0.445 ; Test Loss : 0.061631 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 35 ; Train Loss : 0.059800 ; Train Acc : 0.459 ; Test Loss : 0.060820 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 36 ; Train Loss : 0.058982 ; Train Acc : 0.464 ; Test Loss : 0.060490 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 37 ; Train Loss : 0.058392 ; Train Acc : 0.469 ; Test Loss : 0.060323 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 38 ; Train Loss : 0.058294 ; Train Acc : 0.473 ; Test Loss : 0.060063 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 39 ; Train Loss : 0.058600 ; Train Acc : 0.478 ; Test Loss : 0.060043 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 40 ; Train Loss : 0.058189 ; Train Acc : 0.471 ; Test Loss : 0.060083 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 41 ; Train Loss : 0.058255 ; Train Acc : 0.470 ; Test Loss : 0.060055 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 42 ; Train Loss : 0.058296 ; Train Acc : 0.471 ; Test Loss : 0.059786 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 43 ; Train Loss : 0.058019 ; Train Acc : 0.465 ; Test Loss : 0.059807 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 44 ; Train Loss : 0.057887 ; Train Acc : 0.471 ; Test Loss : 0.059793 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 45 ; Train Loss : 0.057699 ; Train Acc : 0.472 ; Test Loss : 0.059773 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 46 ; Train Loss : 0.057905 ; Train Acc : 0.470 ; Test Loss : 0.059620 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 47 ; Train Loss : 0.057347 ; Train Acc : 0.464 ; Test Loss : 0.059663 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 48 ; Train Loss : 0.057846 ; Train Acc : 0.478 ; Test Loss : 0.059638 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 49 ; Train Loss : 0.057842 ; Train Acc : 0.475 ; Test Loss : 0.059552 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 50 ; Train Loss : 0.057662 ; Train Acc : 0.475 ; Test Loss : 0.059564 ; Test Acc : 0.562 ; LR : 0.045\n",
      "Epoch : 51 ; Train Loss : 0.057497 ; Train Acc : 0.476 ; Test Loss : 0.059430 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 52 ; Train Loss : 0.057205 ; Train Acc : 0.481 ; Test Loss : 0.059372 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 53 ; Train Loss : 0.057289 ; Train Acc : 0.478 ; Test Loss : 0.059742 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 54 ; Train Loss : 0.057459 ; Train Acc : 0.476 ; Test Loss : 0.059453 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 55 ; Train Loss : 0.057649 ; Train Acc : 0.469 ; Test Loss : 0.059603 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 56 ; Train Loss : 0.057742 ; Train Acc : 0.477 ; Test Loss : 0.059360 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 57 ; Train Loss : 0.057303 ; Train Acc : 0.471 ; Test Loss : 0.059371 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 58 ; Train Loss : 0.057170 ; Train Acc : 0.479 ; Test Loss : 0.059279 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 59 ; Train Loss : 0.056784 ; Train Acc : 0.471 ; Test Loss : 0.059382 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 60 ; Train Loss : 0.057032 ; Train Acc : 0.477 ; Test Loss : 0.059336 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 61 ; Train Loss : 0.057287 ; Train Acc : 0.485 ; Test Loss : 0.059352 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 62 ; Train Loss : 0.057023 ; Train Acc : 0.477 ; Test Loss : 0.059317 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 63 ; Train Loss : 0.057039 ; Train Acc : 0.478 ; Test Loss : 0.059310 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 64 ; Train Loss : 0.056744 ; Train Acc : 0.473 ; Test Loss : 0.059241 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 65 ; Train Loss : 0.057025 ; Train Acc : 0.473 ; Test Loss : 0.059492 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 66 ; Train Loss : 0.056879 ; Train Acc : 0.477 ; Test Loss : 0.059614 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 67 ; Train Loss : 0.056977 ; Train Acc : 0.485 ; Test Loss : 0.059315 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 68 ; Train Loss : 0.056665 ; Train Acc : 0.478 ; Test Loss : 0.059553 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 69 ; Train Loss : 0.057081 ; Train Acc : 0.488 ; Test Loss : 0.059687 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 70 ; Train Loss : 0.057061 ; Train Acc : 0.474 ; Test Loss : 0.059283 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 71 ; Train Loss : 0.056945 ; Train Acc : 0.480 ; Test Loss : 0.059391 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 72 ; Train Loss : 0.056821 ; Train Acc : 0.474 ; Test Loss : 0.059375 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 73 ; Train Loss : 0.056909 ; Train Acc : 0.475 ; Test Loss : 0.059314 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 74 ; Train Loss : 0.056814 ; Train Acc : 0.478 ; Test Loss : 0.059757 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 75 ; Train Loss : 0.056696 ; Train Acc : 0.475 ; Test Loss : 0.059366 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 76 ; Train Loss : 0.056574 ; Train Acc : 0.483 ; Test Loss : 0.059478 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 77 ; Train Loss : 0.056426 ; Train Acc : 0.475 ; Test Loss : 0.059277 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 78 ; Train Loss : 0.056441 ; Train Acc : 0.478 ; Test Loss : 0.059402 ; Test Acc : 0.438 ; LR : 0.041\n",
      "Epoch : 79 ; Train Loss : 0.056347 ; Train Acc : 0.479 ; Test Loss : 0.059352 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 80 ; Train Loss : 0.056289 ; Train Acc : 0.479 ; Test Loss : 0.059423 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 81 ; Train Loss : 0.056547 ; Train Acc : 0.493 ; Test Loss : 0.059394 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 82 ; Train Loss : 0.056618 ; Train Acc : 0.478 ; Test Loss : 0.059447 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 83 ; Train Loss : 0.056225 ; Train Acc : 0.476 ; Test Loss : 0.059427 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 84 ; Train Loss : 0.056416 ; Train Acc : 0.478 ; Test Loss : 0.059426 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 85 ; Train Loss : 0.056733 ; Train Acc : 0.477 ; Test Loss : 0.059465 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 86 ; Train Loss : 0.056217 ; Train Acc : 0.475 ; Test Loss : 0.059429 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 87 ; Train Loss : 0.056437 ; Train Acc : 0.485 ; Test Loss : 0.059519 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 88 ; Train Loss : 0.056172 ; Train Acc : 0.478 ; Test Loss : 0.059334 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 89 ; Train Loss : 0.056510 ; Train Acc : 0.486 ; Test Loss : 0.059403 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 90 ; Train Loss : 0.056244 ; Train Acc : 0.484 ; Test Loss : 0.059436 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 91 ; Train Loss : 0.056348 ; Train Acc : 0.477 ; Test Loss : 0.059504 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 92 ; Train Loss : 0.056136 ; Train Acc : 0.480 ; Test Loss : 0.059326 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 93 ; Train Loss : 0.056313 ; Train Acc : 0.480 ; Test Loss : 0.059378 ; Test Acc : 0.625 ; LR : 0.041\n",
      "Epoch : 94 ; Train Loss : 0.056163 ; Train Acc : 0.489 ; Test Loss : 0.059464 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 95 ; Train Loss : 0.056020 ; Train Acc : 0.481 ; Test Loss : 0.059413 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 96 ; Train Loss : 0.055950 ; Train Acc : 0.477 ; Test Loss : 0.059560 ; Test Acc : 0.688 ; LR : 0.041\n",
      "Epoch : 97 ; Train Loss : 0.056191 ; Train Acc : 0.484 ; Test Loss : 0.059724 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 98 ; Train Loss : 0.056219 ; Train Acc : 0.479 ; Test Loss : 0.059391 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 99 ; Train Loss : 0.056077 ; Train Acc : 0.477 ; Test Loss : 0.059502 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 100 ; Train Loss : 0.056153 ; Train Acc : 0.481 ; Test Loss : 0.059674 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 101 ; Train Loss : 0.056381 ; Train Acc : 0.476 ; Test Loss : 0.059417 ; Test Acc : 0.500 ; LR : 0.036\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 102 ; Train Loss : 0.055868 ; Train Acc : 0.483 ; Test Loss : 0.059389 ; Test Acc : 0.562 ; LR : 0.036\n",
      "Epoch : 103 ; Train Loss : 0.055815 ; Train Acc : 0.495 ; Test Loss : 0.059574 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 104 ; Train Loss : 0.056263 ; Train Acc : 0.481 ; Test Loss : 0.059395 ; Test Acc : 0.562 ; LR : 0.036\n",
      "Epoch : 105 ; Train Loss : 0.056064 ; Train Acc : 0.479 ; Test Loss : 0.059435 ; Test Acc : 0.562 ; LR : 0.036\n",
      "Epoch : 106 ; Train Loss : 0.056033 ; Train Acc : 0.482 ; Test Loss : 0.059428 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 107 ; Train Loss : 0.055945 ; Train Acc : 0.478 ; Test Loss : 0.059681 ; Test Acc : 0.562 ; LR : 0.036\n",
      "Epoch : 108 ; Train Loss : 0.055824 ; Train Acc : 0.478 ; Test Loss : 0.059565 ; Test Acc : 0.562 ; LR : 0.036\n",
      "Epoch : 109 ; Train Loss : 0.056046 ; Train Acc : 0.481 ; Test Loss : 0.059513 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 110 ; Train Loss : 0.055890 ; Train Acc : 0.483 ; Test Loss : 0.059656 ; Test Acc : 0.562 ; LR : 0.036\n",
      "Epoch : 111 ; Train Loss : 0.056063 ; Train Acc : 0.475 ; Test Loss : 0.059545 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 112 ; Train Loss : 0.056036 ; Train Acc : 0.483 ; Test Loss : 0.059436 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 113 ; Train Loss : 0.055941 ; Train Acc : 0.492 ; Test Loss : 0.059552 ; Test Acc : 0.562 ; LR : 0.036\n",
      "Epoch : 114 ; Train Loss : 0.056014 ; Train Acc : 0.485 ; Test Loss : 0.059529 ; Test Acc : 0.562 ; LR : 0.036\n",
      "Epoch : 115 ; Train Loss : 0.055986 ; Train Acc : 0.481 ; Test Loss : 0.059496 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 116 ; Train Loss : 0.055782 ; Train Acc : 0.484 ; Test Loss : 0.059525 ; Test Acc : 0.625 ; LR : 0.036\n",
      "Epoch : 117 ; Train Loss : 0.055725 ; Train Acc : 0.479 ; Test Loss : 0.059520 ; Test Acc : 0.688 ; LR : 0.036\n",
      "Epoch : 118 ; Train Loss : 0.056080 ; Train Acc : 0.483 ; Test Loss : 0.059565 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 119 ; Train Loss : 0.055813 ; Train Acc : 0.477 ; Test Loss : 0.059466 ; Test Acc : 0.562 ; LR : 0.036\n",
      "Epoch : 120 ; Train Loss : 0.055795 ; Train Acc : 0.478 ; Test Loss : 0.059496 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 121 ; Train Loss : 0.055686 ; Train Acc : 0.485 ; Test Loss : 0.059571 ; Test Acc : 0.562 ; LR : 0.036\n",
      "Epoch : 122 ; Train Loss : 0.055917 ; Train Acc : 0.472 ; Test Loss : 0.059501 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 123 ; Train Loss : 0.055882 ; Train Acc : 0.480 ; Test Loss : 0.059476 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 124 ; Train Loss : 0.055739 ; Train Acc : 0.484 ; Test Loss : 0.059658 ; Test Acc : 0.562 ; LR : 0.036\n",
      "Epoch : 125 ; Train Loss : 0.055865 ; Train Acc : 0.484 ; Test Loss : 0.059500 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 126 ; Train Loss : 0.055541 ; Train Acc : 0.482 ; Test Loss : 0.059537 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 127 ; Train Loss : 0.055660 ; Train Acc : 0.477 ; Test Loss : 0.059573 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 128 ; Train Loss : 0.055591 ; Train Acc : 0.479 ; Test Loss : 0.059683 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 129 ; Train Loss : 0.055965 ; Train Acc : 0.482 ; Test Loss : 0.059648 ; Test Acc : 0.562 ; LR : 0.036\n",
      "Epoch : 130 ; Train Loss : 0.055883 ; Train Acc : 0.484 ; Test Loss : 0.059621 ; Test Acc : 0.562 ; LR : 0.036\n",
      "Epoch : 131 ; Train Loss : 0.055768 ; Train Acc : 0.483 ; Test Loss : 0.059624 ; Test Acc : 0.562 ; LR : 0.036\n",
      "Epoch : 132 ; Train Loss : 0.055501 ; Train Acc : 0.481 ; Test Loss : 0.059703 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 133 ; Train Loss : 0.055584 ; Train Acc : 0.474 ; Test Loss : 0.059537 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 134 ; Train Loss : 0.055543 ; Train Acc : 0.489 ; Test Loss : 0.059670 ; Test Acc : 0.562 ; LR : 0.036\n",
      "Epoch : 135 ; Train Loss : 0.055787 ; Train Acc : 0.482 ; Test Loss : 0.059663 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 136 ; Train Loss : 0.055782 ; Train Acc : 0.476 ; Test Loss : 0.059841 ; Test Acc : 0.562 ; LR : 0.036\n",
      "Epoch : 137 ; Train Loss : 0.055568 ; Train Acc : 0.480 ; Test Loss : 0.059587 ; Test Acc : 0.562 ; LR : 0.036\n",
      "Epoch : 138 ; Train Loss : 0.055610 ; Train Acc : 0.482 ; Test Loss : 0.059825 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 139 ; Train Loss : 0.055433 ; Train Acc : 0.482 ; Test Loss : 0.059747 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 140 ; Train Loss : 0.055597 ; Train Acc : 0.481 ; Test Loss : 0.059667 ; Test Acc : 0.562 ; LR : 0.036\n",
      "Epoch : 141 ; Train Loss : 0.055459 ; Train Acc : 0.479 ; Test Loss : 0.059537 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 142 ; Train Loss : 0.055579 ; Train Acc : 0.478 ; Test Loss : 0.059891 ; Test Acc : 0.375 ; LR : 0.036\n",
      "Epoch : 143 ; Train Loss : 0.055562 ; Train Acc : 0.481 ; Test Loss : 0.059748 ; Test Acc : 0.562 ; LR : 0.036\n",
      "Epoch : 144 ; Train Loss : 0.055453 ; Train Acc : 0.486 ; Test Loss : 0.059745 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 145 ; Train Loss : 0.055269 ; Train Acc : 0.486 ; Test Loss : 0.059660 ; Test Acc : 0.562 ; LR : 0.036\n",
      "Epoch : 146 ; Train Loss : 0.055371 ; Train Acc : 0.484 ; Test Loss : 0.059718 ; Test Acc : 0.562 ; LR : 0.036\n",
      "Epoch : 147 ; Train Loss : 0.055831 ; Train Acc : 0.492 ; Test Loss : 0.059735 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 148 ; Train Loss : 0.055369 ; Train Acc : 0.477 ; Test Loss : 0.059912 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 149 ; Train Loss : 0.055347 ; Train Acc : 0.483 ; Test Loss : 0.059794 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 150 ; Train Loss : 0.055426 ; Train Acc : 0.483 ; Test Loss : 0.059777 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 151 ; Train Loss : 0.055601 ; Train Acc : 0.484 ; Test Loss : 0.059766 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 152 ; Train Loss : 0.055388 ; Train Acc : 0.484 ; Test Loss : 0.059728 ; Test Acc : 0.625 ; LR : 0.033\n",
      "Epoch : 153 ; Train Loss : 0.055356 ; Train Acc : 0.473 ; Test Loss : 0.059838 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 154 ; Train Loss : 0.055626 ; Train Acc : 0.484 ; Test Loss : 0.059721 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 155 ; Train Loss : 0.055633 ; Train Acc : 0.477 ; Test Loss : 0.059781 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 156 ; Train Loss : 0.055344 ; Train Acc : 0.484 ; Test Loss : 0.059690 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 157 ; Train Loss : 0.055576 ; Train Acc : 0.481 ; Test Loss : 0.059728 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 158 ; Train Loss : 0.055251 ; Train Acc : 0.490 ; Test Loss : 0.059758 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 159 ; Train Loss : 0.055708 ; Train Acc : 0.483 ; Test Loss : 0.059707 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 160 ; Train Loss : 0.055144 ; Train Acc : 0.475 ; Test Loss : 0.059774 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 161 ; Train Loss : 0.055260 ; Train Acc : 0.485 ; Test Loss : 0.059689 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 162 ; Train Loss : 0.055301 ; Train Acc : 0.488 ; Test Loss : 0.059747 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 163 ; Train Loss : 0.055177 ; Train Acc : 0.488 ; Test Loss : 0.059770 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 164 ; Train Loss : 0.055139 ; Train Acc : 0.481 ; Test Loss : 0.059836 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 165 ; Train Loss : 0.055121 ; Train Acc : 0.489 ; Test Loss : 0.059953 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 166 ; Train Loss : 0.055334 ; Train Acc : 0.480 ; Test Loss : 0.059795 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 167 ; Train Loss : 0.055194 ; Train Acc : 0.476 ; Test Loss : 0.059777 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 168 ; Train Loss : 0.055535 ; Train Acc : 0.482 ; Test Loss : 0.059969 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 169 ; Train Loss : 0.055410 ; Train Acc : 0.482 ; Test Loss : 0.059955 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 170 ; Train Loss : 0.055179 ; Train Acc : 0.483 ; Test Loss : 0.059987 ; Test Acc : 0.375 ; LR : 0.033\n",
      "Epoch : 171 ; Train Loss : 0.055259 ; Train Acc : 0.483 ; Test Loss : 0.059843 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 172 ; Train Loss : 0.055144 ; Train Acc : 0.483 ; Test Loss : 0.059954 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 173 ; Train Loss : 0.055646 ; Train Acc : 0.475 ; Test Loss : 0.060004 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 174 ; Train Loss : 0.055276 ; Train Acc : 0.476 ; Test Loss : 0.059937 ; Test Acc : 0.375 ; LR : 0.033\n",
      "Epoch : 175 ; Train Loss : 0.055548 ; Train Acc : 0.483 ; Test Loss : 0.059836 ; Test Acc : 0.562 ; LR : 0.033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 176 ; Train Loss : 0.055498 ; Train Acc : 0.486 ; Test Loss : 0.059885 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 177 ; Train Loss : 0.055147 ; Train Acc : 0.482 ; Test Loss : 0.059828 ; Test Acc : 0.375 ; LR : 0.033\n",
      "Epoch : 178 ; Train Loss : 0.055078 ; Train Acc : 0.483 ; Test Loss : 0.059813 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 179 ; Train Loss : 0.055374 ; Train Acc : 0.486 ; Test Loss : 0.059897 ; Test Acc : 0.375 ; LR : 0.033\n",
      "Epoch : 180 ; Train Loss : 0.055617 ; Train Acc : 0.477 ; Test Loss : 0.059905 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 181 ; Train Loss : 0.055654 ; Train Acc : 0.487 ; Test Loss : 0.060071 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 182 ; Train Loss : 0.055292 ; Train Acc : 0.484 ; Test Loss : 0.059846 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 183 ; Train Loss : 0.055148 ; Train Acc : 0.477 ; Test Loss : 0.059871 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 184 ; Train Loss : 0.055140 ; Train Acc : 0.485 ; Test Loss : 0.059932 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 185 ; Train Loss : 0.055254 ; Train Acc : 0.482 ; Test Loss : 0.059889 ; Test Acc : 0.562 ; LR : 0.033\n",
      "Epoch : 186 ; Train Loss : 0.055202 ; Train Acc : 0.486 ; Test Loss : 0.059921 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 187 ; Train Loss : 0.054924 ; Train Acc : 0.485 ; Test Loss : 0.059975 ; Test Acc : 0.375 ; LR : 0.033\n",
      "Epoch : 188 ; Train Loss : 0.055017 ; Train Acc : 0.483 ; Test Loss : 0.060066 ; Test Acc : 0.375 ; LR : 0.033\n",
      "Epoch : 189 ; Train Loss : 0.055353 ; Train Acc : 0.476 ; Test Loss : 0.059948 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 190 ; Train Loss : 0.054938 ; Train Acc : 0.488 ; Test Loss : 0.059891 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 191 ; Train Loss : 0.055202 ; Train Acc : 0.490 ; Test Loss : 0.059920 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 192 ; Train Loss : 0.055122 ; Train Acc : 0.485 ; Test Loss : 0.059976 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 193 ; Train Loss : 0.055035 ; Train Acc : 0.481 ; Test Loss : 0.059907 ; Test Acc : 0.375 ; LR : 0.033\n",
      "Epoch : 194 ; Train Loss : 0.055032 ; Train Acc : 0.480 ; Test Loss : 0.059939 ; Test Acc : 0.375 ; LR : 0.033\n",
      "Epoch : 195 ; Train Loss : 0.055344 ; Train Acc : 0.482 ; Test Loss : 0.059943 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 196 ; Train Loss : 0.054864 ; Train Acc : 0.478 ; Test Loss : 0.059991 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 197 ; Train Loss : 0.054878 ; Train Acc : 0.492 ; Test Loss : 0.060126 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 198 ; Train Loss : 0.054944 ; Train Acc : 0.485 ; Test Loss : 0.059988 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 199 ; Train Loss : 0.054614 ; Train Acc : 0.478 ; Test Loss : 0.059984 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 200 ; Train Loss : 0.055044 ; Train Acc : 0.483 ; Test Loss : 0.060047 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 201 ; Train Loss : 0.055011 ; Train Acc : 0.484 ; Test Loss : 0.060119 ; Test Acc : 0.375 ; LR : 0.030\n",
      "Epoch : 202 ; Train Loss : 0.055325 ; Train Acc : 0.483 ; Test Loss : 0.060191 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 203 ; Train Loss : 0.055162 ; Train Acc : 0.479 ; Test Loss : 0.060153 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 204 ; Train Loss : 0.055240 ; Train Acc : 0.484 ; Test Loss : 0.059932 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 205 ; Train Loss : 0.055352 ; Train Acc : 0.486 ; Test Loss : 0.060000 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 206 ; Train Loss : 0.054772 ; Train Acc : 0.482 ; Test Loss : 0.060117 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 207 ; Train Loss : 0.054909 ; Train Acc : 0.483 ; Test Loss : 0.060067 ; Test Acc : 0.375 ; LR : 0.030\n",
      "Epoch : 208 ; Train Loss : 0.054972 ; Train Acc : 0.487 ; Test Loss : 0.060167 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 209 ; Train Loss : 0.055128 ; Train Acc : 0.477 ; Test Loss : 0.060081 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 210 ; Train Loss : 0.054837 ; Train Acc : 0.484 ; Test Loss : 0.060140 ; Test Acc : 0.375 ; LR : 0.030\n",
      "Epoch : 211 ; Train Loss : 0.055099 ; Train Acc : 0.484 ; Test Loss : 0.060122 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 212 ; Train Loss : 0.054726 ; Train Acc : 0.482 ; Test Loss : 0.060133 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 213 ; Train Loss : 0.054997 ; Train Acc : 0.481 ; Test Loss : 0.060108 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 214 ; Train Loss : 0.054822 ; Train Acc : 0.482 ; Test Loss : 0.060077 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 215 ; Train Loss : 0.055121 ; Train Acc : 0.484 ; Test Loss : 0.060100 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 216 ; Train Loss : 0.055210 ; Train Acc : 0.484 ; Test Loss : 0.060207 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 217 ; Train Loss : 0.055071 ; Train Acc : 0.483 ; Test Loss : 0.060077 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 218 ; Train Loss : 0.055192 ; Train Acc : 0.485 ; Test Loss : 0.060175 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 219 ; Train Loss : 0.055087 ; Train Acc : 0.485 ; Test Loss : 0.060113 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 220 ; Train Loss : 0.055003 ; Train Acc : 0.484 ; Test Loss : 0.060188 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 221 ; Train Loss : 0.054936 ; Train Acc : 0.475 ; Test Loss : 0.060210 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 222 ; Train Loss : 0.054881 ; Train Acc : 0.485 ; Test Loss : 0.060163 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 223 ; Train Loss : 0.054623 ; Train Acc : 0.490 ; Test Loss : 0.060219 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 224 ; Train Loss : 0.054922 ; Train Acc : 0.481 ; Test Loss : 0.060114 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 225 ; Train Loss : 0.054854 ; Train Acc : 0.480 ; Test Loss : 0.060193 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 226 ; Train Loss : 0.054962 ; Train Acc : 0.481 ; Test Loss : 0.060371 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 227 ; Train Loss : 0.054997 ; Train Acc : 0.484 ; Test Loss : 0.060110 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 228 ; Train Loss : 0.054766 ; Train Acc : 0.479 ; Test Loss : 0.060153 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 229 ; Train Loss : 0.054892 ; Train Acc : 0.484 ; Test Loss : 0.060198 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 230 ; Train Loss : 0.054577 ; Train Acc : 0.489 ; Test Loss : 0.060182 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 231 ; Train Loss : 0.054958 ; Train Acc : 0.490 ; Test Loss : 0.060307 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 232 ; Train Loss : 0.054984 ; Train Acc : 0.490 ; Test Loss : 0.060541 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 233 ; Train Loss : 0.055008 ; Train Acc : 0.483 ; Test Loss : 0.060213 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 234 ; Train Loss : 0.054813 ; Train Acc : 0.491 ; Test Loss : 0.060207 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 235 ; Train Loss : 0.055085 ; Train Acc : 0.477 ; Test Loss : 0.060193 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 236 ; Train Loss : 0.054867 ; Train Acc : 0.489 ; Test Loss : 0.060174 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 237 ; Train Loss : 0.054948 ; Train Acc : 0.485 ; Test Loss : 0.060273 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 238 ; Train Loss : 0.054680 ; Train Acc : 0.478 ; Test Loss : 0.060388 ; Test Acc : 0.375 ; LR : 0.030\n",
      "Epoch : 239 ; Train Loss : 0.054864 ; Train Acc : 0.484 ; Test Loss : 0.060256 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 240 ; Train Loss : 0.054931 ; Train Acc : 0.479 ; Test Loss : 0.060202 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 241 ; Train Loss : 0.055092 ; Train Acc : 0.487 ; Test Loss : 0.060357 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 242 ; Train Loss : 0.054845 ; Train Acc : 0.486 ; Test Loss : 0.060486 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 243 ; Train Loss : 0.054952 ; Train Acc : 0.483 ; Test Loss : 0.060350 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 244 ; Train Loss : 0.054632 ; Train Acc : 0.479 ; Test Loss : 0.060208 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 245 ; Train Loss : 0.055072 ; Train Acc : 0.483 ; Test Loss : 0.060310 ; Test Acc : 0.500 ; LR : 0.030\n",
      "Epoch : 246 ; Train Loss : 0.054613 ; Train Acc : 0.485 ; Test Loss : 0.060242 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 247 ; Train Loss : 0.054670 ; Train Acc : 0.488 ; Test Loss : 0.060485 ; Test Acc : 0.562 ; LR : 0.030\n",
      "Epoch : 248 ; Train Loss : 0.054559 ; Train Acc : 0.486 ; Test Loss : 0.060290 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 249 ; Train Loss : 0.054635 ; Train Acc : 0.484 ; Test Loss : 0.060261 ; Test Acc : 0.438 ; LR : 0.030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 250 ; Train Loss : 0.054813 ; Train Acc : 0.489 ; Test Loss : 0.060404 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 251 ; Train Loss : 0.054809 ; Train Acc : 0.483 ; Test Loss : 0.060371 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 252 ; Train Loss : 0.054905 ; Train Acc : 0.480 ; Test Loss : 0.060384 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 253 ; Train Loss : 0.055071 ; Train Acc : 0.483 ; Test Loss : 0.060311 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 254 ; Train Loss : 0.054826 ; Train Acc : 0.487 ; Test Loss : 0.060307 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 255 ; Train Loss : 0.054798 ; Train Acc : 0.480 ; Test Loss : 0.060426 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 256 ; Train Loss : 0.054721 ; Train Acc : 0.478 ; Test Loss : 0.060297 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 257 ; Train Loss : 0.054812 ; Train Acc : 0.483 ; Test Loss : 0.060332 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 258 ; Train Loss : 0.054458 ; Train Acc : 0.482 ; Test Loss : 0.060347 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 259 ; Train Loss : 0.054650 ; Train Acc : 0.480 ; Test Loss : 0.060399 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 260 ; Train Loss : 0.054815 ; Train Acc : 0.480 ; Test Loss : 0.060373 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 261 ; Train Loss : 0.054906 ; Train Acc : 0.486 ; Test Loss : 0.060385 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 262 ; Train Loss : 0.054646 ; Train Acc : 0.492 ; Test Loss : 0.060482 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 263 ; Train Loss : 0.054731 ; Train Acc : 0.485 ; Test Loss : 0.060427 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 264 ; Train Loss : 0.054725 ; Train Acc : 0.487 ; Test Loss : 0.060387 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 265 ; Train Loss : 0.054770 ; Train Acc : 0.485 ; Test Loss : 0.060524 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 266 ; Train Loss : 0.054614 ; Train Acc : 0.492 ; Test Loss : 0.060537 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 267 ; Train Loss : 0.054761 ; Train Acc : 0.481 ; Test Loss : 0.060359 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 268 ; Train Loss : 0.054688 ; Train Acc : 0.485 ; Test Loss : 0.060672 ; Test Acc : 0.375 ; LR : 0.027\n",
      "Epoch : 269 ; Train Loss : 0.054801 ; Train Acc : 0.477 ; Test Loss : 0.060413 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 270 ; Train Loss : 0.054750 ; Train Acc : 0.486 ; Test Loss : 0.060404 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 271 ; Train Loss : 0.054736 ; Train Acc : 0.487 ; Test Loss : 0.060594 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 272 ; Train Loss : 0.054860 ; Train Acc : 0.485 ; Test Loss : 0.060487 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 273 ; Train Loss : 0.054597 ; Train Acc : 0.476 ; Test Loss : 0.060499 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 274 ; Train Loss : 0.054716 ; Train Acc : 0.484 ; Test Loss : 0.060438 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 275 ; Train Loss : 0.054837 ; Train Acc : 0.481 ; Test Loss : 0.060674 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 276 ; Train Loss : 0.054713 ; Train Acc : 0.491 ; Test Loss : 0.060563 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 277 ; Train Loss : 0.054852 ; Train Acc : 0.483 ; Test Loss : 0.060504 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 278 ; Train Loss : 0.054751 ; Train Acc : 0.492 ; Test Loss : 0.060508 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 279 ; Train Loss : 0.054648 ; Train Acc : 0.483 ; Test Loss : 0.060463 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 280 ; Train Loss : 0.054200 ; Train Acc : 0.487 ; Test Loss : 0.060412 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 281 ; Train Loss : 0.054503 ; Train Acc : 0.484 ; Test Loss : 0.060543 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 282 ; Train Loss : 0.054391 ; Train Acc : 0.484 ; Test Loss : 0.060583 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 283 ; Train Loss : 0.054524 ; Train Acc : 0.486 ; Test Loss : 0.060615 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 284 ; Train Loss : 0.054281 ; Train Acc : 0.481 ; Test Loss : 0.060539 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 285 ; Train Loss : 0.054776 ; Train Acc : 0.485 ; Test Loss : 0.060524 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 286 ; Train Loss : 0.054567 ; Train Acc : 0.486 ; Test Loss : 0.060535 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 287 ; Train Loss : 0.054616 ; Train Acc : 0.486 ; Test Loss : 0.060649 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 288 ; Train Loss : 0.054709 ; Train Acc : 0.489 ; Test Loss : 0.060799 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 289 ; Train Loss : 0.054680 ; Train Acc : 0.486 ; Test Loss : 0.060480 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 290 ; Train Loss : 0.054411 ; Train Acc : 0.482 ; Test Loss : 0.060568 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 291 ; Train Loss : 0.054842 ; Train Acc : 0.484 ; Test Loss : 0.060889 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 292 ; Train Loss : 0.054621 ; Train Acc : 0.482 ; Test Loss : 0.060476 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 293 ; Train Loss : 0.054535 ; Train Acc : 0.482 ; Test Loss : 0.060564 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 294 ; Train Loss : 0.054669 ; Train Acc : 0.483 ; Test Loss : 0.060540 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 295 ; Train Loss : 0.054622 ; Train Acc : 0.486 ; Test Loss : 0.060634 ; Test Acc : 0.562 ; LR : 0.027\n",
      "Epoch : 296 ; Train Loss : 0.054749 ; Train Acc : 0.483 ; Test Loss : 0.060647 ; Test Acc : 0.625 ; LR : 0.027\n",
      "Epoch : 297 ; Train Loss : 0.054804 ; Train Acc : 0.490 ; Test Loss : 0.060716 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 298 ; Train Loss : 0.054780 ; Train Acc : 0.485 ; Test Loss : 0.060542 ; Test Acc : 0.500 ; LR : 0.027\n",
      "Epoch : 299 ; Train Loss : 0.054554 ; Train Acc : 0.483 ; Test Loss : 0.060546 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 300 ; Train Loss : 0.054749 ; Train Acc : 0.488 ; Test Loss : 0.060543 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 301 ; Train Loss : 0.054475 ; Train Acc : 0.484 ; Test Loss : 0.060512 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 302 ; Train Loss : 0.054707 ; Train Acc : 0.484 ; Test Loss : 0.060752 ; Test Acc : 0.375 ; LR : 0.024\n",
      "Epoch : 303 ; Train Loss : 0.054897 ; Train Acc : 0.485 ; Test Loss : 0.060760 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 304 ; Train Loss : 0.054394 ; Train Acc : 0.479 ; Test Loss : 0.060700 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 305 ; Train Loss : 0.054618 ; Train Acc : 0.487 ; Test Loss : 0.060553 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 306 ; Train Loss : 0.054496 ; Train Acc : 0.485 ; Test Loss : 0.060730 ; Test Acc : 0.375 ; LR : 0.024\n",
      "Epoch : 307 ; Train Loss : 0.054720 ; Train Acc : 0.478 ; Test Loss : 0.060611 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 308 ; Train Loss : 0.054389 ; Train Acc : 0.484 ; Test Loss : 0.060640 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 309 ; Train Loss : 0.054508 ; Train Acc : 0.485 ; Test Loss : 0.060763 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 310 ; Train Loss : 0.054451 ; Train Acc : 0.490 ; Test Loss : 0.060612 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 311 ; Train Loss : 0.054224 ; Train Acc : 0.485 ; Test Loss : 0.060641 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 312 ; Train Loss : 0.054673 ; Train Acc : 0.483 ; Test Loss : 0.060974 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 313 ; Train Loss : 0.054631 ; Train Acc : 0.486 ; Test Loss : 0.060656 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 314 ; Train Loss : 0.054523 ; Train Acc : 0.485 ; Test Loss : 0.060729 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 315 ; Train Loss : 0.054217 ; Train Acc : 0.485 ; Test Loss : 0.060674 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 316 ; Train Loss : 0.054419 ; Train Acc : 0.489 ; Test Loss : 0.060702 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 317 ; Train Loss : 0.054405 ; Train Acc : 0.483 ; Test Loss : 0.060732 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 318 ; Train Loss : 0.054583 ; Train Acc : 0.484 ; Test Loss : 0.060692 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 319 ; Train Loss : 0.054451 ; Train Acc : 0.487 ; Test Loss : 0.060802 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 320 ; Train Loss : 0.054316 ; Train Acc : 0.485 ; Test Loss : 0.060674 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 321 ; Train Loss : 0.054384 ; Train Acc : 0.494 ; Test Loss : 0.060711 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 322 ; Train Loss : 0.054556 ; Train Acc : 0.487 ; Test Loss : 0.060882 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 323 ; Train Loss : 0.054688 ; Train Acc : 0.481 ; Test Loss : 0.060848 ; Test Acc : 0.438 ; LR : 0.024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 324 ; Train Loss : 0.054545 ; Train Acc : 0.486 ; Test Loss : 0.060933 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 325 ; Train Loss : 0.054488 ; Train Acc : 0.487 ; Test Loss : 0.060854 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 326 ; Train Loss : 0.054382 ; Train Acc : 0.488 ; Test Loss : 0.060828 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 327 ; Train Loss : 0.054499 ; Train Acc : 0.484 ; Test Loss : 0.061222 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 328 ; Train Loss : 0.054668 ; Train Acc : 0.479 ; Test Loss : 0.061155 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 329 ; Train Loss : 0.054240 ; Train Acc : 0.485 ; Test Loss : 0.060742 ; Test Acc : 0.375 ; LR : 0.024\n",
      "Epoch : 330 ; Train Loss : 0.054470 ; Train Acc : 0.484 ; Test Loss : 0.060680 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 331 ; Train Loss : 0.054395 ; Train Acc : 0.481 ; Test Loss : 0.060718 ; Test Acc : 0.625 ; LR : 0.024\n",
      "Epoch : 332 ; Train Loss : 0.054454 ; Train Acc : 0.490 ; Test Loss : 0.060931 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 333 ; Train Loss : 0.055067 ; Train Acc : 0.482 ; Test Loss : 0.060692 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 334 ; Train Loss : 0.054333 ; Train Acc : 0.492 ; Test Loss : 0.060741 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 335 ; Train Loss : 0.054521 ; Train Acc : 0.482 ; Test Loss : 0.060804 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 336 ; Train Loss : 0.054632 ; Train Acc : 0.483 ; Test Loss : 0.060971 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 337 ; Train Loss : 0.054141 ; Train Acc : 0.487 ; Test Loss : 0.060896 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 338 ; Train Loss : 0.054530 ; Train Acc : 0.487 ; Test Loss : 0.060949 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 339 ; Train Loss : 0.054305 ; Train Acc : 0.484 ; Test Loss : 0.060810 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 340 ; Train Loss : 0.054552 ; Train Acc : 0.483 ; Test Loss : 0.060843 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 341 ; Train Loss : 0.054240 ; Train Acc : 0.492 ; Test Loss : 0.060709 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 342 ; Train Loss : 0.054319 ; Train Acc : 0.489 ; Test Loss : 0.060839 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 343 ; Train Loss : 0.054333 ; Train Acc : 0.489 ; Test Loss : 0.060768 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 344 ; Train Loss : 0.054565 ; Train Acc : 0.478 ; Test Loss : 0.060781 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 345 ; Train Loss : 0.054324 ; Train Acc : 0.485 ; Test Loss : 0.060885 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 346 ; Train Loss : 0.054401 ; Train Acc : 0.484 ; Test Loss : 0.060835 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 347 ; Train Loss : 0.054065 ; Train Acc : 0.485 ; Test Loss : 0.060826 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 348 ; Train Loss : 0.054379 ; Train Acc : 0.482 ; Test Loss : 0.060793 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 349 ; Train Loss : 0.054695 ; Train Acc : 0.481 ; Test Loss : 0.060832 ; Test Acc : 0.500 ; LR : 0.024\n",
      "Epoch : 350 ; Train Loss : 0.054689 ; Train Acc : 0.484 ; Test Loss : 0.060853 ; Test Acc : 0.562 ; LR : 0.024\n",
      "Epoch : 351 ; Train Loss : 0.054362 ; Train Acc : 0.482 ; Test Loss : 0.060824 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 352 ; Train Loss : 0.054382 ; Train Acc : 0.486 ; Test Loss : 0.060996 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 353 ; Train Loss : 0.054258 ; Train Acc : 0.491 ; Test Loss : 0.060871 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 354 ; Train Loss : 0.054366 ; Train Acc : 0.487 ; Test Loss : 0.060951 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 355 ; Train Loss : 0.054576 ; Train Acc : 0.484 ; Test Loss : 0.060929 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 356 ; Train Loss : 0.054287 ; Train Acc : 0.485 ; Test Loss : 0.060894 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 357 ; Train Loss : 0.054280 ; Train Acc : 0.486 ; Test Loss : 0.060800 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 358 ; Train Loss : 0.054231 ; Train Acc : 0.484 ; Test Loss : 0.060949 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 359 ; Train Loss : 0.054408 ; Train Acc : 0.479 ; Test Loss : 0.061079 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 360 ; Train Loss : 0.054731 ; Train Acc : 0.487 ; Test Loss : 0.060894 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 361 ; Train Loss : 0.054214 ; Train Acc : 0.485 ; Test Loss : 0.060942 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 362 ; Train Loss : 0.054048 ; Train Acc : 0.481 ; Test Loss : 0.061219 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 363 ; Train Loss : 0.054185 ; Train Acc : 0.485 ; Test Loss : 0.060974 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 364 ; Train Loss : 0.054622 ; Train Acc : 0.485 ; Test Loss : 0.061133 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 365 ; Train Loss : 0.054267 ; Train Acc : 0.486 ; Test Loss : 0.060944 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 366 ; Train Loss : 0.054196 ; Train Acc : 0.486 ; Test Loss : 0.061075 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 367 ; Train Loss : 0.054088 ; Train Acc : 0.490 ; Test Loss : 0.060936 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 368 ; Train Loss : 0.054103 ; Train Acc : 0.488 ; Test Loss : 0.061002 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 369 ; Train Loss : 0.054273 ; Train Acc : 0.487 ; Test Loss : 0.061246 ; Test Acc : 0.375 ; LR : 0.022\n",
      "Epoch : 370 ; Train Loss : 0.054346 ; Train Acc : 0.487 ; Test Loss : 0.060906 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 371 ; Train Loss : 0.054217 ; Train Acc : 0.484 ; Test Loss : 0.060918 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 372 ; Train Loss : 0.054095 ; Train Acc : 0.486 ; Test Loss : 0.060977 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 373 ; Train Loss : 0.054423 ; Train Acc : 0.483 ; Test Loss : 0.061017 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 374 ; Train Loss : 0.054215 ; Train Acc : 0.487 ; Test Loss : 0.060930 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 375 ; Train Loss : 0.054497 ; Train Acc : 0.484 ; Test Loss : 0.061037 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 376 ; Train Loss : 0.054231 ; Train Acc : 0.494 ; Test Loss : 0.060946 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 377 ; Train Loss : 0.054397 ; Train Acc : 0.491 ; Test Loss : 0.060950 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 378 ; Train Loss : 0.054360 ; Train Acc : 0.484 ; Test Loss : 0.061010 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 379 ; Train Loss : 0.054625 ; Train Acc : 0.485 ; Test Loss : 0.061014 ; Test Acc : 0.375 ; LR : 0.022\n",
      "Epoch : 380 ; Train Loss : 0.054386 ; Train Acc : 0.492 ; Test Loss : 0.060993 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 381 ; Train Loss : 0.054365 ; Train Acc : 0.483 ; Test Loss : 0.061022 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 382 ; Train Loss : 0.054399 ; Train Acc : 0.486 ; Test Loss : 0.061077 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 383 ; Train Loss : 0.054126 ; Train Acc : 0.483 ; Test Loss : 0.061016 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 384 ; Train Loss : 0.054376 ; Train Acc : 0.482 ; Test Loss : 0.060967 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 385 ; Train Loss : 0.054140 ; Train Acc : 0.488 ; Test Loss : 0.061019 ; Test Acc : 0.375 ; LR : 0.022\n",
      "Epoch : 386 ; Train Loss : 0.054210 ; Train Acc : 0.488 ; Test Loss : 0.061114 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 387 ; Train Loss : 0.054004 ; Train Acc : 0.485 ; Test Loss : 0.061066 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 388 ; Train Loss : 0.054267 ; Train Acc : 0.490 ; Test Loss : 0.061015 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 389 ; Train Loss : 0.054508 ; Train Acc : 0.493 ; Test Loss : 0.061219 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 390 ; Train Loss : 0.054699 ; Train Acc : 0.486 ; Test Loss : 0.061022 ; Test Acc : 0.562 ; LR : 0.022\n",
      "Epoch : 391 ; Train Loss : 0.054487 ; Train Acc : 0.484 ; Test Loss : 0.061011 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 392 ; Train Loss : 0.054198 ; Train Acc : 0.487 ; Test Loss : 0.061098 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 393 ; Train Loss : 0.054324 ; Train Acc : 0.487 ; Test Loss : 0.061207 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 394 ; Train Loss : 0.054308 ; Train Acc : 0.486 ; Test Loss : 0.061137 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 395 ; Train Loss : 0.054297 ; Train Acc : 0.487 ; Test Loss : 0.061071 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 396 ; Train Loss : 0.053870 ; Train Acc : 0.486 ; Test Loss : 0.061281 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 397 ; Train Loss : 0.054184 ; Train Acc : 0.487 ; Test Loss : 0.061110 ; Test Acc : 0.500 ; LR : 0.022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 398 ; Train Loss : 0.054499 ; Train Acc : 0.486 ; Test Loss : 0.061173 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 399 ; Train Loss : 0.054396 ; Train Acc : 0.487 ; Test Loss : 0.061101 ; Test Acc : 0.500 ; LR : 0.022\n",
      "Epoch : 400 ; Train Loss : 0.054105 ; Train Acc : 0.491 ; Test Loss : 0.061075 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 401 ; Train Loss : 0.053978 ; Train Acc : 0.482 ; Test Loss : 0.061161 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 402 ; Train Loss : 0.054119 ; Train Acc : 0.488 ; Test Loss : 0.061132 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 403 ; Train Loss : 0.054091 ; Train Acc : 0.486 ; Test Loss : 0.061208 ; Test Acc : 0.375 ; LR : 0.019\n",
      "Epoch : 404 ; Train Loss : 0.054265 ; Train Acc : 0.484 ; Test Loss : 0.061312 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 405 ; Train Loss : 0.054078 ; Train Acc : 0.483 ; Test Loss : 0.061099 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 406 ; Train Loss : 0.054449 ; Train Acc : 0.483 ; Test Loss : 0.061334 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 407 ; Train Loss : 0.053981 ; Train Acc : 0.490 ; Test Loss : 0.061107 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 408 ; Train Loss : 0.054194 ; Train Acc : 0.490 ; Test Loss : 0.061121 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 409 ; Train Loss : 0.054266 ; Train Acc : 0.483 ; Test Loss : 0.061199 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 410 ; Train Loss : 0.054245 ; Train Acc : 0.487 ; Test Loss : 0.061215 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 411 ; Train Loss : 0.054020 ; Train Acc : 0.492 ; Test Loss : 0.061215 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 412 ; Train Loss : 0.054203 ; Train Acc : 0.484 ; Test Loss : 0.061294 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 413 ; Train Loss : 0.054322 ; Train Acc : 0.478 ; Test Loss : 0.061160 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 414 ; Train Loss : 0.054384 ; Train Acc : 0.490 ; Test Loss : 0.061156 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 415 ; Train Loss : 0.054086 ; Train Acc : 0.492 ; Test Loss : 0.061188 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 416 ; Train Loss : 0.053967 ; Train Acc : 0.488 ; Test Loss : 0.061156 ; Test Acc : 0.375 ; LR : 0.019\n",
      "Epoch : 417 ; Train Loss : 0.054045 ; Train Acc : 0.486 ; Test Loss : 0.061192 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 418 ; Train Loss : 0.054057 ; Train Acc : 0.488 ; Test Loss : 0.061251 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 419 ; Train Loss : 0.053953 ; Train Acc : 0.490 ; Test Loss : 0.061195 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 420 ; Train Loss : 0.054107 ; Train Acc : 0.490 ; Test Loss : 0.061201 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 421 ; Train Loss : 0.054071 ; Train Acc : 0.484 ; Test Loss : 0.061253 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 422 ; Train Loss : 0.054118 ; Train Acc : 0.490 ; Test Loss : 0.061209 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 423 ; Train Loss : 0.054110 ; Train Acc : 0.487 ; Test Loss : 0.061182 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 424 ; Train Loss : 0.054457 ; Train Acc : 0.485 ; Test Loss : 0.061290 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 425 ; Train Loss : 0.054146 ; Train Acc : 0.491 ; Test Loss : 0.061209 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 426 ; Train Loss : 0.054166 ; Train Acc : 0.485 ; Test Loss : 0.061405 ; Test Acc : 0.375 ; LR : 0.019\n",
      "Epoch : 427 ; Train Loss : 0.053963 ; Train Acc : 0.487 ; Test Loss : 0.061255 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 428 ; Train Loss : 0.054149 ; Train Acc : 0.488 ; Test Loss : 0.061340 ; Test Acc : 0.375 ; LR : 0.019\n",
      "Epoch : 429 ; Train Loss : 0.054360 ; Train Acc : 0.491 ; Test Loss : 0.061293 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 430 ; Train Loss : 0.054192 ; Train Acc : 0.483 ; Test Loss : 0.061318 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 431 ; Train Loss : 0.054051 ; Train Acc : 0.489 ; Test Loss : 0.061406 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 432 ; Train Loss : 0.054160 ; Train Acc : 0.481 ; Test Loss : 0.061224 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 433 ; Train Loss : 0.054385 ; Train Acc : 0.486 ; Test Loss : 0.061320 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 434 ; Train Loss : 0.054191 ; Train Acc : 0.486 ; Test Loss : 0.061236 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 435 ; Train Loss : 0.054232 ; Train Acc : 0.486 ; Test Loss : 0.061373 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 436 ; Train Loss : 0.054160 ; Train Acc : 0.486 ; Test Loss : 0.061328 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 437 ; Train Loss : 0.054278 ; Train Acc : 0.490 ; Test Loss : 0.061322 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 438 ; Train Loss : 0.054002 ; Train Acc : 0.488 ; Test Loss : 0.061384 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 439 ; Train Loss : 0.054315 ; Train Acc : 0.489 ; Test Loss : 0.061294 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 440 ; Train Loss : 0.054019 ; Train Acc : 0.489 ; Test Loss : 0.061487 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 441 ; Train Loss : 0.054283 ; Train Acc : 0.493 ; Test Loss : 0.061566 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 442 ; Train Loss : 0.054048 ; Train Acc : 0.485 ; Test Loss : 0.061404 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 443 ; Train Loss : 0.054361 ; Train Acc : 0.486 ; Test Loss : 0.061413 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 444 ; Train Loss : 0.054216 ; Train Acc : 0.488 ; Test Loss : 0.061275 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 445 ; Train Loss : 0.054170 ; Train Acc : 0.478 ; Test Loss : 0.061342 ; Test Acc : 0.375 ; LR : 0.019\n",
      "Epoch : 446 ; Train Loss : 0.054070 ; Train Acc : 0.488 ; Test Loss : 0.061446 ; Test Acc : 0.562 ; LR : 0.019\n",
      "Epoch : 447 ; Train Loss : 0.054443 ; Train Acc : 0.482 ; Test Loss : 0.061331 ; Test Acc : 0.375 ; LR : 0.019\n",
      "Epoch : 448 ; Train Loss : 0.054101 ; Train Acc : 0.490 ; Test Loss : 0.061365 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 449 ; Train Loss : 0.054080 ; Train Acc : 0.488 ; Test Loss : 0.061424 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 450 ; Train Loss : 0.054043 ; Train Acc : 0.481 ; Test Loss : 0.061348 ; Test Acc : 0.500 ; LR : 0.019\n",
      "Epoch : 451 ; Train Loss : 0.054532 ; Train Acc : 0.486 ; Test Loss : 0.061336 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 452 ; Train Loss : 0.054086 ; Train Acc : 0.487 ; Test Loss : 0.061463 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 453 ; Train Loss : 0.054354 ; Train Acc : 0.482 ; Test Loss : 0.061941 ; Test Acc : 0.375 ; LR : 0.017\n",
      "Epoch : 454 ; Train Loss : 0.054298 ; Train Acc : 0.476 ; Test Loss : 0.061488 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 455 ; Train Loss : 0.053866 ; Train Acc : 0.480 ; Test Loss : 0.061482 ; Test Acc : 0.375 ; LR : 0.017\n",
      "Epoch : 456 ; Train Loss : 0.054195 ; Train Acc : 0.491 ; Test Loss : 0.061442 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 457 ; Train Loss : 0.054248 ; Train Acc : 0.487 ; Test Loss : 0.061332 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 458 ; Train Loss : 0.054136 ; Train Acc : 0.486 ; Test Loss : 0.061350 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 459 ; Train Loss : 0.053857 ; Train Acc : 0.486 ; Test Loss : 0.061395 ; Test Acc : 0.375 ; LR : 0.017\n",
      "Epoch : 460 ; Train Loss : 0.054140 ; Train Acc : 0.487 ; Test Loss : 0.061387 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 461 ; Train Loss : 0.053948 ; Train Acc : 0.482 ; Test Loss : 0.061410 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 462 ; Train Loss : 0.054041 ; Train Acc : 0.485 ; Test Loss : 0.061485 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 463 ; Train Loss : 0.054370 ; Train Acc : 0.486 ; Test Loss : 0.061371 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 464 ; Train Loss : 0.054063 ; Train Acc : 0.489 ; Test Loss : 0.061412 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 465 ; Train Loss : 0.054024 ; Train Acc : 0.486 ; Test Loss : 0.061410 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 466 ; Train Loss : 0.053695 ; Train Acc : 0.480 ; Test Loss : 0.061521 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 467 ; Train Loss : 0.053828 ; Train Acc : 0.486 ; Test Loss : 0.061514 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 468 ; Train Loss : 0.054155 ; Train Acc : 0.485 ; Test Loss : 0.061587 ; Test Acc : 0.375 ; LR : 0.017\n",
      "Epoch : 469 ; Train Loss : 0.054090 ; Train Acc : 0.486 ; Test Loss : 0.061385 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 470 ; Train Loss : 0.054117 ; Train Acc : 0.488 ; Test Loss : 0.061420 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 471 ; Train Loss : 0.054034 ; Train Acc : 0.485 ; Test Loss : 0.061547 ; Test Acc : 0.438 ; LR : 0.017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 472 ; Train Loss : 0.053992 ; Train Acc : 0.484 ; Test Loss : 0.061567 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 473 ; Train Loss : 0.054131 ; Train Acc : 0.479 ; Test Loss : 0.061432 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 474 ; Train Loss : 0.054091 ; Train Acc : 0.487 ; Test Loss : 0.061432 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 475 ; Train Loss : 0.053861 ; Train Acc : 0.487 ; Test Loss : 0.061485 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 476 ; Train Loss : 0.054225 ; Train Acc : 0.492 ; Test Loss : 0.061511 ; Test Acc : 0.375 ; LR : 0.017\n",
      "Epoch : 477 ; Train Loss : 0.054004 ; Train Acc : 0.487 ; Test Loss : 0.061491 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 478 ; Train Loss : 0.054081 ; Train Acc : 0.479 ; Test Loss : 0.061408 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 479 ; Train Loss : 0.054347 ; Train Acc : 0.484 ; Test Loss : 0.061553 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 480 ; Train Loss : 0.053908 ; Train Acc : 0.487 ; Test Loss : 0.061449 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 481 ; Train Loss : 0.054201 ; Train Acc : 0.489 ; Test Loss : 0.061480 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 482 ; Train Loss : 0.054245 ; Train Acc : 0.487 ; Test Loss : 0.061647 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 483 ; Train Loss : 0.053948 ; Train Acc : 0.494 ; Test Loss : 0.061570 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 484 ; Train Loss : 0.054228 ; Train Acc : 0.480 ; Test Loss : 0.061522 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 485 ; Train Loss : 0.054139 ; Train Acc : 0.481 ; Test Loss : 0.061467 ; Test Acc : 0.562 ; LR : 0.017\n",
      "Epoch : 486 ; Train Loss : 0.054124 ; Train Acc : 0.488 ; Test Loss : 0.061449 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 487 ; Train Loss : 0.053889 ; Train Acc : 0.487 ; Test Loss : 0.061420 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 488 ; Train Loss : 0.054093 ; Train Acc : 0.492 ; Test Loss : 0.061522 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 489 ; Train Loss : 0.054030 ; Train Acc : 0.486 ; Test Loss : 0.061492 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 490 ; Train Loss : 0.053758 ; Train Acc : 0.486 ; Test Loss : 0.061516 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 491 ; Train Loss : 0.054099 ; Train Acc : 0.488 ; Test Loss : 0.061531 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 492 ; Train Loss : 0.053900 ; Train Acc : 0.487 ; Test Loss : 0.061629 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 493 ; Train Loss : 0.053860 ; Train Acc : 0.484 ; Test Loss : 0.061571 ; Test Acc : 0.375 ; LR : 0.017\n",
      "Epoch : 494 ; Train Loss : 0.053962 ; Train Acc : 0.492 ; Test Loss : 0.061523 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 495 ; Train Loss : 0.054163 ; Train Acc : 0.489 ; Test Loss : 0.061622 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 496 ; Train Loss : 0.053985 ; Train Acc : 0.477 ; Test Loss : 0.061569 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 497 ; Train Loss : 0.054190 ; Train Acc : 0.489 ; Test Loss : 0.061538 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 498 ; Train Loss : 0.053859 ; Train Acc : 0.486 ; Test Loss : 0.061613 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 499 ; Train Loss : 0.053641 ; Train Acc : 0.488 ; Test Loss : 0.061556 ; Test Acc : 0.500 ; LR : 0.017\n",
      "Epoch : 500 ; Train Loss : 0.054169 ; Train Acc : 0.487 ; Test Loss : 0.061610 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 501 ; Train Loss : 0.053836 ; Train Acc : 0.490 ; Test Loss : 0.061715 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 502 ; Train Loss : 0.054113 ; Train Acc : 0.489 ; Test Loss : 0.061565 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 503 ; Train Loss : 0.054286 ; Train Acc : 0.485 ; Test Loss : 0.061617 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 504 ; Train Loss : 0.054065 ; Train Acc : 0.487 ; Test Loss : 0.061672 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 505 ; Train Loss : 0.054128 ; Train Acc : 0.491 ; Test Loss : 0.061706 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 506 ; Train Loss : 0.054085 ; Train Acc : 0.487 ; Test Loss : 0.061665 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 507 ; Train Loss : 0.053982 ; Train Acc : 0.485 ; Test Loss : 0.061687 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 508 ; Train Loss : 0.054060 ; Train Acc : 0.491 ; Test Loss : 0.061688 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 509 ; Train Loss : 0.054063 ; Train Acc : 0.489 ; Test Loss : 0.061582 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 510 ; Train Loss : 0.053951 ; Train Acc : 0.488 ; Test Loss : 0.061596 ; Test Acc : 0.625 ; LR : 0.016\n",
      "Epoch : 511 ; Train Loss : 0.053904 ; Train Acc : 0.488 ; Test Loss : 0.061656 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 512 ; Train Loss : 0.053895 ; Train Acc : 0.487 ; Test Loss : 0.061639 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 513 ; Train Loss : 0.053780 ; Train Acc : 0.486 ; Test Loss : 0.061658 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 514 ; Train Loss : 0.053851 ; Train Acc : 0.491 ; Test Loss : 0.061654 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 515 ; Train Loss : 0.053840 ; Train Acc : 0.491 ; Test Loss : 0.061605 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 516 ; Train Loss : 0.054114 ; Train Acc : 0.482 ; Test Loss : 0.061761 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 517 ; Train Loss : 0.054026 ; Train Acc : 0.491 ; Test Loss : 0.061716 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 518 ; Train Loss : 0.054159 ; Train Acc : 0.485 ; Test Loss : 0.061748 ; Test Acc : 0.375 ; LR : 0.016\n",
      "Epoch : 519 ; Train Loss : 0.054024 ; Train Acc : 0.488 ; Test Loss : 0.061686 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 520 ; Train Loss : 0.053699 ; Train Acc : 0.489 ; Test Loss : 0.061677 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 521 ; Train Loss : 0.053840 ; Train Acc : 0.482 ; Test Loss : 0.061646 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 522 ; Train Loss : 0.054117 ; Train Acc : 0.488 ; Test Loss : 0.061735 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 523 ; Train Loss : 0.053716 ; Train Acc : 0.485 ; Test Loss : 0.061717 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 524 ; Train Loss : 0.053794 ; Train Acc : 0.488 ; Test Loss : 0.061623 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 525 ; Train Loss : 0.053869 ; Train Acc : 0.488 ; Test Loss : 0.061743 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 526 ; Train Loss : 0.054113 ; Train Acc : 0.490 ; Test Loss : 0.061679 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 527 ; Train Loss : 0.053672 ; Train Acc : 0.486 ; Test Loss : 0.061725 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 528 ; Train Loss : 0.054114 ; Train Acc : 0.491 ; Test Loss : 0.061708 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 529 ; Train Loss : 0.053948 ; Train Acc : 0.485 ; Test Loss : 0.061831 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 530 ; Train Loss : 0.054005 ; Train Acc : 0.487 ; Test Loss : 0.061790 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 531 ; Train Loss : 0.054102 ; Train Acc : 0.488 ; Test Loss : 0.061797 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 532 ; Train Loss : 0.053942 ; Train Acc : 0.485 ; Test Loss : 0.061973 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 533 ; Train Loss : 0.053726 ; Train Acc : 0.485 ; Test Loss : 0.061709 ; Test Acc : 0.375 ; LR : 0.016\n",
      "Epoch : 534 ; Train Loss : 0.054058 ; Train Acc : 0.491 ; Test Loss : 0.061699 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 535 ; Train Loss : 0.053844 ; Train Acc : 0.495 ; Test Loss : 0.061800 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 536 ; Train Loss : 0.053882 ; Train Acc : 0.491 ; Test Loss : 0.061747 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 537 ; Train Loss : 0.053783 ; Train Acc : 0.491 ; Test Loss : 0.061780 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 538 ; Train Loss : 0.053713 ; Train Acc : 0.489 ; Test Loss : 0.061790 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 539 ; Train Loss : 0.053658 ; Train Acc : 0.491 ; Test Loss : 0.061781 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 540 ; Train Loss : 0.054244 ; Train Acc : 0.485 ; Test Loss : 0.061841 ; Test Acc : 0.375 ; LR : 0.016\n",
      "Epoch : 541 ; Train Loss : 0.053792 ; Train Acc : 0.485 ; Test Loss : 0.061844 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 542 ; Train Loss : 0.053992 ; Train Acc : 0.490 ; Test Loss : 0.061759 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 543 ; Train Loss : 0.053975 ; Train Acc : 0.494 ; Test Loss : 0.061874 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 544 ; Train Loss : 0.054115 ; Train Acc : 0.487 ; Test Loss : 0.061802 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 545 ; Train Loss : 0.054020 ; Train Acc : 0.487 ; Test Loss : 0.061718 ; Test Acc : 0.375 ; LR : 0.016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 546 ; Train Loss : 0.053946 ; Train Acc : 0.487 ; Test Loss : 0.061806 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 547 ; Train Loss : 0.054167 ; Train Acc : 0.484 ; Test Loss : 0.061787 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 548 ; Train Loss : 0.054017 ; Train Acc : 0.486 ; Test Loss : 0.061996 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 549 ; Train Loss : 0.053733 ; Train Acc : 0.492 ; Test Loss : 0.061766 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 550 ; Train Loss : 0.053987 ; Train Acc : 0.491 ; Test Loss : 0.061861 ; Test Acc : 0.500 ; LR : 0.016\n",
      "Epoch : 551 ; Train Loss : 0.054062 ; Train Acc : 0.486 ; Test Loss : 0.061841 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 552 ; Train Loss : 0.053820 ; Train Acc : 0.490 ; Test Loss : 0.061809 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 553 ; Train Loss : 0.053842 ; Train Acc : 0.492 ; Test Loss : 0.061811 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 554 ; Train Loss : 0.054121 ; Train Acc : 0.493 ; Test Loss : 0.061850 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 555 ; Train Loss : 0.053799 ; Train Acc : 0.483 ; Test Loss : 0.061823 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 556 ; Train Loss : 0.054110 ; Train Acc : 0.489 ; Test Loss : 0.061782 ; Test Acc : 0.375 ; LR : 0.014\n",
      "Epoch : 557 ; Train Loss : 0.053654 ; Train Acc : 0.487 ; Test Loss : 0.061853 ; Test Acc : 0.375 ; LR : 0.014\n",
      "Epoch : 558 ; Train Loss : 0.054083 ; Train Acc : 0.487 ; Test Loss : 0.061837 ; Test Acc : 0.625 ; LR : 0.014\n",
      "Epoch : 559 ; Train Loss : 0.053935 ; Train Acc : 0.491 ; Test Loss : 0.061818 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 560 ; Train Loss : 0.054033 ; Train Acc : 0.486 ; Test Loss : 0.061813 ; Test Acc : 0.375 ; LR : 0.014\n",
      "Epoch : 561 ; Train Loss : 0.053698 ; Train Acc : 0.487 ; Test Loss : 0.061970 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 562 ; Train Loss : 0.054330 ; Train Acc : 0.486 ; Test Loss : 0.061930 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 563 ; Train Loss : 0.053981 ; Train Acc : 0.488 ; Test Loss : 0.061838 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 564 ; Train Loss : 0.053848 ; Train Acc : 0.486 ; Test Loss : 0.061880 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 565 ; Train Loss : 0.053817 ; Train Acc : 0.487 ; Test Loss : 0.061917 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 566 ; Train Loss : 0.053820 ; Train Acc : 0.488 ; Test Loss : 0.062055 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 567 ; Train Loss : 0.053801 ; Train Acc : 0.490 ; Test Loss : 0.061932 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 568 ; Train Loss : 0.054065 ; Train Acc : 0.485 ; Test Loss : 0.061898 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 569 ; Train Loss : 0.053889 ; Train Acc : 0.488 ; Test Loss : 0.061834 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 570 ; Train Loss : 0.053642 ; Train Acc : 0.489 ; Test Loss : 0.062048 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 571 ; Train Loss : 0.054008 ; Train Acc : 0.483 ; Test Loss : 0.061977 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 572 ; Train Loss : 0.054028 ; Train Acc : 0.487 ; Test Loss : 0.061821 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 573 ; Train Loss : 0.053661 ; Train Acc : 0.488 ; Test Loss : 0.061819 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 574 ; Train Loss : 0.053540 ; Train Acc : 0.489 ; Test Loss : 0.061836 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 575 ; Train Loss : 0.053761 ; Train Acc : 0.489 ; Test Loss : 0.061841 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 576 ; Train Loss : 0.053978 ; Train Acc : 0.491 ; Test Loss : 0.061814 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 577 ; Train Loss : 0.054011 ; Train Acc : 0.490 ; Test Loss : 0.061796 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 578 ; Train Loss : 0.053884 ; Train Acc : 0.486 ; Test Loss : 0.061870 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 579 ; Train Loss : 0.053789 ; Train Acc : 0.488 ; Test Loss : 0.061895 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 580 ; Train Loss : 0.053702 ; Train Acc : 0.483 ; Test Loss : 0.061863 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 581 ; Train Loss : 0.053904 ; Train Acc : 0.482 ; Test Loss : 0.061878 ; Test Acc : 0.562 ; LR : 0.014\n",
      "Epoch : 582 ; Train Loss : 0.053779 ; Train Acc : 0.491 ; Test Loss : 0.061925 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 583 ; Train Loss : 0.054017 ; Train Acc : 0.483 ; Test Loss : 0.061824 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 584 ; Train Loss : 0.053678 ; Train Acc : 0.487 ; Test Loss : 0.061951 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 585 ; Train Loss : 0.053965 ; Train Acc : 0.484 ; Test Loss : 0.061920 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 586 ; Train Loss : 0.053965 ; Train Acc : 0.485 ; Test Loss : 0.062038 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 587 ; Train Loss : 0.053789 ; Train Acc : 0.488 ; Test Loss : 0.061989 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 588 ; Train Loss : 0.053609 ; Train Acc : 0.488 ; Test Loss : 0.061910 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 589 ; Train Loss : 0.053714 ; Train Acc : 0.486 ; Test Loss : 0.061882 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 590 ; Train Loss : 0.053794 ; Train Acc : 0.490 ; Test Loss : 0.061909 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 591 ; Train Loss : 0.053914 ; Train Acc : 0.488 ; Test Loss : 0.062200 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 592 ; Train Loss : 0.053971 ; Train Acc : 0.487 ; Test Loss : 0.061951 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 593 ; Train Loss : 0.053868 ; Train Acc : 0.488 ; Test Loss : 0.062023 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 594 ; Train Loss : 0.053567 ; Train Acc : 0.488 ; Test Loss : 0.061930 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 595 ; Train Loss : 0.053923 ; Train Acc : 0.485 ; Test Loss : 0.061933 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 596 ; Train Loss : 0.053736 ; Train Acc : 0.483 ; Test Loss : 0.061937 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 597 ; Train Loss : 0.053543 ; Train Acc : 0.487 ; Test Loss : 0.062036 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 598 ; Train Loss : 0.053801 ; Train Acc : 0.486 ; Test Loss : 0.061901 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 599 ; Train Loss : 0.054013 ; Train Acc : 0.489 ; Test Loss : 0.062060 ; Test Acc : 0.500 ; LR : 0.014\n",
      "Epoch : 600 ; Train Loss : 0.053745 ; Train Acc : 0.488 ; Test Loss : 0.062056 ; Test Acc : 0.500 ; LR : 0.014\n",
      "\n",
      "Hidden Neurons : 47 ; Train Loss : 0.054103 ; Train Acc : 0.486 ; Test Loss : 0.062032 ; Test Acc : 0.500\n",
      "\n",
      "\n",
      "Epoch : 1 ; Train Loss : 0.137204 ; Train Acc : 0.110 ; Test Loss : 0.091145 ; Test Acc : 0.188 ; LR : 0.045\n",
      "Epoch : 2 ; Train Loss : 0.090219 ; Train Acc : 0.131 ; Test Loss : 0.089113 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 3 ; Train Loss : 0.087192 ; Train Acc : 0.179 ; Test Loss : 0.085749 ; Test Acc : 0.250 ; LR : 0.045\n",
      "Epoch : 4 ; Train Loss : 0.082988 ; Train Acc : 0.244 ; Test Loss : 0.082252 ; Test Acc : 0.312 ; LR : 0.045\n",
      "Epoch : 5 ; Train Loss : 0.079368 ; Train Acc : 0.297 ; Test Loss : 0.078298 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 6 ; Train Loss : 0.075419 ; Train Acc : 0.350 ; Test Loss : 0.074677 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 7 ; Train Loss : 0.072241 ; Train Acc : 0.375 ; Test Loss : 0.072242 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 8 ; Train Loss : 0.070280 ; Train Acc : 0.372 ; Test Loss : 0.069917 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 9 ; Train Loss : 0.068308 ; Train Acc : 0.378 ; Test Loss : 0.067927 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 10 ; Train Loss : 0.066944 ; Train Acc : 0.377 ; Test Loss : 0.066693 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 11 ; Train Loss : 0.065725 ; Train Acc : 0.385 ; Test Loss : 0.066011 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 12 ; Train Loss : 0.065131 ; Train Acc : 0.389 ; Test Loss : 0.065677 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 13 ; Train Loss : 0.065244 ; Train Acc : 0.387 ; Test Loss : 0.065278 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 14 ; Train Loss : 0.064735 ; Train Acc : 0.391 ; Test Loss : 0.065154 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 15 ; Train Loss : 0.064881 ; Train Acc : 0.392 ; Test Loss : 0.064919 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 16 ; Train Loss : 0.064386 ; Train Acc : 0.389 ; Test Loss : 0.064790 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 17 ; Train Loss : 0.064454 ; Train Acc : 0.392 ; Test Loss : 0.064680 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 18 ; Train Loss : 0.064046 ; Train Acc : 0.396 ; Test Loss : 0.064469 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 19 ; Train Loss : 0.063766 ; Train Acc : 0.390 ; Test Loss : 0.064497 ; Test Acc : 0.438 ; LR : 0.045\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 20 ; Train Loss : 0.063836 ; Train Acc : 0.392 ; Test Loss : 0.064616 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 21 ; Train Loss : 0.063905 ; Train Acc : 0.398 ; Test Loss : 0.064436 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 22 ; Train Loss : 0.063708 ; Train Acc : 0.395 ; Test Loss : 0.064202 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 23 ; Train Loss : 0.063901 ; Train Acc : 0.398 ; Test Loss : 0.064107 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 24 ; Train Loss : 0.063413 ; Train Acc : 0.391 ; Test Loss : 0.064150 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 25 ; Train Loss : 0.063515 ; Train Acc : 0.403 ; Test Loss : 0.064332 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 26 ; Train Loss : 0.063429 ; Train Acc : 0.397 ; Test Loss : 0.064060 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 27 ; Train Loss : 0.063440 ; Train Acc : 0.406 ; Test Loss : 0.064550 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 28 ; Train Loss : 0.063259 ; Train Acc : 0.396 ; Test Loss : 0.064021 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 29 ; Train Loss : 0.063442 ; Train Acc : 0.399 ; Test Loss : 0.063967 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 30 ; Train Loss : 0.063257 ; Train Acc : 0.394 ; Test Loss : 0.064119 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 31 ; Train Loss : 0.063281 ; Train Acc : 0.399 ; Test Loss : 0.063928 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 32 ; Train Loss : 0.062788 ; Train Acc : 0.396 ; Test Loss : 0.063928 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 33 ; Train Loss : 0.063087 ; Train Acc : 0.400 ; Test Loss : 0.063828 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 34 ; Train Loss : 0.063092 ; Train Acc : 0.396 ; Test Loss : 0.063798 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 35 ; Train Loss : 0.062796 ; Train Acc : 0.397 ; Test Loss : 0.063822 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 36 ; Train Loss : 0.063094 ; Train Acc : 0.399 ; Test Loss : 0.063749 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 37 ; Train Loss : 0.062924 ; Train Acc : 0.395 ; Test Loss : 0.064088 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 38 ; Train Loss : 0.063323 ; Train Acc : 0.400 ; Test Loss : 0.063884 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 39 ; Train Loss : 0.062987 ; Train Acc : 0.401 ; Test Loss : 0.063687 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 40 ; Train Loss : 0.063150 ; Train Acc : 0.400 ; Test Loss : 0.063691 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 41 ; Train Loss : 0.062833 ; Train Acc : 0.398 ; Test Loss : 0.064086 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 42 ; Train Loss : 0.062722 ; Train Acc : 0.408 ; Test Loss : 0.063774 ; Test Acc : 0.375 ; LR : 0.045\n",
      "Epoch : 43 ; Train Loss : 0.062666 ; Train Acc : 0.411 ; Test Loss : 0.063470 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 44 ; Train Loss : 0.062373 ; Train Acc : 0.425 ; Test Loss : 0.062753 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 45 ; Train Loss : 0.060288 ; Train Acc : 0.448 ; Test Loss : 0.060578 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 46 ; Train Loss : 0.057982 ; Train Acc : 0.469 ; Test Loss : 0.058938 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 47 ; Train Loss : 0.057022 ; Train Acc : 0.487 ; Test Loss : 0.058356 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 48 ; Train Loss : 0.056423 ; Train Acc : 0.485 ; Test Loss : 0.057908 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 49 ; Train Loss : 0.056418 ; Train Acc : 0.486 ; Test Loss : 0.057632 ; Test Acc : 0.438 ; LR : 0.045\n",
      "Epoch : 50 ; Train Loss : 0.056356 ; Train Acc : 0.488 ; Test Loss : 0.057390 ; Test Acc : 0.500 ; LR : 0.045\n",
      "Epoch : 51 ; Train Loss : 0.056174 ; Train Acc : 0.486 ; Test Loss : 0.057251 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 52 ; Train Loss : 0.056110 ; Train Acc : 0.485 ; Test Loss : 0.057575 ; Test Acc : 0.438 ; LR : 0.041\n",
      "Epoch : 53 ; Train Loss : 0.055906 ; Train Acc : 0.485 ; Test Loss : 0.057211 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 54 ; Train Loss : 0.055840 ; Train Acc : 0.489 ; Test Loss : 0.057242 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 55 ; Train Loss : 0.055743 ; Train Acc : 0.486 ; Test Loss : 0.056998 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 56 ; Train Loss : 0.055702 ; Train Acc : 0.487 ; Test Loss : 0.057067 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 57 ; Train Loss : 0.055927 ; Train Acc : 0.488 ; Test Loss : 0.056916 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 58 ; Train Loss : 0.055478 ; Train Acc : 0.486 ; Test Loss : 0.056937 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 59 ; Train Loss : 0.055477 ; Train Acc : 0.488 ; Test Loss : 0.057144 ; Test Acc : 0.438 ; LR : 0.041\n",
      "Epoch : 60 ; Train Loss : 0.055226 ; Train Acc : 0.487 ; Test Loss : 0.056796 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 61 ; Train Loss : 0.055582 ; Train Acc : 0.487 ; Test Loss : 0.056762 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 62 ; Train Loss : 0.055270 ; Train Acc : 0.488 ; Test Loss : 0.056873 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 63 ; Train Loss : 0.055410 ; Train Acc : 0.488 ; Test Loss : 0.056901 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 64 ; Train Loss : 0.055206 ; Train Acc : 0.488 ; Test Loss : 0.056874 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 65 ; Train Loss : 0.055214 ; Train Acc : 0.489 ; Test Loss : 0.056799 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 66 ; Train Loss : 0.055561 ; Train Acc : 0.489 ; Test Loss : 0.057096 ; Test Acc : 0.438 ; LR : 0.041\n",
      "Epoch : 67 ; Train Loss : 0.055153 ; Train Acc : 0.487 ; Test Loss : 0.056953 ; Test Acc : 0.562 ; LR : 0.041\n",
      "Epoch : 68 ; Train Loss : 0.055033 ; Train Acc : 0.489 ; Test Loss : 0.056765 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 69 ; Train Loss : 0.054836 ; Train Acc : 0.487 ; Test Loss : 0.056755 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 70 ; Train Loss : 0.055076 ; Train Acc : 0.489 ; Test Loss : 0.056639 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 71 ; Train Loss : 0.054455 ; Train Acc : 0.492 ; Test Loss : 0.056912 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 72 ; Train Loss : 0.055255 ; Train Acc : 0.481 ; Test Loss : 0.056964 ; Test Acc : 0.438 ; LR : 0.041\n",
      "Epoch : 73 ; Train Loss : 0.055234 ; Train Acc : 0.490 ; Test Loss : 0.056588 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 74 ; Train Loss : 0.054848 ; Train Acc : 0.490 ; Test Loss : 0.056691 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 75 ; Train Loss : 0.054812 ; Train Acc : 0.491 ; Test Loss : 0.056585 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 76 ; Train Loss : 0.054900 ; Train Acc : 0.490 ; Test Loss : 0.056707 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 77 ; Train Loss : 0.054735 ; Train Acc : 0.490 ; Test Loss : 0.056733 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 78 ; Train Loss : 0.054565 ; Train Acc : 0.489 ; Test Loss : 0.056676 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 79 ; Train Loss : 0.054551 ; Train Acc : 0.485 ; Test Loss : 0.056647 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 80 ; Train Loss : 0.054571 ; Train Acc : 0.492 ; Test Loss : 0.056721 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 81 ; Train Loss : 0.054659 ; Train Acc : 0.490 ; Test Loss : 0.056578 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 82 ; Train Loss : 0.054658 ; Train Acc : 0.491 ; Test Loss : 0.056584 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 83 ; Train Loss : 0.054472 ; Train Acc : 0.493 ; Test Loss : 0.056629 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 84 ; Train Loss : 0.054770 ; Train Acc : 0.492 ; Test Loss : 0.056513 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 85 ; Train Loss : 0.054591 ; Train Acc : 0.491 ; Test Loss : 0.057141 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 86 ; Train Loss : 0.054697 ; Train Acc : 0.490 ; Test Loss : 0.056669 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 87 ; Train Loss : 0.054434 ; Train Acc : 0.491 ; Test Loss : 0.056604 ; Test Acc : 0.438 ; LR : 0.041\n",
      "Epoch : 88 ; Train Loss : 0.054235 ; Train Acc : 0.492 ; Test Loss : 0.056583 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 89 ; Train Loss : 0.054449 ; Train Acc : 0.491 ; Test Loss : 0.056636 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 90 ; Train Loss : 0.054494 ; Train Acc : 0.491 ; Test Loss : 0.056574 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 91 ; Train Loss : 0.054631 ; Train Acc : 0.493 ; Test Loss : 0.056862 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 92 ; Train Loss : 0.054532 ; Train Acc : 0.494 ; Test Loss : 0.056618 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 93 ; Train Loss : 0.054328 ; Train Acc : 0.494 ; Test Loss : 0.056628 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 94 ; Train Loss : 0.054442 ; Train Acc : 0.481 ; Test Loss : 0.056498 ; Test Acc : 0.500 ; LR : 0.041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 95 ; Train Loss : 0.054207 ; Train Acc : 0.494 ; Test Loss : 0.056679 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 96 ; Train Loss : 0.054121 ; Train Acc : 0.495 ; Test Loss : 0.056600 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 97 ; Train Loss : 0.054676 ; Train Acc : 0.494 ; Test Loss : 0.056621 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 98 ; Train Loss : 0.054500 ; Train Acc : 0.492 ; Test Loss : 0.056574 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 99 ; Train Loss : 0.054086 ; Train Acc : 0.494 ; Test Loss : 0.056554 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 100 ; Train Loss : 0.054320 ; Train Acc : 0.494 ; Test Loss : 0.056598 ; Test Acc : 0.500 ; LR : 0.041\n",
      "Epoch : 101 ; Train Loss : 0.054608 ; Train Acc : 0.495 ; Test Loss : 0.056583 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 102 ; Train Loss : 0.054497 ; Train Acc : 0.495 ; Test Loss : 0.056558 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 103 ; Train Loss : 0.054126 ; Train Acc : 0.495 ; Test Loss : 0.056568 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 104 ; Train Loss : 0.053988 ; Train Acc : 0.495 ; Test Loss : 0.056520 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 105 ; Train Loss : 0.054266 ; Train Acc : 0.488 ; Test Loss : 0.056555 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 106 ; Train Loss : 0.054227 ; Train Acc : 0.493 ; Test Loss : 0.056559 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 107 ; Train Loss : 0.053893 ; Train Acc : 0.495 ; Test Loss : 0.056536 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 108 ; Train Loss : 0.054212 ; Train Acc : 0.494 ; Test Loss : 0.056570 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 109 ; Train Loss : 0.054125 ; Train Acc : 0.495 ; Test Loss : 0.056540 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 110 ; Train Loss : 0.054412 ; Train Acc : 0.488 ; Test Loss : 0.056865 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 111 ; Train Loss : 0.054444 ; Train Acc : 0.495 ; Test Loss : 0.056491 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 112 ; Train Loss : 0.054009 ; Train Acc : 0.495 ; Test Loss : 0.056592 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 113 ; Train Loss : 0.054305 ; Train Acc : 0.495 ; Test Loss : 0.056737 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 114 ; Train Loss : 0.054263 ; Train Acc : 0.496 ; Test Loss : 0.056798 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 115 ; Train Loss : 0.053997 ; Train Acc : 0.496 ; Test Loss : 0.056605 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 116 ; Train Loss : 0.054099 ; Train Acc : 0.494 ; Test Loss : 0.056658 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 117 ; Train Loss : 0.054293 ; Train Acc : 0.495 ; Test Loss : 0.056704 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 118 ; Train Loss : 0.054415 ; Train Acc : 0.496 ; Test Loss : 0.056909 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 119 ; Train Loss : 0.053769 ; Train Acc : 0.500 ; Test Loss : 0.056707 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 120 ; Train Loss : 0.054010 ; Train Acc : 0.497 ; Test Loss : 0.056586 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 121 ; Train Loss : 0.053824 ; Train Acc : 0.493 ; Test Loss : 0.056707 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 122 ; Train Loss : 0.053830 ; Train Acc : 0.496 ; Test Loss : 0.056687 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 123 ; Train Loss : 0.054066 ; Train Acc : 0.496 ; Test Loss : 0.056677 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 124 ; Train Loss : 0.053973 ; Train Acc : 0.497 ; Test Loss : 0.056738 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 125 ; Train Loss : 0.053850 ; Train Acc : 0.497 ; Test Loss : 0.056707 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 126 ; Train Loss : 0.053914 ; Train Acc : 0.497 ; Test Loss : 0.056754 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 127 ; Train Loss : 0.053873 ; Train Acc : 0.498 ; Test Loss : 0.057129 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 128 ; Train Loss : 0.053779 ; Train Acc : 0.497 ; Test Loss : 0.056690 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 129 ; Train Loss : 0.053830 ; Train Acc : 0.491 ; Test Loss : 0.056857 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 130 ; Train Loss : 0.054299 ; Train Acc : 0.498 ; Test Loss : 0.057124 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 131 ; Train Loss : 0.054085 ; Train Acc : 0.497 ; Test Loss : 0.056885 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 132 ; Train Loss : 0.054026 ; Train Acc : 0.498 ; Test Loss : 0.056659 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 133 ; Train Loss : 0.053644 ; Train Acc : 0.496 ; Test Loss : 0.056782 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 134 ; Train Loss : 0.053912 ; Train Acc : 0.497 ; Test Loss : 0.056672 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 135 ; Train Loss : 0.053888 ; Train Acc : 0.491 ; Test Loss : 0.056800 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 136 ; Train Loss : 0.053513 ; Train Acc : 0.494 ; Test Loss : 0.056666 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 137 ; Train Loss : 0.053817 ; Train Acc : 0.498 ; Test Loss : 0.056681 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 138 ; Train Loss : 0.053725 ; Train Acc : 0.497 ; Test Loss : 0.056721 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 139 ; Train Loss : 0.053536 ; Train Acc : 0.499 ; Test Loss : 0.056888 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 140 ; Train Loss : 0.054059 ; Train Acc : 0.491 ; Test Loss : 0.056785 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 141 ; Train Loss : 0.054089 ; Train Acc : 0.498 ; Test Loss : 0.056861 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 142 ; Train Loss : 0.054100 ; Train Acc : 0.498 ; Test Loss : 0.057422 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 143 ; Train Loss : 0.054004 ; Train Acc : 0.499 ; Test Loss : 0.057167 ; Test Acc : 0.500 ; LR : 0.036\n",
      "Epoch : 144 ; Train Loss : 0.053884 ; Train Acc : 0.498 ; Test Loss : 0.056696 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 145 ; Train Loss : 0.053671 ; Train Acc : 0.501 ; Test Loss : 0.056764 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 146 ; Train Loss : 0.053569 ; Train Acc : 0.497 ; Test Loss : 0.057025 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 147 ; Train Loss : 0.053927 ; Train Acc : 0.498 ; Test Loss : 0.056812 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 148 ; Train Loss : 0.053768 ; Train Acc : 0.497 ; Test Loss : 0.056894 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 149 ; Train Loss : 0.053636 ; Train Acc : 0.499 ; Test Loss : 0.056804 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 150 ; Train Loss : 0.053727 ; Train Acc : 0.498 ; Test Loss : 0.056826 ; Test Acc : 0.438 ; LR : 0.036\n",
      "Epoch : 151 ; Train Loss : 0.053796 ; Train Acc : 0.499 ; Test Loss : 0.056979 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 152 ; Train Loss : 0.053548 ; Train Acc : 0.499 ; Test Loss : 0.056804 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 153 ; Train Loss : 0.053501 ; Train Acc : 0.498 ; Test Loss : 0.056992 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 154 ; Train Loss : 0.053916 ; Train Acc : 0.500 ; Test Loss : 0.056952 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 155 ; Train Loss : 0.053905 ; Train Acc : 0.498 ; Test Loss : 0.056935 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 156 ; Train Loss : 0.053748 ; Train Acc : 0.499 ; Test Loss : 0.056991 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 157 ; Train Loss : 0.053555 ; Train Acc : 0.499 ; Test Loss : 0.057114 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 158 ; Train Loss : 0.053496 ; Train Acc : 0.498 ; Test Loss : 0.056931 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 159 ; Train Loss : 0.053620 ; Train Acc : 0.498 ; Test Loss : 0.056818 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 160 ; Train Loss : 0.053428 ; Train Acc : 0.499 ; Test Loss : 0.056821 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 161 ; Train Loss : 0.053528 ; Train Acc : 0.499 ; Test Loss : 0.056911 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 162 ; Train Loss : 0.053527 ; Train Acc : 0.502 ; Test Loss : 0.056795 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 163 ; Train Loss : 0.053468 ; Train Acc : 0.499 ; Test Loss : 0.056932 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 164 ; Train Loss : 0.053710 ; Train Acc : 0.499 ; Test Loss : 0.056928 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 165 ; Train Loss : 0.053339 ; Train Acc : 0.498 ; Test Loss : 0.056881 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 166 ; Train Loss : 0.053524 ; Train Acc : 0.487 ; Test Loss : 0.056881 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 167 ; Train Loss : 0.053553 ; Train Acc : 0.499 ; Test Loss : 0.057098 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 168 ; Train Loss : 0.053681 ; Train Acc : 0.499 ; Test Loss : 0.056774 ; Test Acc : 0.438 ; LR : 0.033\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 169 ; Train Loss : 0.053557 ; Train Acc : 0.499 ; Test Loss : 0.056870 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 170 ; Train Loss : 0.053654 ; Train Acc : 0.501 ; Test Loss : 0.057066 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 171 ; Train Loss : 0.053423 ; Train Acc : 0.494 ; Test Loss : 0.056824 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 172 ; Train Loss : 0.053587 ; Train Acc : 0.499 ; Test Loss : 0.056826 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 173 ; Train Loss : 0.053124 ; Train Acc : 0.499 ; Test Loss : 0.056885 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 174 ; Train Loss : 0.053348 ; Train Acc : 0.499 ; Test Loss : 0.056969 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 175 ; Train Loss : 0.053504 ; Train Acc : 0.500 ; Test Loss : 0.056961 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 176 ; Train Loss : 0.053182 ; Train Acc : 0.500 ; Test Loss : 0.056902 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 177 ; Train Loss : 0.053703 ; Train Acc : 0.499 ; Test Loss : 0.057027 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 178 ; Train Loss : 0.053465 ; Train Acc : 0.500 ; Test Loss : 0.057013 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 179 ; Train Loss : 0.053278 ; Train Acc : 0.500 ; Test Loss : 0.056872 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 180 ; Train Loss : 0.053331 ; Train Acc : 0.500 ; Test Loss : 0.057058 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 181 ; Train Loss : 0.053374 ; Train Acc : 0.499 ; Test Loss : 0.056953 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 182 ; Train Loss : 0.053478 ; Train Acc : 0.500 ; Test Loss : 0.057037 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 183 ; Train Loss : 0.053271 ; Train Acc : 0.501 ; Test Loss : 0.056974 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 184 ; Train Loss : 0.053238 ; Train Acc : 0.501 ; Test Loss : 0.057007 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 185 ; Train Loss : 0.053691 ; Train Acc : 0.501 ; Test Loss : 0.056996 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 186 ; Train Loss : 0.053329 ; Train Acc : 0.500 ; Test Loss : 0.057255 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 187 ; Train Loss : 0.053325 ; Train Acc : 0.501 ; Test Loss : 0.056901 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 188 ; Train Loss : 0.053154 ; Train Acc : 0.502 ; Test Loss : 0.056934 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 189 ; Train Loss : 0.053433 ; Train Acc : 0.501 ; Test Loss : 0.056933 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 190 ; Train Loss : 0.053114 ; Train Acc : 0.502 ; Test Loss : 0.057096 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 191 ; Train Loss : 0.053353 ; Train Acc : 0.500 ; Test Loss : 0.057073 ; Test Acc : 0.500 ; LR : 0.033\n",
      "Epoch : 192 ; Train Loss : 0.053265 ; Train Acc : 0.501 ; Test Loss : 0.056979 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 193 ; Train Loss : 0.053439 ; Train Acc : 0.501 ; Test Loss : 0.057188 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 194 ; Train Loss : 0.053263 ; Train Acc : 0.500 ; Test Loss : 0.057062 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 195 ; Train Loss : 0.053278 ; Train Acc : 0.489 ; Test Loss : 0.056959 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 196 ; Train Loss : 0.053318 ; Train Acc : 0.500 ; Test Loss : 0.057199 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 197 ; Train Loss : 0.053324 ; Train Acc : 0.501 ; Test Loss : 0.057016 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 198 ; Train Loss : 0.053123 ; Train Acc : 0.502 ; Test Loss : 0.057019 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 199 ; Train Loss : 0.052838 ; Train Acc : 0.501 ; Test Loss : 0.056997 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 200 ; Train Loss : 0.053476 ; Train Acc : 0.501 ; Test Loss : 0.056993 ; Test Acc : 0.438 ; LR : 0.033\n",
      "Epoch : 201 ; Train Loss : 0.053044 ; Train Acc : 0.501 ; Test Loss : 0.057024 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 202 ; Train Loss : 0.053287 ; Train Acc : 0.501 ; Test Loss : 0.057537 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 203 ; Train Loss : 0.053259 ; Train Acc : 0.500 ; Test Loss : 0.057128 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 204 ; Train Loss : 0.053210 ; Train Acc : 0.501 ; Test Loss : 0.057109 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 205 ; Train Loss : 0.053511 ; Train Acc : 0.502 ; Test Loss : 0.057057 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 206 ; Train Loss : 0.053185 ; Train Acc : 0.501 ; Test Loss : 0.057101 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 207 ; Train Loss : 0.053016 ; Train Acc : 0.501 ; Test Loss : 0.057129 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 208 ; Train Loss : 0.052894 ; Train Acc : 0.501 ; Test Loss : 0.057376 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 209 ; Train Loss : 0.053284 ; Train Acc : 0.501 ; Test Loss : 0.057198 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 210 ; Train Loss : 0.053354 ; Train Acc : 0.502 ; Test Loss : 0.057213 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 211 ; Train Loss : 0.053128 ; Train Acc : 0.501 ; Test Loss : 0.056989 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 212 ; Train Loss : 0.053254 ; Train Acc : 0.493 ; Test Loss : 0.057574 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 213 ; Train Loss : 0.053236 ; Train Acc : 0.502 ; Test Loss : 0.057027 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 214 ; Train Loss : 0.053209 ; Train Acc : 0.502 ; Test Loss : 0.057079 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 215 ; Train Loss : 0.053203 ; Train Acc : 0.501 ; Test Loss : 0.057065 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 216 ; Train Loss : 0.053086 ; Train Acc : 0.502 ; Test Loss : 0.057218 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 217 ; Train Loss : 0.053251 ; Train Acc : 0.502 ; Test Loss : 0.057052 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 218 ; Train Loss : 0.052711 ; Train Acc : 0.501 ; Test Loss : 0.057140 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 219 ; Train Loss : 0.052814 ; Train Acc : 0.501 ; Test Loss : 0.057086 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 220 ; Train Loss : 0.052859 ; Train Acc : 0.502 ; Test Loss : 0.057215 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 221 ; Train Loss : 0.053298 ; Train Acc : 0.501 ; Test Loss : 0.057234 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 222 ; Train Loss : 0.053094 ; Train Acc : 0.501 ; Test Loss : 0.057506 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 223 ; Train Loss : 0.053506 ; Train Acc : 0.495 ; Test Loss : 0.057148 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 224 ; Train Loss : 0.052951 ; Train Acc : 0.502 ; Test Loss : 0.057094 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 225 ; Train Loss : 0.053010 ; Train Acc : 0.501 ; Test Loss : 0.057200 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 226 ; Train Loss : 0.053286 ; Train Acc : 0.502 ; Test Loss : 0.057189 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 227 ; Train Loss : 0.052968 ; Train Acc : 0.502 ; Test Loss : 0.057266 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 228 ; Train Loss : 0.053118 ; Train Acc : 0.502 ; Test Loss : 0.057378 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 229 ; Train Loss : 0.053260 ; Train Acc : 0.502 ; Test Loss : 0.057137 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 230 ; Train Loss : 0.052751 ; Train Acc : 0.502 ; Test Loss : 0.057137 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 231 ; Train Loss : 0.052579 ; Train Acc : 0.502 ; Test Loss : 0.057215 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 232 ; Train Loss : 0.052887 ; Train Acc : 0.502 ; Test Loss : 0.057347 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 233 ; Train Loss : 0.052775 ; Train Acc : 0.503 ; Test Loss : 0.057152 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 234 ; Train Loss : 0.053120 ; Train Acc : 0.502 ; Test Loss : 0.057198 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 235 ; Train Loss : 0.053127 ; Train Acc : 0.503 ; Test Loss : 0.057273 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 236 ; Train Loss : 0.053391 ; Train Acc : 0.502 ; Test Loss : 0.057238 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 237 ; Train Loss : 0.053054 ; Train Acc : 0.503 ; Test Loss : 0.057260 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 238 ; Train Loss : 0.052933 ; Train Acc : 0.503 ; Test Loss : 0.057445 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 239 ; Train Loss : 0.052915 ; Train Acc : 0.501 ; Test Loss : 0.057183 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 240 ; Train Loss : 0.052621 ; Train Acc : 0.503 ; Test Loss : 0.057251 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 241 ; Train Loss : 0.052600 ; Train Acc : 0.501 ; Test Loss : 0.057556 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 242 ; Train Loss : 0.053008 ; Train Acc : 0.501 ; Test Loss : 0.057356 ; Test Acc : 0.438 ; LR : 0.030\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 243 ; Train Loss : 0.052992 ; Train Acc : 0.502 ; Test Loss : 0.057326 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 244 ; Train Loss : 0.053114 ; Train Acc : 0.503 ; Test Loss : 0.057751 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 245 ; Train Loss : 0.053049 ; Train Acc : 0.502 ; Test Loss : 0.057240 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 246 ; Train Loss : 0.053048 ; Train Acc : 0.502 ; Test Loss : 0.057318 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 247 ; Train Loss : 0.052652 ; Train Acc : 0.492 ; Test Loss : 0.057255 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 248 ; Train Loss : 0.052885 ; Train Acc : 0.503 ; Test Loss : 0.057329 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 249 ; Train Loss : 0.053296 ; Train Acc : 0.504 ; Test Loss : 0.057461 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 250 ; Train Loss : 0.053009 ; Train Acc : 0.501 ; Test Loss : 0.057266 ; Test Acc : 0.438 ; LR : 0.030\n",
      "Epoch : 251 ; Train Loss : 0.052770 ; Train Acc : 0.503 ; Test Loss : 0.057227 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 252 ; Train Loss : 0.052711 ; Train Acc : 0.503 ; Test Loss : 0.057246 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 253 ; Train Loss : 0.052832 ; Train Acc : 0.504 ; Test Loss : 0.057420 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 254 ; Train Loss : 0.052878 ; Train Acc : 0.503 ; Test Loss : 0.057414 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 255 ; Train Loss : 0.052897 ; Train Acc : 0.504 ; Test Loss : 0.057400 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 256 ; Train Loss : 0.052941 ; Train Acc : 0.501 ; Test Loss : 0.057323 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 257 ; Train Loss : 0.052759 ; Train Acc : 0.504 ; Test Loss : 0.057435 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 258 ; Train Loss : 0.053023 ; Train Acc : 0.503 ; Test Loss : 0.057269 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 259 ; Train Loss : 0.052890 ; Train Acc : 0.504 ; Test Loss : 0.057361 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 260 ; Train Loss : 0.053063 ; Train Acc : 0.503 ; Test Loss : 0.057630 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 261 ; Train Loss : 0.052694 ; Train Acc : 0.503 ; Test Loss : 0.057283 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 262 ; Train Loss : 0.052810 ; Train Acc : 0.503 ; Test Loss : 0.057323 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 263 ; Train Loss : 0.053079 ; Train Acc : 0.502 ; Test Loss : 0.057583 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 264 ; Train Loss : 0.053019 ; Train Acc : 0.504 ; Test Loss : 0.057420 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 265 ; Train Loss : 0.052783 ; Train Acc : 0.502 ; Test Loss : 0.057367 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 266 ; Train Loss : 0.052896 ; Train Acc : 0.503 ; Test Loss : 0.057679 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 267 ; Train Loss : 0.052822 ; Train Acc : 0.503 ; Test Loss : 0.057454 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 268 ; Train Loss : 0.052800 ; Train Acc : 0.504 ; Test Loss : 0.057335 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 269 ; Train Loss : 0.052806 ; Train Acc : 0.502 ; Test Loss : 0.057391 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 270 ; Train Loss : 0.052809 ; Train Acc : 0.503 ; Test Loss : 0.057479 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 271 ; Train Loss : 0.053153 ; Train Acc : 0.503 ; Test Loss : 0.057446 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 272 ; Train Loss : 0.052818 ; Train Acc : 0.503 ; Test Loss : 0.057405 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 273 ; Train Loss : 0.052467 ; Train Acc : 0.498 ; Test Loss : 0.057380 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 274 ; Train Loss : 0.052782 ; Train Acc : 0.503 ; Test Loss : 0.057578 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 275 ; Train Loss : 0.052742 ; Train Acc : 0.503 ; Test Loss : 0.057365 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 276 ; Train Loss : 0.052726 ; Train Acc : 0.503 ; Test Loss : 0.057507 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 277 ; Train Loss : 0.052862 ; Train Acc : 0.503 ; Test Loss : 0.057597 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 278 ; Train Loss : 0.052600 ; Train Acc : 0.504 ; Test Loss : 0.057427 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 279 ; Train Loss : 0.052896 ; Train Acc : 0.503 ; Test Loss : 0.057423 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 280 ; Train Loss : 0.053079 ; Train Acc : 0.503 ; Test Loss : 0.057527 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 281 ; Train Loss : 0.052574 ; Train Acc : 0.502 ; Test Loss : 0.057389 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 282 ; Train Loss : 0.052594 ; Train Acc : 0.503 ; Test Loss : 0.057497 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 283 ; Train Loss : 0.052740 ; Train Acc : 0.503 ; Test Loss : 0.057520 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 284 ; Train Loss : 0.052833 ; Train Acc : 0.504 ; Test Loss : 0.057634 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 285 ; Train Loss : 0.052790 ; Train Acc : 0.503 ; Test Loss : 0.057469 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 286 ; Train Loss : 0.053060 ; Train Acc : 0.502 ; Test Loss : 0.057803 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 287 ; Train Loss : 0.052811 ; Train Acc : 0.503 ; Test Loss : 0.057425 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 288 ; Train Loss : 0.053061 ; Train Acc : 0.504 ; Test Loss : 0.057575 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 289 ; Train Loss : 0.052554 ; Train Acc : 0.504 ; Test Loss : 0.057725 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 290 ; Train Loss : 0.052985 ; Train Acc : 0.503 ; Test Loss : 0.057454 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 291 ; Train Loss : 0.052794 ; Train Acc : 0.504 ; Test Loss : 0.057485 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 292 ; Train Loss : 0.052986 ; Train Acc : 0.503 ; Test Loss : 0.057531 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 293 ; Train Loss : 0.052789 ; Train Acc : 0.503 ; Test Loss : 0.057482 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 294 ; Train Loss : 0.052889 ; Train Acc : 0.504 ; Test Loss : 0.057656 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 295 ; Train Loss : 0.052703 ; Train Acc : 0.504 ; Test Loss : 0.057512 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 296 ; Train Loss : 0.052731 ; Train Acc : 0.503 ; Test Loss : 0.057488 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 297 ; Train Loss : 0.052868 ; Train Acc : 0.502 ; Test Loss : 0.057564 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 298 ; Train Loss : 0.052806 ; Train Acc : 0.502 ; Test Loss : 0.057790 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 299 ; Train Loss : 0.052775 ; Train Acc : 0.504 ; Test Loss : 0.057641 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 300 ; Train Loss : 0.052791 ; Train Acc : 0.503 ; Test Loss : 0.057554 ; Test Acc : 0.438 ; LR : 0.027\n",
      "Epoch : 301 ; Train Loss : 0.052436 ; Train Acc : 0.502 ; Test Loss : 0.057471 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 302 ; Train Loss : 0.052741 ; Train Acc : 0.505 ; Test Loss : 0.057780 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 303 ; Train Loss : 0.052822 ; Train Acc : 0.503 ; Test Loss : 0.057507 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 304 ; Train Loss : 0.052623 ; Train Acc : 0.504 ; Test Loss : 0.057511 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 305 ; Train Loss : 0.052570 ; Train Acc : 0.503 ; Test Loss : 0.057574 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 306 ; Train Loss : 0.052712 ; Train Acc : 0.500 ; Test Loss : 0.057679 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 307 ; Train Loss : 0.052601 ; Train Acc : 0.504 ; Test Loss : 0.057736 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 308 ; Train Loss : 0.052404 ; Train Acc : 0.504 ; Test Loss : 0.057561 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 309 ; Train Loss : 0.052562 ; Train Acc : 0.503 ; Test Loss : 0.057721 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 310 ; Train Loss : 0.052457 ; Train Acc : 0.503 ; Test Loss : 0.057543 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 311 ; Train Loss : 0.052310 ; Train Acc : 0.503 ; Test Loss : 0.057573 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 312 ; Train Loss : 0.052635 ; Train Acc : 0.503 ; Test Loss : 0.057731 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 313 ; Train Loss : 0.052802 ; Train Acc : 0.503 ; Test Loss : 0.057623 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 314 ; Train Loss : 0.052734 ; Train Acc : 0.504 ; Test Loss : 0.057803 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 315 ; Train Loss : 0.052706 ; Train Acc : 0.505 ; Test Loss : 0.057971 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 316 ; Train Loss : 0.052620 ; Train Acc : 0.504 ; Test Loss : 0.057672 ; Test Acc : 0.438 ; LR : 0.024\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 317 ; Train Loss : 0.052510 ; Train Acc : 0.503 ; Test Loss : 0.057720 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 318 ; Train Loss : 0.052547 ; Train Acc : 0.494 ; Test Loss : 0.057619 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 319 ; Train Loss : 0.052956 ; Train Acc : 0.503 ; Test Loss : 0.057638 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 320 ; Train Loss : 0.052598 ; Train Acc : 0.504 ; Test Loss : 0.057860 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 321 ; Train Loss : 0.052512 ; Train Acc : 0.503 ; Test Loss : 0.057580 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 322 ; Train Loss : 0.053023 ; Train Acc : 0.503 ; Test Loss : 0.057612 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 323 ; Train Loss : 0.052633 ; Train Acc : 0.504 ; Test Loss : 0.057757 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 324 ; Train Loss : 0.052437 ; Train Acc : 0.504 ; Test Loss : 0.057669 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 325 ; Train Loss : 0.052541 ; Train Acc : 0.505 ; Test Loss : 0.057930 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 326 ; Train Loss : 0.052643 ; Train Acc : 0.494 ; Test Loss : 0.057663 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 327 ; Train Loss : 0.052665 ; Train Acc : 0.504 ; Test Loss : 0.057805 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 328 ; Train Loss : 0.052741 ; Train Acc : 0.503 ; Test Loss : 0.057916 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 329 ; Train Loss : 0.052549 ; Train Acc : 0.503 ; Test Loss : 0.057749 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 330 ; Train Loss : 0.052536 ; Train Acc : 0.505 ; Test Loss : 0.057682 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 331 ; Train Loss : 0.052318 ; Train Acc : 0.504 ; Test Loss : 0.057816 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 332 ; Train Loss : 0.052645 ; Train Acc : 0.504 ; Test Loss : 0.057650 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 333 ; Train Loss : 0.052451 ; Train Acc : 0.503 ; Test Loss : 0.057720 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 334 ; Train Loss : 0.052413 ; Train Acc : 0.504 ; Test Loss : 0.057899 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 335 ; Train Loss : 0.052386 ; Train Acc : 0.504 ; Test Loss : 0.057706 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 336 ; Train Loss : 0.052568 ; Train Acc : 0.504 ; Test Loss : 0.057892 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 337 ; Train Loss : 0.052471 ; Train Acc : 0.504 ; Test Loss : 0.057757 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 338 ; Train Loss : 0.052467 ; Train Acc : 0.505 ; Test Loss : 0.057726 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 339 ; Train Loss : 0.052493 ; Train Acc : 0.503 ; Test Loss : 0.057801 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 340 ; Train Loss : 0.052667 ; Train Acc : 0.504 ; Test Loss : 0.058024 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 341 ; Train Loss : 0.052492 ; Train Acc : 0.504 ; Test Loss : 0.057706 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 342 ; Train Loss : 0.053120 ; Train Acc : 0.504 ; Test Loss : 0.057744 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 343 ; Train Loss : 0.052773 ; Train Acc : 0.503 ; Test Loss : 0.057802 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 344 ; Train Loss : 0.052317 ; Train Acc : 0.504 ; Test Loss : 0.057796 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 345 ; Train Loss : 0.052679 ; Train Acc : 0.503 ; Test Loss : 0.057817 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 346 ; Train Loss : 0.052505 ; Train Acc : 0.503 ; Test Loss : 0.057781 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 347 ; Train Loss : 0.052555 ; Train Acc : 0.504 ; Test Loss : 0.057789 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 348 ; Train Loss : 0.052599 ; Train Acc : 0.504 ; Test Loss : 0.057829 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 349 ; Train Loss : 0.052580 ; Train Acc : 0.504 ; Test Loss : 0.057760 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 350 ; Train Loss : 0.052510 ; Train Acc : 0.503 ; Test Loss : 0.057849 ; Test Acc : 0.438 ; LR : 0.024\n",
      "Epoch : 351 ; Train Loss : 0.052572 ; Train Acc : 0.504 ; Test Loss : 0.057917 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 352 ; Train Loss : 0.052376 ; Train Acc : 0.504 ; Test Loss : 0.057785 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 353 ; Train Loss : 0.052450 ; Train Acc : 0.503 ; Test Loss : 0.057772 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 354 ; Train Loss : 0.052267 ; Train Acc : 0.505 ; Test Loss : 0.057797 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 355 ; Train Loss : 0.052358 ; Train Acc : 0.504 ; Test Loss : 0.057837 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 356 ; Train Loss : 0.052496 ; Train Acc : 0.503 ; Test Loss : 0.057782 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 357 ; Train Loss : 0.052487 ; Train Acc : 0.504 ; Test Loss : 0.057792 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 358 ; Train Loss : 0.052461 ; Train Acc : 0.504 ; Test Loss : 0.057935 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 359 ; Train Loss : 0.052530 ; Train Acc : 0.505 ; Test Loss : 0.057781 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 360 ; Train Loss : 0.052312 ; Train Acc : 0.505 ; Test Loss : 0.057895 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 361 ; Train Loss : 0.052684 ; Train Acc : 0.505 ; Test Loss : 0.057981 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 362 ; Train Loss : 0.052233 ; Train Acc : 0.503 ; Test Loss : 0.057832 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 363 ; Train Loss : 0.052154 ; Train Acc : 0.504 ; Test Loss : 0.057904 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 364 ; Train Loss : 0.052692 ; Train Acc : 0.504 ; Test Loss : 0.057892 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 365 ; Train Loss : 0.052311 ; Train Acc : 0.504 ; Test Loss : 0.057896 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 366 ; Train Loss : 0.052310 ; Train Acc : 0.504 ; Test Loss : 0.057956 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 367 ; Train Loss : 0.052361 ; Train Acc : 0.505 ; Test Loss : 0.057837 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 368 ; Train Loss : 0.051976 ; Train Acc : 0.504 ; Test Loss : 0.057800 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 369 ; Train Loss : 0.052343 ; Train Acc : 0.505 ; Test Loss : 0.057819 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 370 ; Train Loss : 0.052556 ; Train Acc : 0.504 ; Test Loss : 0.058159 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 371 ; Train Loss : 0.052541 ; Train Acc : 0.505 ; Test Loss : 0.058193 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 372 ; Train Loss : 0.052249 ; Train Acc : 0.504 ; Test Loss : 0.057818 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 373 ; Train Loss : 0.052373 ; Train Acc : 0.505 ; Test Loss : 0.058033 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 374 ; Train Loss : 0.052410 ; Train Acc : 0.504 ; Test Loss : 0.057890 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 375 ; Train Loss : 0.052263 ; Train Acc : 0.503 ; Test Loss : 0.057877 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 376 ; Train Loss : 0.052382 ; Train Acc : 0.505 ; Test Loss : 0.057869 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 377 ; Train Loss : 0.052431 ; Train Acc : 0.504 ; Test Loss : 0.057854 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 378 ; Train Loss : 0.052074 ; Train Acc : 0.504 ; Test Loss : 0.057904 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 379 ; Train Loss : 0.052359 ; Train Acc : 0.505 ; Test Loss : 0.057991 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 380 ; Train Loss : 0.052264 ; Train Acc : 0.504 ; Test Loss : 0.058001 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 381 ; Train Loss : 0.052262 ; Train Acc : 0.503 ; Test Loss : 0.058056 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 382 ; Train Loss : 0.052328 ; Train Acc : 0.505 ; Test Loss : 0.057884 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 383 ; Train Loss : 0.052421 ; Train Acc : 0.504 ; Test Loss : 0.058204 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 384 ; Train Loss : 0.052658 ; Train Acc : 0.505 ; Test Loss : 0.057908 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 385 ; Train Loss : 0.052536 ; Train Acc : 0.504 ; Test Loss : 0.057885 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 386 ; Train Loss : 0.052288 ; Train Acc : 0.506 ; Test Loss : 0.057915 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 387 ; Train Loss : 0.052343 ; Train Acc : 0.504 ; Test Loss : 0.057907 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 388 ; Train Loss : 0.052058 ; Train Acc : 0.505 ; Test Loss : 0.057999 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 389 ; Train Loss : 0.052444 ; Train Acc : 0.505 ; Test Loss : 0.058146 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 390 ; Train Loss : 0.052323 ; Train Acc : 0.505 ; Test Loss : 0.057991 ; Test Acc : 0.438 ; LR : 0.022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 391 ; Train Loss : 0.052128 ; Train Acc : 0.505 ; Test Loss : 0.058027 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 392 ; Train Loss : 0.052172 ; Train Acc : 0.504 ; Test Loss : 0.057952 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 393 ; Train Loss : 0.052574 ; Train Acc : 0.505 ; Test Loss : 0.058200 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 394 ; Train Loss : 0.052306 ; Train Acc : 0.505 ; Test Loss : 0.057935 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 395 ; Train Loss : 0.052525 ; Train Acc : 0.504 ; Test Loss : 0.058012 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 396 ; Train Loss : 0.052319 ; Train Acc : 0.505 ; Test Loss : 0.058104 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 397 ; Train Loss : 0.052675 ; Train Acc : 0.504 ; Test Loss : 0.057972 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 398 ; Train Loss : 0.052279 ; Train Acc : 0.505 ; Test Loss : 0.057950 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 399 ; Train Loss : 0.052090 ; Train Acc : 0.504 ; Test Loss : 0.058107 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 400 ; Train Loss : 0.052350 ; Train Acc : 0.505 ; Test Loss : 0.058047 ; Test Acc : 0.438 ; LR : 0.022\n",
      "Epoch : 401 ; Train Loss : 0.051801 ; Train Acc : 0.504 ; Test Loss : 0.058088 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 402 ; Train Loss : 0.052223 ; Train Acc : 0.504 ; Test Loss : 0.058167 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 403 ; Train Loss : 0.052468 ; Train Acc : 0.504 ; Test Loss : 0.058069 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 404 ; Train Loss : 0.052357 ; Train Acc : 0.504 ; Test Loss : 0.058149 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 405 ; Train Loss : 0.052226 ; Train Acc : 0.504 ; Test Loss : 0.058105 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 406 ; Train Loss : 0.052309 ; Train Acc : 0.505 ; Test Loss : 0.058316 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 407 ; Train Loss : 0.052407 ; Train Acc : 0.504 ; Test Loss : 0.058026 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 408 ; Train Loss : 0.052596 ; Train Acc : 0.504 ; Test Loss : 0.058054 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 409 ; Train Loss : 0.052191 ; Train Acc : 0.505 ; Test Loss : 0.058259 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 410 ; Train Loss : 0.052432 ; Train Acc : 0.505 ; Test Loss : 0.058050 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 411 ; Train Loss : 0.052280 ; Train Acc : 0.497 ; Test Loss : 0.058055 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 412 ; Train Loss : 0.052236 ; Train Acc : 0.505 ; Test Loss : 0.058155 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 413 ; Train Loss : 0.052525 ; Train Acc : 0.505 ; Test Loss : 0.058059 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 414 ; Train Loss : 0.052186 ; Train Acc : 0.504 ; Test Loss : 0.058114 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 415 ; Train Loss : 0.052550 ; Train Acc : 0.506 ; Test Loss : 0.058112 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 416 ; Train Loss : 0.052390 ; Train Acc : 0.506 ; Test Loss : 0.058151 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 417 ; Train Loss : 0.052433 ; Train Acc : 0.505 ; Test Loss : 0.058211 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 418 ; Train Loss : 0.052120 ; Train Acc : 0.505 ; Test Loss : 0.058192 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 419 ; Train Loss : 0.052326 ; Train Acc : 0.505 ; Test Loss : 0.058204 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 420 ; Train Loss : 0.052055 ; Train Acc : 0.505 ; Test Loss : 0.058372 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 421 ; Train Loss : 0.052393 ; Train Acc : 0.505 ; Test Loss : 0.058142 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 422 ; Train Loss : 0.052021 ; Train Acc : 0.505 ; Test Loss : 0.058118 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 423 ; Train Loss : 0.052190 ; Train Acc : 0.505 ; Test Loss : 0.058364 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 424 ; Train Loss : 0.052371 ; Train Acc : 0.505 ; Test Loss : 0.058326 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 425 ; Train Loss : 0.052470 ; Train Acc : 0.505 ; Test Loss : 0.058171 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 426 ; Train Loss : 0.052276 ; Train Acc : 0.505 ; Test Loss : 0.058165 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 427 ; Train Loss : 0.052180 ; Train Acc : 0.505 ; Test Loss : 0.058248 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 428 ; Train Loss : 0.052076 ; Train Acc : 0.505 ; Test Loss : 0.058137 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 429 ; Train Loss : 0.052077 ; Train Acc : 0.506 ; Test Loss : 0.058227 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 430 ; Train Loss : 0.052180 ; Train Acc : 0.506 ; Test Loss : 0.058144 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 431 ; Train Loss : 0.052563 ; Train Acc : 0.505 ; Test Loss : 0.058167 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 432 ; Train Loss : 0.052354 ; Train Acc : 0.505 ; Test Loss : 0.058331 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 433 ; Train Loss : 0.052274 ; Train Acc : 0.505 ; Test Loss : 0.058141 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 434 ; Train Loss : 0.052400 ; Train Acc : 0.506 ; Test Loss : 0.058637 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 435 ; Train Loss : 0.052328 ; Train Acc : 0.504 ; Test Loss : 0.058229 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 436 ; Train Loss : 0.052134 ; Train Acc : 0.505 ; Test Loss : 0.058333 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 437 ; Train Loss : 0.052418 ; Train Acc : 0.505 ; Test Loss : 0.058296 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 438 ; Train Loss : 0.052029 ; Train Acc : 0.504 ; Test Loss : 0.058223 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 439 ; Train Loss : 0.052144 ; Train Acc : 0.504 ; Test Loss : 0.058492 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 440 ; Train Loss : 0.052194 ; Train Acc : 0.505 ; Test Loss : 0.058183 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 441 ; Train Loss : 0.052137 ; Train Acc : 0.505 ; Test Loss : 0.058179 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 442 ; Train Loss : 0.051978 ; Train Acc : 0.505 ; Test Loss : 0.058296 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 443 ; Train Loss : 0.052424 ; Train Acc : 0.505 ; Test Loss : 0.058312 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 444 ; Train Loss : 0.051972 ; Train Acc : 0.505 ; Test Loss : 0.058302 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 445 ; Train Loss : 0.052447 ; Train Acc : 0.506 ; Test Loss : 0.058344 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 446 ; Train Loss : 0.052208 ; Train Acc : 0.505 ; Test Loss : 0.058161 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 447 ; Train Loss : 0.052382 ; Train Acc : 0.505 ; Test Loss : 0.058349 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 448 ; Train Loss : 0.051963 ; Train Acc : 0.505 ; Test Loss : 0.058284 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 449 ; Train Loss : 0.052246 ; Train Acc : 0.504 ; Test Loss : 0.058295 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 450 ; Train Loss : 0.052484 ; Train Acc : 0.505 ; Test Loss : 0.058440 ; Test Acc : 0.438 ; LR : 0.019\n",
      "Epoch : 451 ; Train Loss : 0.052451 ; Train Acc : 0.506 ; Test Loss : 0.058511 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 452 ; Train Loss : 0.052078 ; Train Acc : 0.505 ; Test Loss : 0.058189 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 453 ; Train Loss : 0.052172 ; Train Acc : 0.506 ; Test Loss : 0.058639 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 454 ; Train Loss : 0.052586 ; Train Acc : 0.506 ; Test Loss : 0.058389 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 455 ; Train Loss : 0.052201 ; Train Acc : 0.505 ; Test Loss : 0.058386 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 456 ; Train Loss : 0.051995 ; Train Acc : 0.505 ; Test Loss : 0.058510 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 457 ; Train Loss : 0.052300 ; Train Acc : 0.505 ; Test Loss : 0.058226 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 458 ; Train Loss : 0.052124 ; Train Acc : 0.505 ; Test Loss : 0.058264 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 459 ; Train Loss : 0.052230 ; Train Acc : 0.506 ; Test Loss : 0.058302 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 460 ; Train Loss : 0.052478 ; Train Acc : 0.506 ; Test Loss : 0.058211 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 461 ; Train Loss : 0.052181 ; Train Acc : 0.506 ; Test Loss : 0.058266 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 462 ; Train Loss : 0.052356 ; Train Acc : 0.505 ; Test Loss : 0.058449 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 463 ; Train Loss : 0.052069 ; Train Acc : 0.505 ; Test Loss : 0.058287 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 464 ; Train Loss : 0.052272 ; Train Acc : 0.505 ; Test Loss : 0.058395 ; Test Acc : 0.438 ; LR : 0.017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 465 ; Train Loss : 0.052224 ; Train Acc : 0.505 ; Test Loss : 0.058440 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 466 ; Train Loss : 0.052048 ; Train Acc : 0.506 ; Test Loss : 0.058313 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 467 ; Train Loss : 0.052156 ; Train Acc : 0.506 ; Test Loss : 0.058450 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 468 ; Train Loss : 0.052198 ; Train Acc : 0.505 ; Test Loss : 0.058261 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 469 ; Train Loss : 0.051848 ; Train Acc : 0.506 ; Test Loss : 0.058314 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 470 ; Train Loss : 0.052123 ; Train Acc : 0.505 ; Test Loss : 0.058321 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 471 ; Train Loss : 0.051643 ; Train Acc : 0.506 ; Test Loss : 0.058508 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 472 ; Train Loss : 0.052275 ; Train Acc : 0.506 ; Test Loss : 0.058359 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 473 ; Train Loss : 0.052370 ; Train Acc : 0.507 ; Test Loss : 0.058291 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 474 ; Train Loss : 0.052104 ; Train Acc : 0.506 ; Test Loss : 0.058501 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 475 ; Train Loss : 0.052405 ; Train Acc : 0.505 ; Test Loss : 0.058523 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 476 ; Train Loss : 0.052368 ; Train Acc : 0.505 ; Test Loss : 0.058339 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 477 ; Train Loss : 0.052118 ; Train Acc : 0.505 ; Test Loss : 0.058703 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 478 ; Train Loss : 0.052250 ; Train Acc : 0.504 ; Test Loss : 0.058337 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 479 ; Train Loss : 0.052204 ; Train Acc : 0.505 ; Test Loss : 0.058485 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 480 ; Train Loss : 0.051745 ; Train Acc : 0.505 ; Test Loss : 0.058308 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 481 ; Train Loss : 0.052001 ; Train Acc : 0.505 ; Test Loss : 0.058438 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 482 ; Train Loss : 0.052009 ; Train Acc : 0.506 ; Test Loss : 0.058620 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 483 ; Train Loss : 0.052276 ; Train Acc : 0.504 ; Test Loss : 0.058358 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 484 ; Train Loss : 0.052293 ; Train Acc : 0.505 ; Test Loss : 0.058474 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 485 ; Train Loss : 0.052452 ; Train Acc : 0.505 ; Test Loss : 0.058840 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 486 ; Train Loss : 0.052583 ; Train Acc : 0.506 ; Test Loss : 0.058399 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 487 ; Train Loss : 0.051823 ; Train Acc : 0.505 ; Test Loss : 0.058304 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 488 ; Train Loss : 0.052383 ; Train Acc : 0.506 ; Test Loss : 0.058365 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 489 ; Train Loss : 0.052272 ; Train Acc : 0.506 ; Test Loss : 0.058495 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 490 ; Train Loss : 0.052191 ; Train Acc : 0.505 ; Test Loss : 0.058371 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 491 ; Train Loss : 0.052157 ; Train Acc : 0.506 ; Test Loss : 0.058357 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 492 ; Train Loss : 0.052231 ; Train Acc : 0.506 ; Test Loss : 0.058357 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 493 ; Train Loss : 0.051939 ; Train Acc : 0.505 ; Test Loss : 0.058573 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 494 ; Train Loss : 0.052329 ; Train Acc : 0.506 ; Test Loss : 0.058617 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 495 ; Train Loss : 0.052168 ; Train Acc : 0.505 ; Test Loss : 0.058615 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 496 ; Train Loss : 0.052399 ; Train Acc : 0.504 ; Test Loss : 0.058413 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 497 ; Train Loss : 0.052067 ; Train Acc : 0.505 ; Test Loss : 0.058502 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 498 ; Train Loss : 0.052328 ; Train Acc : 0.505 ; Test Loss : 0.058312 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 499 ; Train Loss : 0.052358 ; Train Acc : 0.505 ; Test Loss : 0.058688 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 500 ; Train Loss : 0.052276 ; Train Acc : 0.506 ; Test Loss : 0.058555 ; Test Acc : 0.438 ; LR : 0.017\n",
      "Epoch : 501 ; Train Loss : 0.052133 ; Train Acc : 0.507 ; Test Loss : 0.058487 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 502 ; Train Loss : 0.052181 ; Train Acc : 0.506 ; Test Loss : 0.058503 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 503 ; Train Loss : 0.052424 ; Train Acc : 0.507 ; Test Loss : 0.058351 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 504 ; Train Loss : 0.052608 ; Train Acc : 0.505 ; Test Loss : 0.058554 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 505 ; Train Loss : 0.052223 ; Train Acc : 0.506 ; Test Loss : 0.058408 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 506 ; Train Loss : 0.051835 ; Train Acc : 0.505 ; Test Loss : 0.058342 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 507 ; Train Loss : 0.051902 ; Train Acc : 0.505 ; Test Loss : 0.058509 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 508 ; Train Loss : 0.051748 ; Train Acc : 0.506 ; Test Loss : 0.058395 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 509 ; Train Loss : 0.052268 ; Train Acc : 0.506 ; Test Loss : 0.058444 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 510 ; Train Loss : 0.052094 ; Train Acc : 0.506 ; Test Loss : 0.058415 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 511 ; Train Loss : 0.052216 ; Train Acc : 0.505 ; Test Loss : 0.058432 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 512 ; Train Loss : 0.052326 ; Train Acc : 0.506 ; Test Loss : 0.058645 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 513 ; Train Loss : 0.052195 ; Train Acc : 0.506 ; Test Loss : 0.058581 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 514 ; Train Loss : 0.052210 ; Train Acc : 0.506 ; Test Loss : 0.058652 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 515 ; Train Loss : 0.051893 ; Train Acc : 0.505 ; Test Loss : 0.058455 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 516 ; Train Loss : 0.052137 ; Train Acc : 0.506 ; Test Loss : 0.058467 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 517 ; Train Loss : 0.052209 ; Train Acc : 0.506 ; Test Loss : 0.058597 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 518 ; Train Loss : 0.052440 ; Train Acc : 0.506 ; Test Loss : 0.058547 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 519 ; Train Loss : 0.052110 ; Train Acc : 0.506 ; Test Loss : 0.058710 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 520 ; Train Loss : 0.052094 ; Train Acc : 0.506 ; Test Loss : 0.058463 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 521 ; Train Loss : 0.052021 ; Train Acc : 0.506 ; Test Loss : 0.058508 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 522 ; Train Loss : 0.051869 ; Train Acc : 0.506 ; Test Loss : 0.058505 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 523 ; Train Loss : 0.052219 ; Train Acc : 0.506 ; Test Loss : 0.058508 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 524 ; Train Loss : 0.052044 ; Train Acc : 0.506 ; Test Loss : 0.058602 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 525 ; Train Loss : 0.052119 ; Train Acc : 0.506 ; Test Loss : 0.058585 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 526 ; Train Loss : 0.052102 ; Train Acc : 0.505 ; Test Loss : 0.058625 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 527 ; Train Loss : 0.051858 ; Train Acc : 0.505 ; Test Loss : 0.058704 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 528 ; Train Loss : 0.051891 ; Train Acc : 0.506 ; Test Loss : 0.058693 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 529 ; Train Loss : 0.052301 ; Train Acc : 0.506 ; Test Loss : 0.058496 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 530 ; Train Loss : 0.051895 ; Train Acc : 0.506 ; Test Loss : 0.058728 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 531 ; Train Loss : 0.051983 ; Train Acc : 0.506 ; Test Loss : 0.058526 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 532 ; Train Loss : 0.051870 ; Train Acc : 0.506 ; Test Loss : 0.058623 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 533 ; Train Loss : 0.052121 ; Train Acc : 0.506 ; Test Loss : 0.058651 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 534 ; Train Loss : 0.052078 ; Train Acc : 0.506 ; Test Loss : 0.058547 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 535 ; Train Loss : 0.052201 ; Train Acc : 0.506 ; Test Loss : 0.058731 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 536 ; Train Loss : 0.051685 ; Train Acc : 0.505 ; Test Loss : 0.058547 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 537 ; Train Loss : 0.052214 ; Train Acc : 0.506 ; Test Loss : 0.058573 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 538 ; Train Loss : 0.052139 ; Train Acc : 0.505 ; Test Loss : 0.058755 ; Test Acc : 0.438 ; LR : 0.016\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : 539 ; Train Loss : 0.052102 ; Train Acc : 0.506 ; Test Loss : 0.058585 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 540 ; Train Loss : 0.051953 ; Train Acc : 0.506 ; Test Loss : 0.058572 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 541 ; Train Loss : 0.052048 ; Train Acc : 0.506 ; Test Loss : 0.058564 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 542 ; Train Loss : 0.052147 ; Train Acc : 0.506 ; Test Loss : 0.058696 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 543 ; Train Loss : 0.051921 ; Train Acc : 0.506 ; Test Loss : 0.058595 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 544 ; Train Loss : 0.052081 ; Train Acc : 0.507 ; Test Loss : 0.058585 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 545 ; Train Loss : 0.052351 ; Train Acc : 0.507 ; Test Loss : 0.058560 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 546 ; Train Loss : 0.051857 ; Train Acc : 0.506 ; Test Loss : 0.058611 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 547 ; Train Loss : 0.052235 ; Train Acc : 0.506 ; Test Loss : 0.058671 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 548 ; Train Loss : 0.052539 ; Train Acc : 0.506 ; Test Loss : 0.058691 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 549 ; Train Loss : 0.051935 ; Train Acc : 0.507 ; Test Loss : 0.058654 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 550 ; Train Loss : 0.051909 ; Train Acc : 0.506 ; Test Loss : 0.058735 ; Test Acc : 0.438 ; LR : 0.016\n",
      "Epoch : 551 ; Train Loss : 0.051820 ; Train Acc : 0.507 ; Test Loss : 0.058887 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 552 ; Train Loss : 0.052124 ; Train Acc : 0.506 ; Test Loss : 0.058622 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 553 ; Train Loss : 0.052217 ; Train Acc : 0.507 ; Test Loss : 0.058686 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 554 ; Train Loss : 0.052124 ; Train Acc : 0.507 ; Test Loss : 0.058601 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 555 ; Train Loss : 0.051785 ; Train Acc : 0.505 ; Test Loss : 0.058750 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 556 ; Train Loss : 0.052209 ; Train Acc : 0.506 ; Test Loss : 0.058642 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 557 ; Train Loss : 0.051895 ; Train Acc : 0.507 ; Test Loss : 0.058624 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 558 ; Train Loss : 0.052081 ; Train Acc : 0.507 ; Test Loss : 0.058915 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 559 ; Train Loss : 0.052074 ; Train Acc : 0.505 ; Test Loss : 0.058992 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 560 ; Train Loss : 0.051931 ; Train Acc : 0.505 ; Test Loss : 0.058781 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 561 ; Train Loss : 0.052182 ; Train Acc : 0.506 ; Test Loss : 0.058600 ; Test Acc : 0.438 ; LR : 0.014\n",
      "Epoch : 562 ; Train Loss : 0.052004 ; Train Acc : 0.507 ; Test Loss : 0.058735 ; Test Acc : 0.438 ; LR : 0.014\n"
     ]
    }
   ],
   "source": [
    "train_losses, train_accs, test_losses, test_accs = [], [], [], []\n",
    "\n",
    "for hidden_unit in hidden_units:\n",
    "    model = simple_FC(hidden_unit)\n",
    "    \n",
    "    if hidden_unit == 1:\n",
    "        torch.nn.init.xavier_uniform_(model.features[1].weight, gain=1.0)\n",
    "        torch.nn.init.xavier_uniform_(model.classifier.weight, gain=1.0)\n",
    "    else: \n",
    "        torch.nn.init.normal_(model.features[1].weight, mean=0.0, std=0.1)\n",
    "        torch.nn.init.normal_(model.classifier.weight, mean=0.0, std=0.1)\n",
    "    \n",
    "    model = model.to(device)\n",
    "    optimizer = torch.optim.SGD(model.parameters(), momentum=momentum, lr=learning_rate)\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    \n",
    "    train_loss, train_acc, test_loss, test_acc = train_and_evaluate_model(model, hidden_unit, optimizer, criterion, n_epochs)\n",
    "    print(\"\\nHidden Neurons : %d ; Train Loss : %f ; Train Acc : %.3f ; Test Loss : %f ; Test Acc : %.3f\\n\\n\" \\\n",
    "              % (hidden_unit, train_loss, train_acc, test_loss, test_acc))\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accs.append(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdf2dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hidden_units, train_losses, marker='o', label='train')\n",
    "plt.plot(hidden_units, test_losses, marker='o', label='test')\n",
    "plt.xlabel('Number of Hidden Units')\n",
    "plt.ylabel('MSE loss')\n",
    "plt.title('Double Descent Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580e81c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(parameters, train_losses, marker='o', label='train')\n",
    "plt.plot(parameters, test_losses, marker='o', label='test')\n",
    "plt.xlabel('Number of Model Parameters')\n",
    "plt.ylabel('MSE loss')\n",
    "plt.title('Double Descent Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa093b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(hidden_units, train_accs, marker='o', label='train')\n",
    "plt.plot(hidden_units, test_accs, marker='o', label='test')\n",
    "plt.xlabel('Number of Hidden Units')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Double Descent Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfda2cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(parameters, train_accs, marker='o', label='train')\n",
    "plt.plot(parameters, test_accs, marker='o', label='test')\n",
    "plt.xlabel('Number of Model Parameters')\n",
    "plt.ylabel('MSE loss')\n",
    "plt.title('Double Descent Curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f9b939",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
