{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import sys\n",
    "sys.path.append('../.')\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "activation = 'logistic'\n",
    "\n",
    "sample_size = 4000\n",
    "feature_dimension = 78\n",
    "# Sample wise double descent for linear model only affected by input Dimension, even if some of input is disturbance (useless) term\n",
    "used_feature = feature_dimension\n",
    "noise_level = 0\n",
    "bias = 0\n",
    "y_dimension = 1\n",
    "test_size = 0.5\n",
    "# set a random seed (int) if you want to the datasets to be fixed\n",
    "RAND_ST = None\n",
    "\n",
    "X, y = datasets.linear_regression_with_gaussian(sample_size=sample_size, feautre_size=feature_dimension, used_feature=used_feature, noise_level=noise_level, bias=bias, y_dimension=y_dimension, random_state=RAND_ST)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = scaler.transform(X)\n",
    "\n",
    "weight = np.random.random((X.shape[1], y_dimension))\n",
    "y = 1 / (1 + np.exp(-1 * X.dot(weight)))\n",
    "y[y >= 0.5] = 1\n",
    "y[y < 0.5] = 0\n",
    "y = y.reshape(sample_size)\n",
    "\n",
    "model_range = 100\n",
    "# X = np.column_stack((X, np.ones((X.shape[0], model_range-X.shape[1]))))\n",
    "# X = np.column_stack((X, np.random.rand(X.shape[0], model_range-X.shape[1])))\n",
    "\n",
    "# Sklearn already has bias term inside the nn\n",
    "# X = np.column_stack((np.ones(sample_size), X))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noiseless Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "model_size = [size for size in range(1, model_range, 1)]\n",
    "\n",
    "for size in model_size:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=RAND_ST, shuffle=True)\n",
    "    model = MLPRegressor(hidden_layer_sizes=(size,), activation=activation, solver='sgd', alpha=0, beta_1=0, beta_2=0, batch_size=1, max_iter=5000, momentum=0, validation_fraction=0.0).fit(X_train, y_train)\n",
    "    \n",
    "    train_losses.append(np.sum(np.power(model.predict(X_train)-y_train, 2)) / X_test.shape[0])\n",
    "    test_losses.append(np.sum(np.power(model.predict(X_test)-y_test, 2)) / X_test.shape[0])\n",
    "    \n",
    "    # print(np.sum(np.power(model.predict(X_train[:, :size])-y_train, 2)) / X_test.shape[0])\n",
    "    # print(np.sum(np.power(model.predict(X_test[:, :size])-y_test, 2)) / X_test.shape[0])\n",
    "    # print(size)\n",
    "    # print()\n",
    "\n",
    "# lowestIndex = np.argsort(test_losses)\n",
    "\n",
    "plt.xlabel(\"Model size\")\n",
    "plt.ylabel(\"loss (MSELoss)\")\n",
    "plt.plot(model_size, train_losses, label = \"train loss\")\n",
    "plt.plot(model_size, test_losses, label = \"test loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Noisy Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X + np.random.normal(0, 0.3, (sample_size, X.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "model_size = [size for size in range(1, model_range, 1)]\n",
    "\n",
    "for size in model_size:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=RAND_ST, shuffle=True)\n",
    "    model = MLPRegressor(hidden_layer_sizes=(size,), activation=activation, solver='sgd', alpha=0, beta_1=0, beta_2=0, batch_size=1, max_iter=5000, momentum=0, validation_fraction=0.0).fit(X_train, y_train)\n",
    "    \n",
    "    train_losses.append(np.sum(np.power(model.predict(X_train)-y_train, 2)) / X_test.shape[0])\n",
    "    test_losses.append(np.sum(np.power(model.predict(X_test)-y_test, 2)) / X_test.shape[0])\n",
    "    \n",
    "    # print(np.sum(np.power(model.predict(X_train[:, :size])-y_train, 2)) / X_test.shape[0])\n",
    "    # print(np.sum(np.power(model.predict(X_test[:, :size])-y_test, 2)) / X_test.shape[0])\n",
    "    # print(size)\n",
    "    # print()\n",
    "\n",
    "# lowestIndex = np.argsort(test_losses)\n",
    "\n",
    "plt.xlabel(\"Model size\")\n",
    "plt.ylabel(\"loss (MSELoss)\")\n",
    "plt.plot(model_size, train_losses, label = \"train loss\")\n",
    "plt.plot(model_size, test_losses, label = \"test loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
